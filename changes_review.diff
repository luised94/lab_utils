diff --git a/.Rprofile b/.Rprofile
new file mode 100644
index 0000000..81b960f
--- /dev/null
+++ b/.Rprofile
@@ -0,0 +1 @@
+source("renv/activate.R")
diff --git a/.gitignore b/.gitignore
index 58a116f..72f66d9 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,9 +1,10 @@
 */sampleGridConfig.R
+repository_aggregate.md
+treestructure*.txt
 sampleGridConfig.R
 slurm-*.out
 test.log
 Rplots.pdf
-**/renv.lock
 **/renv/library/
 **/renv/profiles/
 **/renv/settings.dcf
diff --git a/001_setupR/000_installingR4.2.0.sh b/001_setupR/000_installingR4.2.0.sh
deleted file mode 100644
index bb4edfd..0000000
--- a/001_setupR/000_installingR4.2.0.sh
+++ /dev/null
@@ -1,56 +0,0 @@
-# Installing R 4.2.0
-# Purpose: Create a working environment that is similar to linux cluster to minimize the chance of conflict.
-# See Brain Mode short version for thread
-
-sudo apt-get update
-sudo apt install r-base
-sudo apt install build-essential libcurl4-gnutls-dev libxml2-dev libssl-dev
-
-R_VERSION="R-4.2.0"
-DIR_TO_INSTALL=$HOME
-curl --output "$HOME/${R_VERSION}.tar.gz" "https://cran.r-project.org/src/base/$(echo ${R_VERSION} | cut  -d. -f1)/${R_VERSION}.tar.gz"
-
-# Install Java
-sudo apt-get install default-jdk
-export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
-#echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/my_config/dotfiles/bashrc
-export PATH=$JAVA_HOME/bin:$PATH
-#sudo R CMD javareconf
-
-tar -xzvf ${R_VERSION}.tar.gz
-cd ${R_VERSION}
-
-
-./configure --enable-R-shlib --with-blas --with-lapack --without-x
-make
-sudo make install
-
-# Install for PDF and html rendenring of manuals
-# sudo apt-get install texinfo
-# sudo apt-get install texlive texlive-fonts-recommended texlive-latex-recommended texlive-latex-extra
-
-# Install X11 headers and libs
-# sudo apt-get install libx11-dev xserver-xorg-dev xorg-dev
-
-export PATH=/usr/local/bin:$PATH
-#echo "export PATH=/usr/local/bin:$PATH" >> ~/.bashrc
-which R # should show R version 4.2.0
-
-mkdir -p $HOME/R/library
-
-
-
-
-
-export R_LIBS_USER=$HOME/R/library
-alias R='R --no-save'
-R
-
-Sys.getenv(HOME)
-dir.create(Sys.getenv("R_LIBS_USER"), recursive = TRUE)
-.libPaths(Sys.getenv("R_LIBS_USER"))
-.libPaths()
-options(repos = c(CRAN = "https://cloud.r-project.org"))
-install.packages(c("renv", "xml2", "lintr", "roxygen2", "languageserver"), dependencies = TRUE, INSTALL_opts = '--no-lock')
-install.packages(c("languageserver"), dependencies = TRUE, INSTALL_opts = '--no-lock')
-q()
diff --git a/001_setupR/002_uninstallR.sh b/001_setupR/002_uninstallR.sh
deleted file mode 100644
index 85cc2be..0000000
--- a/001_setupR/002_uninstallR.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-sudo apt-get remove r-base r-base-core r-recommended
-sudo apt-get autoremove
-sudo apt-get purge r-base r-base-core r-recommended
-sudo rm -rf /usr/local/lib/R
-sudo rm -rf /usr/lib/R
-sudo rm -rf /etc/R
-rm -rf ~/.R
-rm -rf ~/R
-rm -rf ~/R-4.2.0/
-rm -rf ~/.cache/R
-sudo apt-get update
-sudo apt-get clean
-sudo apt-get autoclean
-rm -rf ~/R/x86_64-pc-linux-gnu-library
-sudo find / -name "*R-*" -type d
diff --git a/001_setupR/deprecatedCode/001_sh_node_installBHFromSource.sh b/001_setupR/deprecatedCode/001_sh_node_installBHFromSource.sh
deleted file mode 100644
index c2441df..0000000
--- a/001_setupR/deprecatedCode/001_sh_node_installBHFromSource.sh
+++ /dev/null
@@ -1,4 +0,0 @@
-# Installing BH from source.
-# Trying to install flowCore for flow_Cytometry data and am having problems install cytolib, one of the dependencies. 
-# A github repo suggested that BH 1.75 would solve the issue.
-# This is just to document the commands I ran after attempting to install cytolib usijg renv.
diff --git a/R/config/core_config.R b/R/config/core_config.R
new file mode 100644
index 0000000..c3ed303
--- /dev/null
+++ b/R/config/core_config.R
@@ -0,0 +1,30 @@
+# R/config/core_config.R
+#' Core Configuration Settings
+#' @export CORE_CONFIG
+CORE_CONFIG <- list(
+    # System
+    SYSTEM = list(
+        VERSION = "1.0.0",
+        R_MIN_VERSION = "4.2.0",
+        LOCK_TIMEOUT = 30,
+        LOCK_RETRIES = 3
+    ),
+    # Logging
+    LOGGING = list(
+        LEVELS = c("TRACE", "DEBUG", "INFO", "WARNING", "ERROR", "FATAL"),
+        DEFAULT_ROOT = "~/logs",
+        FORMAT = "[%s] [%s] [%s] %s",  # timestamp, level, context, message
+        MAX_SIZE = 10e6  # 10MB
+    ),
+    # Paths
+    PATHS = list(
+        ROOT = "~/lab_utils",
+        MODULES = "~/lab_utils/R/modules",
+        CONFIG = "~/lab_utils/R/config",
+        DATA = "~/data"
+    ),
+    MODULES = list(
+        REQUIRED = c("bmc", "ngs", "genome"),
+        OPTIONAL = c("viz")
+    )
+)
diff --git a/R/config/modules/bmc_sample_grid_config.R b/R/config/modules/bmc_sample_grid_config.R
new file mode 100644
index 0000000..51c63e2
--- /dev/null
+++ b/R/config/modules/bmc_sample_grid_config.R
@@ -0,0 +1,208 @@
+#!/usr/bin/env Rscript
+# TODO: Do we need to validate the experiment_id format?
+# TODO: Should we add validation for category value uniqueness?
+# R/config/experiment_config.R
+
+#' Experiment Configuration
+#' @description Define experimental setup and validation
+#' @export EXPERIMENT_CONFIG
+
+
+EXPERIMENT_CONFIG <- list(
+    METADATA = list(
+        EXPERIMENT_ID = "241010Bel",
+        EXPECTED_SAMPLES = 65,
+        VERSION = "1.0.0"
+    ),
+    
+    CATEGORIES = list(
+        rescue_allele = c("NONE", "WT", "4R", "PS"),
+        auxin_treatment = c("NO", "YES"),
+        time_after_release = c("0", "1", "2"),
+        antibody = c("Input", "ProtG", "HM1108", "V5", "ALFA", "UM174")
+    ),
+    
+    INVALID_COMBINATIONS = list(
+        rescue_allele_auxin_treatment = quote(rescue_allele %in% c("4R", "PS") & auxin_treatment == "NO"),
+        protg_time_after_release = quote(antibody == "ProtG" & time_after_release %in% c("1", "2")),
+        input_time_after_release = quote(antibody == "Input" & time_after_release %in% c("1", "2")),
+        input_rescue_allele_auxin_treatment = quote(antibody == "Input" & rescue_allele %in% c("NONE", "WT") & auxin_treatment == "YES")
+    ),
+    
+    EXPERIMENTAL_CONDITIONS = list(
+        is_input = quote(time_after_release == "0" & antibody == "Input"),
+        is_protg = quote(rescue_allele == "WT" & time_after_release == "0" & antibody == "ProtG" & auxin_treatment == "NO"),
+        is_v5 = quote(antibody == "V5"),
+        is_alfa = quote(antibody == "ALFA"),
+        is_1108 = quote(antibody == "HM1108" & time_after_release == "0"),
+        is_174 = quote(antibody == "UM174")
+    ),
+    
+    COMPARISONS = list(
+        comp_1108forNoneAndWT = quote(antibody == "HM1108" & rescue_allele %in% c("NONE", "WT")),
+        comp_1108forNoneAndWT_auxin = quote(antibody == "HM1108" & auxin_treatment == "YES"),
+        comp_timeAfterReleaseV5WT = quote(antibody == "V5" & rescue_allele == "WT" & auxin_treatment == "YES"),
+        comp_timeAfterReleaseV5NoTag = quote(antibody == "V5" & rescue_allele == "NONE" & auxin_treatment == "YES"),
+        comp_V5atTwoHours = quote(antibody == "V5" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_UM174atTwoHours = quote(antibody == "UM174" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_ALFAforNoRescueNoTreat = quote(antibody == "ALFA" & rescue_allele == "NONE" & auxin_treatment == "NO"),
+        comp_ALFAforNoRescueWithTreat = quote(antibody == "ALFA" & rescue_allele == "NONE" & auxin_treatment == "YES"),
+        comp_ALFAatTwoHoursForAllAlleles = quote(antibody == "ALFA" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_UM174atZeroHoursForAllAlleles = quote(antibody == "UM174" & time_after_release == "0" & auxin_treatment == "YES"),
+        comp_AuxinEffectOnUM174 = quote(antibody == "UM174" & time_after_release == "2" & rescue_allele %in% c("NONE", "WT"))
+    ),
+    
+    CONTROL_FACTORS = list(
+        genotype = c("rescue_allele")
+    ),
+    
+    COLUMN_ORDER = c("antibody", "rescue_allele", "auxin_treatment", "time_after_release")
+)
+
+#' Validate Category Values
+validate_experiment_categories <- function(experiment_config) {
+    for (category_name in names(experiment_config$CATEGORIES)) {
+        values <- experiment_config$CATEGORIES[[category_name]]
+        if (!is.character(values)) {
+            stop(sprintf(
+                "Category '%s' values must be character vectors, got %s",
+                category_name, class(values)
+            ))
+        }
+    }
+    invisible(TRUE)
+}
+
+#' Validate Column References
+validate_experiment_column_references <- function(experiment_config) {
+    # Get valid column names
+    valid_columns <- names(experiment_config$CATEGORIES)
+    
+    # Check comparison groups
+    for (comp_name in names(experiment_config$COMPARISONS)) {
+        comp_expr <- experiment_config$COMPARISONS[[comp_name]]
+        comp_vars <- all.vars(comp_expr)
+        invalid_cols <- setdiff(comp_vars, valid_columns)
+        if (length(invalid_cols) > 0) {
+            stop(sprintf(
+                "Invalid columns in comparison '%s': %s",
+                comp_name, paste(invalid_cols, collapse = ", ")
+            ))
+        }
+    }
+    
+    # Check control factors
+    for (factor_name in names(experiment_config$CONTROL_FACTORS)) {
+        invalid_cols <- setdiff(
+            experiment_config$CONTROL_FACTORS[[factor_name]],
+            valid_columns
+        )
+        if (length(invalid_cols) > 0) {
+            stop(sprintf(
+                "Invalid columns in control factor '%s': %s",
+                factor_name, paste(invalid_cols, collapse = ", ")
+            ))
+        }
+    }
+    
+    # Check experimental conditions
+    for (cond_name in names(experiment_config$EXPERIMENTAL_CONDITIONS)) {
+        cond_expr <- experiment_config$EXPERIMENTAL_CONDITIONS[[cond_name]]
+        cond_vars <- all.vars(cond_expr)
+        invalid_cols <- setdiff(cond_vars, valid_columns)
+        if (length(invalid_cols) > 0) {
+            stop(sprintf(
+                "Invalid columns in condition '%s': %s",
+                cond_name, paste(invalid_cols, collapse = ", ")
+            ))
+        }
+    }
+    
+    invisible(TRUE)
+}
+
+#' Validate Column Order
+validate_experiment_column_order <- function(experiment_config) {
+    if (!identical(
+        sort(names(experiment_config$CATEGORIES)),
+        sort(experiment_config$COLUMN_ORDER)
+    )) {
+        stop("Column order must include all category columns")
+    }
+    invisible(TRUE)
+}
+
+#' Generate Experiment Sample Grid
+#' @param project_config List Project configuration
+#' @param experiment_config List Experiment configuration
+#' @param init_logging Logical Whether to initialize logging
+#' @return data.frame Filtered experiment grid
+generate_experiment_grid <- function(
+    experiment_config = EXPERIMENT_CONFIG,
+    init_logging = TRUE
+) {
+    # Initialize logging if requested
+    if (init_logging) {
+        if (file.exists("~/lab_utils/R/functions/logging_utils.R")) {
+            source("~/lab_utils/R/functions/logging_utils.R")
+            log_file <- initialize_logging(script_name = "experiment_grid")
+            log_info("Starting experiment grid generation", log_file)
+        } else {
+            warning("logging_utils.R not found. Proceeding without logging.")
+        }
+    }
+    
+    tryCatch({
+        # Run validations
+        validate_experiment_categories(experiment_config)
+        validate_experiment_column_references(experiment_config)
+        validate_experiment_column_order(experiment_config)
+
+        # Generate combinations
+        grid <- do.call(expand.grid, experiment_config$CATEGORIES)
+        
+        # Filter invalid combinations
+        invalid_idx <- Reduce(
+            `|`, 
+            lapply(experiment_config$INVALID_COMBINATIONS, eval, envir = grid)
+        )
+        grid <- subset(grid, !invalid_idx)
+        
+        # Apply experimental conditions
+        valid_idx <- Reduce(
+            `|`, 
+            lapply(experiment_config$EXPERIMENTAL_CONDITIONS, eval, envir = grid)
+        )
+        grid <- subset(grid, valid_idx)
+        
+        # Verify sample count
+        n_samples <- nrow(grid)
+        if (n_samples != experiment_config$METADATA$EXPECTED_SAMPLES) {
+            warning(sprintf(
+                "Expected %d samples, got %d", 
+                experiment_config$METADATA$EXPECTED_SAMPLES, 
+                n_samples
+            ))
+        }
+        
+        # Sort columns
+        grid <- grid[, experiment_config$COLUMN_ORDER]
+        
+        # Add attributes
+        attr(grid, "control_factors") <- experiment_config$CONTROL_FACTORS
+        attr(grid, "experiment_id") <- experiment_config$METADATA$EXPERIMENT_ID
+        
+        if (init_logging) {
+            log_info(sprintf("Generated grid with %d samples", nrow(grid)))
+        }
+        
+        return(grid)
+        
+    }, error = function(e) {
+        msg <- sprintf("Failed to generate experiment grid: %s", e$message)
+        if (init_logging) {
+            log_error(msg)
+        }
+        stop(msg)
+    })
+}
diff --git a/R/config/modules/data_sources_config.R b/R/config/modules/data_sources_config.R
new file mode 100644
index 0000000..f14952b
--- /dev/null
+++ b/R/config/modules/data_sources_config.R
@@ -0,0 +1,44 @@
+#' Add to existing configurations
+CONFIG <- list(
+    SOURCES = list(
+        HAWKINS = list(
+            URL = "https://ars.els-cdn.com/content/image/1-s2.0-S2211124713005834-mmc2.xlsx",
+            FILE = "hawkins-origins-timing.xlsx",
+            TYPE = "timing"
+        ),
+        
+        EATON_PEAKS = list(
+            URL = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM424nnn/GSM424494/suppl/GSM424494_wt_G2_orc_chip_combined.bed.gz",
+            FILE = "eaton_peaks.bed.gz",
+            TYPE = "peaks"
+        ),
+        
+        SGD = list(
+            FEATURES = list(
+                URL = "https://downloads.yeastgenome.org/curation/chromosomal_feature/SGD_features.tab",
+                FILE = "SGD_features.tab",
+                TYPE = "features"
+            ),
+            GFF = list(
+                URL = "https://downloads.yeastgenome.org/curation/chromosomal_feature/saccharomyces_cerevisiae.gff.gz",
+                FILE = "saccharomyces_cerevisiae.gff.gz",
+                TYPE = "annotation"
+            )
+        ),
+        
+        EATON_ACS = list(
+            URL = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM424nnn/GSM424494/suppl/GSM424494_acs_locations.bed.gz",
+            FILE = "eaton_acs.bed.gz",
+            TYPE = "acs"
+        )
+    ),
+    
+    SGD_COLUMNS = c(
+        "Primary_SGDID", "Feature_type", "Feature_qualifier",
+        "Feature_name", "Standard_gene_name", "Alias",
+        "Parent_feature_name", "Secondary_SGDID", "Chromosome",
+        "Start_coordinate", "Stop_coordinate", "Strand",
+        "Genetic_position", "Coordinate_version",
+        "Sequence_version", "Description"
+    )
+)
diff --git a/R/config/modules/feature_config.R b/R/config/modules/feature_config.R
new file mode 100644
index 0000000..a1a4edf
--- /dev/null
+++ b/R/config/modules/feature_config.R
@@ -0,0 +1,38 @@
+#' Add to existing configurations
+CONFIG <- list(
+    FEATURES = list(
+        PATHS = list(
+            BASE_DIR = file.path(Sys.getenv("HOME"), "data", "feature_files")
+        ),
+        
+        PATTERNS = list(
+            EXCLUDE = c(
+                "sample-key.tab",
+                "\\.rds$",
+                "_converted.bed$"
+            ),
+            VERIFY = c(
+                "\\.rds$",
+                "_converted.bed$"
+            )
+        ),
+        
+        FILE_TYPES = list(
+            NUCLEOSOME = "Nucleosome_calls",
+            TIMING = "hawkins",
+            TRANSCRIPTION = "Rhee",
+            FEATURES = "SGD"
+        ),
+        
+        COLUMNS = list(
+            NUCLEOSOME = list(
+                EXCLUDE = c("Nucleosome ID", "Nucleosome dyad", "Chromosome"),
+                POSITION = "Nucleosome dyad"
+            ),
+            TIMING = list(
+                EXCLUDE = c("Chromosome", "Position"),
+                WINDOW = 100  # +/- bp around position
+            )
+        )
+    )
+)
diff --git a/R/config/modules/genome_config.R b/R/config/modules/genome_config.R
new file mode 100644
index 0000000..feba220
--- /dev/null
+++ b/R/config/modules/genome_config.R
@@ -0,0 +1,108 @@
+#' Genome Configuration Parameters
+CONFIG <- list(
+    PATHS = list(
+        BASE_DIR = file.path(Sys.getenv("HOME"), "data"),
+        GENOME_DIR = "REFGENS",
+        DOCUMENTATION_DIR = "documentation"
+    ),
+    
+    PATTERNS = list(
+        GENOME = "S288C_refgenome.fna",
+        SAMPLE_TABLE = "sample_table",
+        CONTROL_FACTOR_PREFIX = "X__cf_"
+    ),
+    
+    CHROMOSOME_MAPPING = list(
+        STYLES = c("UCSC", "Roman", "Numeric"),
+        ROMAN = c("I", "II", "III", "IV", "V", "VI", "VII", "VIII",
+                 "IX", "X", "XI", "XII", "XIII", "XIV", "XV", "XVI"),
+        PREFIX = "chr"
+    ),
+    
+    REQUIRED_PACKAGES = c(
+        "QuasR",
+        "GenomicAlignments",
+        "Gviz",
+        "rtracklayer",
+        "ShortRead",
+        "tidyverse"
+    )
+)
+CONFIG <- list(
+    PATHS = list(
+        FEATURE_DIR = file.path(Sys.getenv("HOME"), "data", "feature_files")
+    ),
+    
+    FEATURE_TYPES = list(
+        PEAKS = "eaton_peaks",
+        ORIGINS = "origins",
+        GENES = "genes"
+    ),
+    
+    REQUIRED_CATEGORIES = list(
+        BASE = "antibody",
+        OPTIONAL = c("condition", "treatment", "timepoint")
+    ),
+    
+    LABEL_CONFIG = list(
+        SEPARATOR = "_",
+        MAX_LENGTH = 50,
+        TRUNCATE_SUFFIX = "..."
+    )
+)
+#' Merge with existing configurations
+CONFIG <- list(
+    GENOME = list(
+        DEFAULT_STRAND = "*",
+        DEFAULT_CHROMOSOME = "chr1",
+        STYLES = list(
+            UCSC = "UCSC",
+            ENSEMBL = "ENSEMBL"
+        )
+    ),
+    
+    VISUALIZATION = list(
+        TRACKS = list(
+            COLORS = c(
+                PRIMARY = "#1f77b4",
+                SECONDARY = "#ff7f0e",
+                HIGHLIGHT = "#2ca02c"
+            ),
+            TYPES = list(
+                DATA = "l",
+                ANNOTATION = "gene"
+            )
+        ),
+        DEFAULTS = list(
+            GENOME = "hg19",
+            MIN_HEIGHT = 0,
+            MAX_HEIGHT = 100
+        )
+    )
+)
+#' Add to existing genome configurations
+CONFIG <- list(
+    CHROMOSOMES = list(
+        SPECIAL = c("X", "Y", "MT", "M"),
+        PREFIX = "chr",
+        DEFAULT = list(
+            NAME = "chrX",
+            START = 1,
+            END = 170000,
+            STRAND = "*"
+        )
+    ),
+    
+    NAMING = list(
+        PATTERNS = list(
+            ROMAN = "^[IVXLCDM]+$",
+            NUMERIC = "^\\d+$",
+            PREFIX = "^chr"
+        ),
+        STYLES = list(
+            UCSC = "UCSC",     # chrI, chrII, etc.
+            ENSEMBL = "ENSEMBL", # 1, 2, etc.
+            NCBI = "NCBI"      # I, II, etc.
+        )
+    )
+)
diff --git a/R/config/modules/ngs_config.R b/R/config/modules/ngs_config.R
new file mode 100644
index 0000000..559c7e1
--- /dev/null
+++ b/R/config/modules/ngs_config.R
@@ -0,0 +1,25 @@
+#' Merge with existing configurations
+CONFIG <- list(
+    PATHS = list(
+        BASE_DIR = Sys.getenv("HOME"),
+        SUBDIRS = list(
+            ALIGNMENT = "alignment",
+            BIGWIG = "bigwig",
+            DOCUMENTATION = "documentation"
+        )
+    ),
+    
+    PATTERNS = list(
+        SAMPLE_TABLE = "sample_table",
+        CONTROL_FACTOR_PREFIX = "X__cf_",
+        REFERENCE_GENOME = "S288C",
+        BAM_SUFFIX = ".bam$"
+    ),
+    
+    CONTROL = list(
+        MAX_CONTROLS = 1,
+        DEFAULT_INDEX = 1,
+        ANTIBODY_COLUMN = "antibody",
+        INPUT_VALUE = "Input"
+    )
+)
diff --git a/R/config/modules/package_config.R b/R/config/modules/package_config.R
new file mode 100644
index 0000000..409070a
--- /dev/null
+++ b/R/config/modules/package_config.R
@@ -0,0 +1,146 @@
+#' Package Management Configuration
+CONFIG <- list(
+    REPOSITORIES = list(
+        CRAN = "https://cloud.r-project.org",
+        BIOC = BiocManager::repositories()
+    ),
+    
+    PACKAGES = list(
+        FASTQ_ANALYSIS = c(
+            "ShortRead",
+            "Rsubread",
+            "Biostrings",
+            "dada2"
+        ),
+        
+        TRACK_VISUALIZATION = c(
+            "QuasR",
+            "GenomicAlignments",
+            "Gviz",
+            "rtracklayer"
+        ),
+        
+        PEAK_ANALYSIS = c(
+            "ChIPseeker",
+            "ChIPpeakAnno",
+            "DiffBind",
+            "normR",
+            "mosaics",
+            "csaw"
+        ),
+        
+        MOTIF_ANALYSIS = c(
+            "motifStack",
+            "TFBSTools",
+            "JASPAR2020",
+            "universalmotif",
+            "memes"
+        ),
+        
+        VISUALIZATION = c(
+            "ggbio",
+            "ComplexHeatmap",
+            "EnhancedVolcano",
+            "ggplot2",
+            "ggcoverage",
+            "gggenome"
+        ),
+        
+        STATISTICS = c(
+            "DESeq2",
+            "edgeR",
+            "limma"
+        ),
+        
+        CORE = c(
+            "ggplot2",
+            "rmarkdown",
+            "knitr",
+            "tidyverse",
+            "furrr"
+        )
+    ),
+    
+    RENV = list(
+        LOCKFILE = "renv.lock",
+        LIBRARY = "renv/library"
+    )
+)
+#' Add to existing package configurations
+CONFIG <- list(
+    BIOINFORMATICS = list(
+        CORE = c(
+            "BiocGenerics",
+            "MatrixGenerics",
+            "GenomeInfoDb",
+            "GenomicRanges"
+        ),
+        
+        VISUALIZATION = c(
+            "ggplot2",
+            "pheatmap",
+            "ComplexHeatmap",
+            "circlize",
+            "ggbio",
+            "Gviz"
+        ),
+        
+        SEQUENCING = c(
+            "Rqc",
+            "ShortRead",
+            "QuasR",
+            "Rsubread",
+            "Rsamtools",
+            "Rbowtie",
+            "Rbowtie2"
+        ),
+        
+        ANALYSIS = c(
+            "DESeq2",
+            "RUVSeq",
+            "methylKit",
+            "ChIPpeakAnno",
+            "normr"
+        ),
+        
+        ANNOTATION = c(
+            "AnnotationHub",
+            "GenomicFeatures",
+            "BSgenome",
+            "BSgenome.Scerevisiae.UCSC.sacCer3"
+        ),
+        
+        MOTIF = c(
+            "MotifDb",
+            "TFBSTools",
+            "rGADEM",
+            "JASPAR2018"
+        )
+    ),
+    
+    DEVELOPMENT = list(
+        DOCUMENTATION = c(
+            "rmarkdown",
+            "xaringan",
+            "officer",
+            "quarto"
+        ),
+        
+        STYLE = c(
+            "styler",
+            "formatR"
+        ),
+        
+        TOOLS = c(
+            "devtools",
+            "rcrossref",
+            "taskscheduleR",
+            "bio3d"
+        )
+    ),
+    
+    GITHUB_PACKAGES = list(
+        "crsh/citr",
+        "paleolimbot/rbbt"
+    )
+)
diff --git a/R/config/modules/project_config.R b/R/config/modules/project_config.R
new file mode 100644
index 0000000..5c63332
--- /dev/null
+++ b/R/config/modules/project_config.R
@@ -0,0 +1,75 @@
+#!/usr/bin/env Rscript
+
+#' Project Configuration Definition
+#' @export PROJECT_CONFIG
+PROJECT_CONFIG <- list(
+    SYSTEM = list(
+        VERSION = "1.0.0",
+        R_MIN_VERSION = "4.2.0",
+        PLATFORM = .Platform$OS.type
+    ),
+    
+    PATHS = list(
+        ROOT = normalizePath("~/lab_utils"),
+        R_BASE = file.path(normalizePath("~/lab_utils"), "R"),
+        FUNCTIONS = "functions",
+        SCRIPTS = "scripts",
+        CONFIG = "config",
+        DATA = normalizePath("~/data"),
+        LOGS = normalizePath("~/logs")
+    ),
+    
+    LOAD_SEQUENCE = list(
+        CRITICAL = c(
+            "logging_utils.R",
+            "environment_utils.R",
+            "validation_utils.R"
+        )
+    ),
+    FILE_TYPES = list(
+            NGS = list(
+                EXTENSIONS = c(
+                    BAM = "\\.bam$",
+                    FASTQ = "\\.(fastq|fq)(\\.gz)?$",
+                    BIGWIG = "\\.bw$",
+                    BED = "\\.bed$",
+                    NARROWPEAK = "\\.narrowPeak$",
+                    MOTIF = "\\.(meme|pwm|jaspar)$"
+                ),
+                REQUIRED_INDEX = c(
+                    BAM = "\\.bai$",
+                    BIGWIG = "\\.bw\\.tbi$"
+                )
+            )
+    ),
+    VALIDATION = list(
+         LIMITS = list(
+                CHROMOSOME = c(min = 1, max = 16),
+                READ_LENGTH = c(min = 20, max = 150),
+                PEAK_SCORE = c(min = 0, max = 1000)
+            ),
+            REQUIRED_DIRS = c(
+                "peak",
+                "alignment",
+                "plots",
+                "documentation",
+                "fastq",
+                "processedFastq"
+            )
+        )
+)
+
+
+#    PATTERNS = list(
+#        R_FILES = "\\.R$",
+#        EXCLUDE = c("^\\.", "^_", "test_", "example_")
+#    ),
+#
+#    ENVIRONMENT = list(
+#        REQUIRED_VARS = c(
+#            "HOME",
+#            "R_LIBS_USER"
+#        ),
+#        RENV_SETTINGS = list(
+#            AUTO_ACTIVATE = TRUE,
+#            SNAPSHOT_INTERVAL = 86400  # 24 hours
diff --git a/R/config/modules/quality_control_config.R b/R/config/modules/quality_control_config.R
new file mode 100644
index 0000000..287d7b6
--- /dev/null
+++ b/R/config/modules/quality_control_config.R
@@ -0,0 +1,53 @@
+#' Add to existing QC configurations
+CONFIG <- list(
+    FASTQC = list(
+        PATTERNS = list(
+            DATA_FILE = "fastqc_data",
+            MODULE_START = "^>>",
+            MODULE_END = ">>END_MODULE",
+            HEADER = "^#"
+        ),
+        
+        OUTPUT = list(
+            DATE_FORMAT = "%Y-%m-%d-%H-%M-%S",
+            SEPARATOR = "\t",
+            EXTENSIONS = list(
+                MODULE = ".tab",
+                SUMMARY = "_summary.tab"
+            )
+        ),
+        
+        COLUMNS = list(
+            SUMMARY = c("Stat", "Value")
+        )
+    ),
+    
+    PATHS = list(
+        BASE_DIR = file.path(Sys.getenv("HOME"), "data"),
+        QC_SUBDIR = "qualityControl"
+    )
+)
+#' Add to existing QC configurations
+CONFIG <- list(
+    BAM_QC = list(
+        PATTERNS = list(
+            FLAGSTAT = "bamFlagstat",
+            SAMPLE_INFO = "sample_info"
+        ),
+        
+        METRICS = list(
+            TOTAL_READS = "total.*reads",
+            MAPPED_READS = "^mapped$"
+        ),
+        
+        OUTPUT = list(
+            DATE_FORMAT = "%Y-%m-%d-%H-%M-%S",
+            SEPARATOR = "\t"
+        )
+    ),
+    
+    PATHS = list(
+        QC_DIR = "qualityControl",
+        DOC_DIR = "documentation"
+    )
+)
diff --git a/R/config/modules/sample_config.R b/R/config/modules/sample_config.R
new file mode 100644
index 0000000..a6d7a37
--- /dev/null
+++ b/R/config/modules/sample_config.R
@@ -0,0 +1,23 @@
+#' Add to existing configurations
+CONFIG <- list(
+    PATHS = list(
+        SUBDIRS = list(
+            ALIGNMENT = "alignment",
+            BIGWIG = "bigwig"
+        )
+    ),
+    
+    FILES = list(
+        PATTERNS = list(
+            BAM = ".bam$",
+            REFERENCE = "S288C"
+        )
+    ),
+    
+    VALIDATION = list(
+        REQUIRED_COLUMNS = c(
+            "sample_ID",
+            "short_name"
+        )
+    )
+)
diff --git a/R/config/modules/visualization_config.R b/R/config/modules/visualization_config.R
new file mode 100644
index 0000000..abac767
--- /dev/null
+++ b/R/config/modules/visualization_config.R
@@ -0,0 +1,17 @@
+#' Add to existing visualization configurations
+CONFIG <- list(
+    MODES = list(
+        INTERACTIVE = list(
+            DEFAULT_DIR = "240808Bel",
+            DEFAULT_CHROMOSOME = 10
+        )
+    ),
+    
+    SYNC = list(
+        COMMAND = "rsync -nav %s:%s/%s/plots/* %s/%s/plots/",
+        USER = "username",
+        DOMAIN = "domain",
+        REMOTE_BASE = "~/data",
+        LOCAL_BASE = "/local/dir"
+    )
+)
diff --git a/code_management/000_createFileWithName.sh b/R/core/file_ops.sh
similarity index 100%
rename from code_management/000_createFileWithName.sh
rename to R/core/file_ops.sh
diff --git a/R/core/lock.R b/R/core/lock.R
new file mode 100644
index 0000000..c266403
--- /dev/null
+++ b/R/core/lock.R
@@ -0,0 +1,38 @@
+# R/core/lock.R
+
+#' Acquire File Lock
+#' @param file_path Character Path to file
+#' @param timeout Integer Seconds to wait
+#' @return Logical TRUE if lock acquired
+#' @export
+acquire_lock <- function(
+    file_path,
+    timeout = CORE_CONFIG$SYSTEM$LOCK_TIMEOUT
+) {
+    lock_file <- paste0(file_path, ".lock")
+    start_time <- Sys.time()
+    
+    while (difftime(Sys.time(), start_time, units = "secs") < timeout) {
+        if (!file.exists(lock_file)) {
+            # Create lock file
+            tryCatch({
+                file.create(lock_file)
+                writeLines(as.character(Sys.getpid()), lock_file)
+                return(TRUE)
+            }, error = function(e) NULL)
+        }
+        Sys.sleep(0.1)
+    }
+    return(FALSE)
+}
+
+#' Release File Lock
+#' @param file_path Character Path to file
+#' @return Logical TRUE if successful
+release_lock <- function(file_path) {
+    lock_file <- paste0(file_path, ".lock")
+    if (file.exists(lock_file)) {
+        unlink(lock_file)
+    }
+    return(!file.exists(lock_file))
+}
diff --git a/R/core/logging.R b/R/core/logging.R
new file mode 100644
index 0000000..bc2be44
--- /dev/null
+++ b/R/core/logging.R
@@ -0,0 +1,180 @@
+#!/usr/bin/env Rscript
+
+#' Logging Utilities
+#' @description Centralized logging system for R scripts
+#' @export
+
+
+
+#' Initialize Logging System
+#' @param script_name Character Script identifier
+#' @param log_dir Character Optional log directory
+#' @return Character Path to log file
+initialize_logging <- function(
+    script_name = basename(sys.frame(1)$ofile),
+    log_dir = file.path(CORE_CONFIG$PATHS$LOGS, format(Sys.Date(), "%Y-%m"))
+) {
+    tryCatch({
+
+        stopifnot(
+            "Context must be character" = is.character(script_name),
+            "Log directory must be character" = is.character(log_dir)
+        )
+        # Create log directory
+        dir.create(log_dir, recursive = TRUE, showWarnings = FALSE)
+        
+        # Set log file path
+        log_file <- file.path(
+            log_dir,
+            sprintf(
+                "%s_%s.log",
+                format(Sys.Date(), "%Y-%m-%d"),
+                script_name
+            )
+        )
+        
+        # Initialize with atomic write
+        if (!file.exists(log_file)) {
+            if (acquire_lock(log_file)) {
+                writeLines(
+                    c(
+                        sprintf("=== Log Started: %s ===", format(Sys.time())),
+                        sprintf("Script: %s", script_name),
+                        sprintf("User: %s", Sys.info()[["user"]]),
+                        sprintf("R Version: %s", R.version.string)
+                    ),
+                    log_file
+                )
+                release_lock(log_file)
+            }
+        }
+        
+        return(log_file)
+        
+    }, error = function(e) {
+        stop(sprintf("Logging initialization failed: %s", e$message))
+    })
+}
+
+
+#' Write Log Message
+#' @param level Character Log level
+#' @param message Character Message to log
+#' @param log_file Character Path to log file
+#' @return Logical TRUE if successful
+write_log_message <- function(
+    level,
+    message,
+    log_file = NULL
+) {
+    
+    # Validate inputs
+    stopifnot(
+        "Level must be in CORE_CONFIG$LOGGING$LEVELS" = 
+            level %in% CORE_CONFIG$LOGGING$LEVELS,
+        "Message must be character" = is.character(message),
+        "Log file must be character" = is.character(log_file) || is.null(log_file)
+    )
+    # Format message
+    entry <- sprintf(
+        "[%s] [%s] %s",
+        format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
+        level,
+        message
+    )
+    
+    # Console output
+    cat(entry, "\n", file = stderr())
+    
+    # File output with lock
+    if (!is.null(log_file)) {
+        if (acquire_lock(log_file)) {
+            write(entry, file = log_file, append = TRUE)
+            release_lock(log_file)
+            return(TRUE)
+        }
+        return(FALSE)
+    }
+    
+    return(TRUE)
+}
+
+#' Check Log Status
+#' @param log_file Character Path to log file
+#' @return List Log status information
+check_log_status <- function(log_file) {
+    list(
+        exists = file.exists(log_file),
+        size = if (file.exists(log_file)) file.info(log_file)$size else 0,
+        last_modified = if (file.exists(log_file)) 
+            format(file.info(log_file)$mtime, "%Y-%m-%d %H:%M:%S") else NA
+    )
+}
+
+
+#' Log System Information
+#' @param log_file Character Path to log file
+#' @return None
+log_system_info <- function(log_file) {
+    write_log_message(
+        level = "INFO",
+        message = paste(
+            "System Info:",
+            "R", R.version.string,
+            "on", paste(Sys.info()[c("sysname", "release")], collapse = " "),
+            "Platform:", sessionInfo()$platform
+        ),
+        log_file = log_file
+    )
+}
+
+#' Log Git Information
+#' @param log_file Character Path to log file
+#' @return None
+log_git_info <- function(log_file) {
+    tryCatch({
+        git_branch <- system("git rev-parse --abbrev-ref HEAD", intern = TRUE)
+        git_hash <- system("git rev-parse HEAD", intern = TRUE)
+        write_log_message(
+            level = "INFO",
+            message = sprintf("Git: branch=%s commit=%s", git_branch, git_hash),
+            log_file = log_file
+        )
+    }, error = function(e) {
+        write_log_message(
+            level = "WARNING",
+            message = "Git information unavailable",
+            log_file = log_file
+        )
+    })
+}
+
+
+#' Convenience logging functions
+#' @param message Character Message to log
+#' @param log_file Character Path to log file
+#' @return None
+write_log_trace <- function(message, log_file = NULL) {
+    write_log_message("TRACE", message, log_file)
+}
+
+write_log_debug <- function(message, log_file = NULL) {
+    write_log_message("DEBUG", message, log_file)
+}
+
+write_log_info <- function(message, log_file = NULL) {
+    write_log_message("INFO", message, log_file)
+}
+
+write_log_warning <- function(message, log_file = NULL) {
+    write_log_message("WARNING", message, log_file)
+}
+
+write_log_error <- function(message, log_file = NULL) {
+    write_log_message("ERROR", message, log_file)
+}
+
+write_log_fatal <- function(message, log_file = NULL) {
+    write_log_message("FATAL", message, log_file)
+    stop(message)
+}
diff --git a/R/modules/003_table_operations.R b/R/modules/003_table_operations.R
new file mode 100644
index 0000000..e028a09
--- /dev/null
+++ b/R/modules/003_table_operations.R
@@ -0,0 +1,163 @@
+source("~/lab_utils/R/functions/001_logging.R")
+source("~/lab_utils/R/init.R")
+library(assertthat)
+
+sort_columns <- function(df, column_sort_order) {
+    # Input validation
+    if(!setequal(column_sort_order, colnames(df))) {
+        log_error("Column must be sorted using all columns to ensure proper order.")
+        stop("Modify column_sort_order variable appropriately.")
+    }
+    # Create a list of sorting criteria
+    sort_criteria <- lapply(column_sort_order, function(col) df[[col]])
+    # Sort the dataframe
+    df[do.call(order, sort_criteria), ]
+}
+
+add_sample_names_to_table <- function(df) {
+     df$full_name <- apply(df, 1, paste, collapse = "_")
+     colnames_sans_fullname <- !grepl("full_name", colnames(df))
+     df_sans_fullname <- df[, colnames_sans_fullname]
+     df$short_name <- apply(df_sans_fullname, 1, function(row) paste0(substr(row, 1, 1), collapse = ""))
+     return(df)
+ }
+
+verify_expected_number_of_samples <- function(df, expected_number_of_samples) {
+    # Input validation
+    assert_that(is.numeric(expected_number_of_samples), "expected_number_of_samples must be a numeric.")
+    if(!nrow(df) == expected_number_of_samples) {
+        log_error("Combinations grid does not contain the expected_number_of_samples.")
+        log_info("Elements of sample_table:\n")
+        print(df)
+        log_info("Dimensions of sample_table:\n")
+        print(dim(df))
+        log_info("Breakdown by antibody:\n")
+        print(table(df$antibody))
+        stop("Update the impossible_settings and experiment_conditions variable.\nEnsure that it matches configuration_settings$expected_number_of_samples.")
+    }
+}
+
+add_comparisons <- function(df, comparison_list) {
+    log_info("Adding columns with comparison values\n")
+
+    # Input validation
+    if (!is.data.frame(df)) {
+        stop("Input 'df' must be a data.frame", call. = FALSE)
+    }
+    if (!is.list(comparison_list) || length(comparison_list) == 0) {
+        stop("Input 'comparison_list' must be a non-empty list", call. = FALSE)
+    }
+
+
+    ## Check if all comparison expressions are valid
+    #invalid_comps <- sapply(comparison_list, function(comp) !is.language(comp))
+    #if (any(invalid_comps)) {
+    #    stop("Invalid comparison expressions: ", 
+    #         paste(names(comparison_list)[invalid_comps], collapse = ", "), 
+    #         call. = FALSE)
+    #}
+    #                                                                                                       
+    ## Get all unique column names referenced in comparisons
+    #all_cols <- unique(unlist(lapply(comparison_list, all.vars)))
+    #missing_cols <- setdiff(all_cols, names(df))
+    #if (length(missing_cols) > 0) {
+    #    stop("Columns referenced in comparisons but not in dataframe: ", 
+    #         paste(missing_cols, collapse = ", "), 
+    #         call. = FALSE)
+    #}
+    #                                                                                                       
+    ## Performance optimization: pre-allocate result list
+    #results <- vector("list", length(comparison_list))
+    #names(results) <- names(comparison_list)
+    #                                                                                                       
+    ## Evaluate comparisons
+    #for (comp_name in names(comparison_list)) {
+    #    tryCatch({
+    #        log_info(paste("Evaluating comparison:", comp_name))
+    #        results[[comp_name]] <- eval(comparison_list[[comp_name]], df)
+    #        if (!is.logical(results[[comp_name]])) {
+    #            stop("Comparison result must be logical", call. = FALSE)
+    #        }
+    #        if (length(results[[comp_name]]) != nrow(df)) {
+    #            stop("Comparison result length does not match number of rows in dataframe", call. = FALSE)
+    #        }
+    #    }, error = function(e) {
+    #        stop(paste("Error in comparison", comp_name, ":", e$message), call. = FALSE)
+    #    })
+    #}
+    #                                                                                                       
+    ## Add results to dataframe
+    #df[names(results)] <- results
+    # For all comparisons, create column with that name and TRUE/FALSE values for rows.
+    for(comp_name in names(comparison_list)) {
+        df[[comp_name]] <- eval(comparison_list[[comp_name]], df)
+
+    }
+    return(df)
+}
+
+# Function to add new comparisons easily
+add_new_comparison <- function(df, name, condition) {
+    df[[name]] <- eval(condition, df)
+    return(df)
+}
+
+add_attributes <- function(df, control_factors) {
+    cat("Adding attributes to column names\n")
+    control_columns <- unlist(unname(control_factors))
+    at_least_one_not_in_df_column <- !all(control_columns %in% colnames(df))
+    if(at_least_one_not_in_df_column){
+        control_column_not_in_df <- which(!(control_columns %in% colnames(df)))
+        log_error("One of the control factors is in the dataframe.")
+        stop("Verify the columns in categories and control_factors list to ensure you are assigning correctly\n")
+    }
+
+    for (factor in names(control_factors)) {
+        new_column_name <- paste0("X__cf_", factor)
+        df[[new_column_name]] <- paste(control_factors[[factor]], collapse = ",")
+    }
+    return(df)
+
+}
+
+create_bmc_table <- function(named_samples_table) {
+    cat("Making bmc_table from sample table\n")
+    bmc_table <- data.frame(SampleName = named_samples_table$full_name,
+       Vol..uL = 10,
+       Conc = 0,
+       Type = "ChIP",
+       Genome = "Saccharomyces cerevisiae",
+       Notes = ifelse(named_samples_table$antibody == "Input", "Run on fragment analyzer.", "Run on femto pulse."),
+       Pool = "A"
+    )
+    return(bmc_table)
+}
+
+table_has_ID_column <- function(sample_table){
+    if(!("sample_ID" %in% colnames(sample_table))){
+        cat("No sample_ID column found.\n")
+        cat("Must determine sample_IDs from fastq files\n")
+        return(FALSE)
+    } else {
+        cat("Table has sample_ID column.\n")
+        return(TRUE)
+    }
+}
+modify_and_output_table <- function(sample_table, sample_ID_array, output_file_path) {
+    if(nrow(sample_table) != length(sample_ID_array)) {
+        cat("Number of rows is different from length of sample_ID_array.\n")
+        cat("Verify fastq file names to ensure proper number is being extracted.\n")
+        cat(sprintf("Number of rows: %s\n", nrow(sample_table)))
+        cat(sprintf("Length of array: %s\n", length(sample_ID_array)))
+        stop()
+    } else if ("sample_ID" %in% colnames(sample_table)) {
+        cat("sample_ID already part of the sample table.\n")
+        print(colnames(sample_table))
+        stop()
+    } else {
+        sample_table$sample_ID <- sample_ID_array
+        print(head(sample_table))
+        write.table(sample_table, output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
+        cat("Output modified sample table with sample ID column.\n")
+    }
+}
diff --git a/R/modules/bam_finder.R b/R/modules/bam_finder.R
new file mode 100644
index 0000000..a6eb489
--- /dev/null
+++ b/R/modules/bam_finder.R
@@ -0,0 +1,51 @@
+#' BAM File Management Functions
+find_sample_bam <- function(sample_info,
+                           bam_dir,
+                           reference_pattern = CONFIG$FILES$PATTERNS$REFERENCE) {
+    log_info("Finding BAM file for:", sample_info$short_name)
+    
+    bam_files <- find_matching_bams(
+        sample_info$sample_ID,
+        bam_dir
+    )
+    
+    if (length(bam_files) == 0) {
+        log_warning("No BAM files found for:", sample_info$sample_ID)
+        return(NULL)
+    }
+    
+    # Filter for reference genome
+    ref_files <- filter_reference_bams(
+        bam_files,
+        reference_pattern
+    )
+    
+    if (length(ref_files) == 0) {
+        log_warning("No reference BAM files found for:", sample_info$sample_ID)
+        return(NULL)
+    }
+    
+    ref_files[1]
+}
+
+find_matching_bams <- function(sample_id, bam_dir) {
+    pattern <- sprintf(".*%s.*%s", sample_id, CONFIG$FILES$PATTERNS$BAM)
+    list.files(bam_dir, pattern = pattern, full.names = TRUE)
+}
+
+filter_reference_bams <- function(bam_files,
+                                reference_pattern) {
+    bam_files[grepl(reference_pattern, bam_files)]
+}
+
+validate_bam_file <- function(file_path, sample_info) {
+    if (!file.exists(file_path)) {
+        log_warning(sprintf(
+            "BAM file not found for sample %s (%s)",
+            sample_info$short_name,
+            sample_info$sample_ID
+        ))
+        return(FALSE)
+    }
+    return(TRUE)
+}
diff --git a/R/modules/bam_qc_analyzer.R b/R/modules/bam_qc_analyzer.R
new file mode 100644
index 0000000..1bdb293
--- /dev/null
+++ b/R/modules/bam_qc_analyzer.R
@@ -0,0 +1,66 @@
+#' BAM QC Analysis Functions
+analyze_bam_qc <- function(directory,
+                          config = CONFIG$BAM_QC) {
+    log_info("Analyzing BAM QC for:", directory)
+    
+    # Get flagstat files
+    flagstat_files <- find_flagstat_files(directory)
+    
+    if (length(flagstat_files) == 0) {
+        log_warning("No flagstat files found")
+        return(NULL)
+    }
+    
+    # Get genome names
+    genome_names <- extract_genome_names(flagstat_files)
+    
+    # Get sample information
+    sample_info <- load_sample_info(directory)
+    
+    # Calculate mapping percentages
+    mapping_stats <- calculate_mapping_stats(
+        flagstat_files,
+        sample_info,
+        genome_names
+    )
+    
+    mapping_stats
+}
+
+find_flagstat_files <- function(directory) {
+    pattern <- CONFIG$BAM_QC$PATTERNS$FLAGSTAT
+    qc_dir <- file.path(directory, CONFIG$PATHS$QC_DIR)
+    
+    list.files(
+        qc_dir,
+        pattern = pattern,
+        recursive = FALSE,
+        full.names = TRUE
+    )
+}
+
+extract_genome_names <- function(files) {
+    unique(sapply(files, function(path) {
+        parts <- strsplit(basename(path), "_")[[1]]
+        parts[length(parts) - 1]
+    }))
+}
+
+load_sample_info <- function(directory) {
+    pattern <- CONFIG$BAM_QC$PATTERNS$SAMPLE_INFO
+    doc_dir <- file.path(directory, CONFIG$PATHS$DOC_DIR)
+    
+    info_file <- list.files(
+        doc_dir,
+        pattern = pattern,
+        recursive = FALSE,
+        full.names = TRUE
+    )[1]
+    
+    if (is.na(info_file)) {
+        log_error("Sample info file not found")
+        return(NULL)
+    }
+    
+    read.table(info_file, sep = ",", header = TRUE)
+}
diff --git a/R/modules/bigwig_processor.R b/R/modules/bigwig_processor.R
new file mode 100644
index 0000000..6f6229d
--- /dev/null
+++ b/R/modules/bigwig_processor.R
@@ -0,0 +1,37 @@
+#' BigWig File Processing Functions
+find_bigwig_file <- function(directory,
+                           sample_id,
+                           pattern) {
+    log_info("Finding bigwig file for:", sample_id)
+    
+    matches <- list.files(
+        directory,
+        pattern = sample_id,
+        full.names = TRUE,
+        recursive = TRUE
+    )
+    
+    bigwig_files <- matches[grepl(pattern, matches)]
+    
+    if (length(bigwig_files) == 0) {
+        log_warning("No bigwig file found for:", sample_id)
+        return(NULL)
+    }
+    
+    if (length(bigwig_files) > 1) {
+        log_warning("Multiple bigwig files found, using first")
+    }
+    
+    bigwig_files[1]
+}
+
+import_bigwig_data <- function(file_path, region) {
+    log_info("Importing bigwig data from:", file_path)
+    
+    tryCatch({
+        import(con = file_path, which = region)
+    }, error = function(e) {
+        log_error("Failed to import bigwig:", e$message)
+        NULL
+    })
+}
diff --git a/R/modules/chromosome_converter.R b/R/modules/chromosome_converter.R
new file mode 100644
index 0000000..7a1b02f
--- /dev/null
+++ b/R/modules/chromosome_converter.R
@@ -0,0 +1,53 @@
+#' Chromosome Name Conversion Functions
+convert_chromosome_names <- function(names,
+                                   target_style,
+                                   config = CONFIG$CHROMOSOMES) {
+    log_info("Converting chromosome names to style:", target_style)
+    
+    vapply(
+        names,
+        function(chr) convert_single_chromosome(chr, target_style, config),
+        character(1)
+    )
+}
+
+convert_single_chromosome <- function(chr,
+                                    target_style,
+                                    config = CONFIG$CHROMOSOMES) {
+    # Remove existing prefix if present
+    chr <- gsub(CONFIG$NAMING$PATTERNS$PREFIX, "", chr, 
+                ignore.case = TRUE)
+    
+    # Handle special chromosomes
+    if (chr %in% config$SPECIAL) {
+        return(paste0(config$PREFIX, chr))
+    }
+    
+    # Convert based on current format
+    if (grepl(CONFIG$NAMING$PATTERNS$ROMAN, chr)) {
+        return(handle_roman_chromosome(chr, target_style))
+    } else if (grepl(CONFIG$NAMING$PATTERNS$NUMERIC, chr)) {
+        return(handle_numeric_chromosome(chr, target_style))
+    }
+    
+    log_warning("Unable to convert chromosome:", chr)
+    return(paste0(config$PREFIX, chr))
+}
+
+handle_roman_chromosome <- function(chr, target_style) {
+    switch(target_style,
+        "ENSEMBL" = as.character(arabic(chr)),
+        "UCSC" = paste0(CONFIG$CHROMOSOMES$PREFIX, chr),
+        "NCBI" = chr,
+        stop("Unknown target style:", target_style)
+    )
+}
+
+handle_numeric_chromosome <- function(chr, target_style) {
+    switch(target_style,
+        "ENSEMBL" = chr,
+        "UCSC" = paste0(CONFIG$CHROMOSOMES$PREFIX, as.roman(as.integer(chr))),
+        "NCBI" = as.roman(as.integer(chr)),
+        stop("Unknown target style:", target_style)
+    )
+}
diff --git a/R/modules/control_handler.R b/R/modules/control_handler.R
new file mode 100644
index 0000000..edbf40d
--- /dev/null
+++ b/R/modules/control_handler.R
@@ -0,0 +1,54 @@
+#' Control Sample Management Functions
+find_control_sample <- function(sample_row,
+                              sample_table,
+                              factors,
+                              bigwig_dir,
+                              pattern) {
+    log_info("Finding control sample")
+    
+    control_index <- determine_matching_control(
+        sample_row,
+        sample_table,
+        factors
+    )
+    
+    if (length(control_index) == 0) {
+        log_warning("No matching control found, using fallback")
+        return(find_fallback_control(sample_table, bigwig_dir, pattern))
+    }
+    
+    control_data <- get_control_data(
+        sample_table[control_index, ],
+        bigwig_dir,
+        pattern
+    )
+    
+    if (is.null(control_data)) {
+        log_warning("Control data not found, using fallback")
+        return(find_fallback_control(sample_table, bigwig_dir, pattern))
+    }
+    
+    control_data
+}
+
+get_control_data <- function(control_sample,
+                           bigwig_dir,
+                           pattern) {
+    log_info("Getting control data for:", control_sample$sample_ID)
+    
+    bigwig_file <- find_bigwig_file(
+        bigwig_dir,
+        control_sample$sample_ID,
+        pattern
+    )
+    
+    if (is.null(bigwig_file)) {
+        return(NULL)
+    }
+    
+    list(
+        id = control_sample$sample_ID,
+        name = control_sample$short_name,
+        file = bigwig_file
+    )
+}
diff --git a/R/modules/control_manager.sh b/R/modules/control_manager.sh
new file mode 100644
index 0000000..3254c7d
--- /dev/null
+++ b/R/modules/control_manager.sh
@@ -0,0 +1,79 @@
+#' Enhanced control sample management
+find_valid_control <- function(sample_row,
+                             sample_table,
+                             bam_dir,
+                             factors) {
+    log_info("Finding valid control sample")
+    
+    # Find matching control
+    control_index <- determine_matching_control(
+        sample_row,
+        sample_table,
+        factors
+    )
+    
+    # Validate and possibly adjust control
+    control_index <- validate_control_index(
+        control_index,
+        sample_table,
+        bam_dir
+    )
+    
+    if (control_index > 0) {
+        return(get_control_bam(
+            sample_table[control_index, ],
+            bam_dir
+        ))
+    }
+    
+    log_warning("No valid control found")
+    return(NULL)
+}
+
+validate_control_index <- function(index,
+                                 sample_table,
+                                 bam_dir) {
+    if (length(index) == 0) {
+        log_warning("No matching control found, using fallback")
+        return(find_fallback_control(sample_table, bam_dir))
+    }
+    
+    if (length(index) > CONFIG$CONTROL$MAX_CONTROLS) {
+        log_warning("Multiple controls found, using first")
+        index <- index[1]
+    }
+    
+    # Validate BAM file exists
+    control_bam <- find_matching_bam(
+        sample_table$sample_ID[index],
+        bam_dir
+    )
+    
+    if (!is.null(control_bam)) {
+        return(index)
+    }
+    
+    log_warning("Control BAM not found, searching for alternative")
+    return(find_fallback_control(sample_table, bam_dir))
+}
+
+find_fallback_control <- function(sample_table, bam_dir) {
+    log_info("Searching for fallback control")
+    
+    input_samples <- sample_table[
+        sample_table[[CONFIG$CONTROL$ANTIBODY_COLUMN]] == 
+        CONFIG$CONTROL$INPUT_VALUE,
+    ]
+    
+    for (i in seq_len(nrow(input_samples))) {
+        if (!is.null(find_matching_bam(
+            input_samples$sample_ID[i],
+            bam_dir
+        ))) {
+            return(which(sample_table$sample_ID == 
+                        input_samples$sample_ID[i]))
+        }
+    }
+    
+    return(CONFIG$CONTROL$DEFAULT_INDEX)
+}
diff --git a/R/modules/data_converter.R b/R/modules/data_converter.R
new file mode 100644
index 0000000..39b455a
--- /dev/null
+++ b/R/modules/data_converter.R
@@ -0,0 +1,72 @@
+#' Data Conversion Functions
+convert_to_granges <- function(data,
+                             file_name,
+                             config = CONFIG$FEATURES) {
+    log_info("Converting data to GRanges")
+    
+    if (is(data, "GRanges")) {
+        return(data)
+    }
+    
+    converter <- determine_converter(file_name)
+    converter(data, file_name)
+}
+
+determine_converter <- function(file_name) {
+    for (type in names(CONFIG$FEATURES$FILE_TYPES)) {
+        if (grepl(CONFIG$FEATURES$FILE_TYPES[[type]], file_name)) {
+            return(get(paste0("convert_", tolower(type))))
+        }
+    }
+    
+    function(data, file_name) {
+        log_warning("No specific converter for:", file_name)
+        as(data, "GRanges")
+    }
+}
+
+convert_nucleosome <- function(data, file_name) {
+    log_info("Converting nucleosome data")
+    
+    config <- CONFIG$FEATURES$COLUMNS$NUCLEOSOME
+    
+    # Create metadata
+    metadata <- data[, !(colnames(data) %in% config$EXCLUDE)]
+    colnames(metadata) <- clean_column_names(colnames(metadata))
+    
+    GRanges(
+        seqnames = data$`Nucleosome ID`,
+        ranges = IRanges(
+            start = data[[config$POSITION]],
+            end = data[[config$POSITION]]
+        ),
+        strand = "*",
+        chromosome = data$Chromosome,
+        metadata
+    )
+}
+
+convert_timing <- function(data, file_name) {
+    log_info("Converting timing data")
+    
+    config <- CONFIG$FEATURES$COLUMNS$TIMING
+    window <- config$WINDOW
+    
+    # Create origin names
+    names <- paste0(data$Chromosome, "_", data$Position)
+    
+    # Create metadata
+    metadata <- data[, !(colnames(data) %in% config$EXCLUDE)]
+    colnames(metadata) <- clean_column_names(colnames(metadata))
+    
+    GRanges(
+        seqnames = names,
+        ranges = IRanges(
+            start = data$Position - window,
+            end = data$Position + window
+        ),
+        strand = "*",
+        chromosome = data$Chromosome,
+        metadata
+    )
+}
diff --git a/R/modules/data_downloader.R b/R/modules/data_downloader.R
new file mode 100644
index 0000000..24f0f77
--- /dev/null
+++ b/R/modules/data_downloader.R
@@ -0,0 +1,62 @@
+#' Data Download Functions
+download_feature_data <- function(source_config,
+                                base_dir,
+                                timestamp) {
+    log_info("Downloading feature data")
+    
+    results <- list()
+    
+    for (source in names(source_config)) {
+        result <- download_source_data(
+            source,
+            source_config[[source]],
+            base_dir,
+            timestamp
+        )
+        
+        results[[source]] <- result
+    }
+    
+    results
+}
+
+download_source_data <- function(source_name,
+                               config,
+                               base_dir,
+                               timestamp) {
+    log_info("Downloading:", source_name)
+    
+    output_file <- generate_output_path(
+        base_dir,
+        config$FILE,
+        timestamp
+    )
+    
+    # Download file
+    download_file(config$URL, output_file)
+    
+    # Process if compressed
+    if (is_compressed(output_file)) {
+        output_file <- decompress_file(output_file)
+    }
+    
+    # Validate and process
+    process_downloaded_file(
+        output_file,
+        source_name,
+        config$TYPE
+    )
+}
+
+download_file <- function(url, dest_path) {
+    log_info("Downloading:", basename(dest_path))
+    
+    tryCatch({
+        curl::curl_download(url, dest_path, mode = "wb")
+        log_info("Download complete")
+        TRUE
+    }, error = function(e) {
+        log_error("Download failed:", e$message)
+        FALSE
+    })
+}
diff --git a/R/modules/data_preparer.R b/R/modules/data_preparer.R
new file mode 100644
index 0000000..d8a687f
--- /dev/null
+++ b/R/modules/data_preparer.R
@@ -0,0 +1,49 @@
+#' Data Preparation Functions
+prepare_visualization_data <- function(directory,
+                                    chromosome = CONFIG$DEFAULTS$CHROMOSOME,
+                                    genome_dir = CONFIG$DEFAULTS$GENOME_DIR,
+                                    genome_pattern = CONFIG$DEFAULTS$GENOME_PATTERN) {
+    log_info("Preparing visualization data")
+    
+    # Validate directory
+    directory_path <- validate_input(directory)
+    
+    # Load sample table
+    log_info("Loading sample table")
+    sample_table <- load_sample_table(directory_path)
+    
+    # Load reference genome
+    log_info("Loading reference genome")
+    ref_genome <- load_reference_genome(
+        genome_dir = genome_dir,
+        genome_pattern = genome_pattern
+    )
+    
+    # Create genome range
+    log_info("Creating genome range")
+    genome_range <- create_chromosome_GRange(ref_genome)
+    
+    list(
+        directory = directory_path,
+        samples = sample_table,
+        genome = ref_genome,
+        range = genome_range
+    )
+}
+
+prepare_feature_track <- function(chromosome,
+                                genome_range,
+                                pattern = CONFIG$DEFAULTS$FEATURE_PATTERN) {
+    log_info("Preparing feature track")
+    
+    feature_data <- load_feature_file_GRange(
+        chromosome_to_plot = chromosome,
+        feature_file_pattern = pattern,
+        genomeRange_to_get = genome_range
+    )
+    
+    AnnotationTrack(
+        feature_data,
+        name = paste("Origin Peaks", "Eaton 2010", sep = "")
+    )
+}
diff --git a/R/modules/environment_utils.R b/R/modules/environment_utils.R
new file mode 100644
index 0000000..755bdd8
--- /dev/null
+++ b/R/modules/environment_utils.R
@@ -0,0 +1,126 @@
+#!/usr/bin/env Rscript
+# R/functions/environment_utils.R
+
+#' Environment Management Utilities
+#' @description Centralized environment management system
+#' @export
+
+#' Validate System Environment
+#' @param config List Configuration settings
+#' @param min_memory Numeric Minimum required memory in GB
+#' @return List Environment status
+validate_system_environment <- function(
+    config = PROJECT_CONFIG,
+    min_memory = 4
+) {
+    tryCatch({
+        # Check R version
+        check_r_version(
+            R_system_version = getRversion(),
+            R_project_version = package_version(config$SYSTEM$R_MIN_VERSION)
+        )
+        
+        # Check memory
+        check_system_memory(
+            min_memory = min_memory,
+            current_memory = memory.limit()
+        )
+        
+        # Check environment variables
+        check_environment_variables(
+            required_vars = config$ENVIRONMENT$REQUIRED_VARS,
+            current_vars = names(Sys.getenv())
+        )
+        
+        # Log system info
+        log_system_info()
+        
+        invisible(TRUE)
+    }, error = function(e) {
+        log_error("Environment validation failed:", e$message)
+        stop(e)
+    })
+}
+
+#' Check System Memory
+#' @param min_memory Numeric Required memory in GB
+#' @param current_memory Numeric Available memory in MB
+#' @return Logical TRUE if check passes
+check_system_memory <- function(
+    min_memory,
+    current_memory
+) {
+    if (current_memory < min_memory * 1024) {
+        log_warning(sprintf(
+            "Low memory: %d MB available, %d GB required",
+            current_memory,
+            min_memory
+        ))
+    }
+    invisible(TRUE)
+}
+
+#' Check Environment Variables
+#' @param required_vars Character vector Required variables
+#' @param current_vars Character vector Current variables
+#' @return Logical TRUE if check passes
+check_environment_variables <- function(
+    required_vars,
+    current_vars
+) {
+    missing_vars <- setdiff(required_vars, current_vars)
+    if (length(missing_vars) > 0) {
+        stop(sprintf(
+            "Missing environment variables: %s",
+            paste(missing_vars, collapse = ", ")
+        ))
+    }
+    invisible(TRUE)
+}
+
+#' Setup Project Paths
+#' @param config List Configuration settings
+#' @return List Path information
+setup_project_paths <- function(
+    config = PROJECT_CONFIG
+) {
+    tryCatch({
+        # Validate base paths
+        for (path in c(config$PATHS$ROOT, "~/logs")) {
+            if (!dir.exists(path)) {
+                dir.create(path, recursive = TRUE)
+            }
+        }
+        
+        # Return path information
+        list(
+            root = normalizePath(config$PATHS$ROOT),
+            logs = normalizePath("~/logs"),
+            data = if (dir.exists("~/data")) normalizePath("~/data") else NULL
+        )
+    }, error = function(e) {
+        log_error("Path setup failed:", e$message)
+        stop(e)
+    })
+}
+
+#' Clean R Environment
+#' @param keep Character vector Names to keep
+#' @return None
+clean_environment <- function(
+    keep = c("PROJECT_CONFIG")
+) {
+    # Detach packages
+    attached <- paste0("package:", names(sessionInfo()$otherPkgs))
+    for (pkg in attached) {
+        try(detach(pkg, character.only = TRUE, unload = TRUE), silent = TRUE)
+    }
+    
+    # Clear workspace except kept objects
+    rm(list = setdiff(ls(all.names = TRUE), keep), envir = .GlobalEnv)
+    
+    # Force garbage collection
+    gc()
+    
+    invisible(TRUE)
+}
diff --git a/R/modules/feature_processor.R b/R/modules/feature_processor.R
new file mode 100644
index 0000000..5fb464a
--- /dev/null
+++ b/R/modules/feature_processor.R
@@ -0,0 +1,190 @@
+#' Feature File Processing Functions
+load_feature_file_GRange <- function(chromosome_to_plot = 10,
+                                   feature_file_pattern = CONFIG$FEATURE_TYPES$PEAKS,
+                                   genomeRange_to_get) {
+    log_info("Loading feature file:", feature_file_pattern)
+    
+    # Validate directory
+    feature_dir <- CONFIG$PATHS$FEATURE_DIR
+    if (!dir.exists(feature_dir)) {
+        log_error("Feature directory not found:", feature_dir)
+        stop("Missing feature directory")
+    }
+    
+    # Find feature file
+    feature_files <- list.files(feature_dir,
+                              pattern = feature_file_pattern,
+                              full.names = TRUE,
+                              recursive = TRUE)
+    
+    if (length(feature_files) != 1) {
+        log_error("Invalid number of feature files:", length(feature_files))
+        stop("Feature file error")
+    }
+    
+    # Load and process features
+    feature_grange <- tryCatch({
+        import.bed(feature_files[1])
+    }, error = function(e) {
+        log_error("Failed to import feature file:", e$message)
+        stop("Import error")
+    })
+    
+    # Process chromosome styles
+    feature_style <- determine_chr_style(seqlevels(feature_grange))
+    genome_style <- determine_chr_style(seqlevels(genomeRange_to_get))
+    
+    log_info("Feature style:", feature_style)
+    log_info("Genome style:", genome_style)
+    
+    # Normalize chromosome styles
+    feature_grange_subset <- normalize_feature_range(
+        feature_grange,
+        genomeRange_to_get,
+        feature_style,
+        genome_style
+    )
+    
+    return(feature_grange_subset)
+}
+
+normalize_feature_range <- function(feature_grange,
+                                  genome_range,
+                                  feature_style,
+                                  genome_style) {
+    if (feature_style == genome_style) {
+        log_info("Styles match, using direct subsetting")
+        return(subsetByOverlaps(feature_grange, genome_range))
+    }
+    
+    log_info("Adjusting styles for compatibility")
+    
+    # Adjust genome range to match feature style
+    adjusted_genome_range <- genome_range
+    new_seqlevels <- normalize_chr_names(seqlevels(genome_range),
+                                       feature_style)
+    seqlevels(adjusted_genome_range) <- new_seqlevels
+    
+    # Subset features
+    feature_subset <- subsetByOverlaps(feature_grange,
+                                     adjusted_genome_range)
+    
+    # Convert back to genome style
+    final_seqlevels <- normalize_chr_names(seqlevels(feature_subset),
+                                         genome_style)
+    seqlevels(feature_subset) <- final_seqlevels
+    
+    return(feature_subset)
+}
+#' Feature File Processing Functions
+process_feature_files <- function(input_dir = CONFIG$FEATURES$PATHS$BASE_DIR) {
+    log_info("Processing feature files")
+    
+    # Validate directory
+    validate_directory(input_dir)
+    
+    # Get file list
+    files <- get_feature_files(input_dir)
+    
+    if (length(files) == 0) {
+        log_warning("No files to process")
+        return(NULL)
+    }
+    
+    # Process each file
+    results <- process_files(files)
+    
+    # Verify outputs
+    verify_outputs(input_dir)
+    
+    results
+}
+
+process_files <- function(files) {
+    log_info("Processing", length(files), "files")
+    
+    lapply(files, function(file) {
+        tryCatch({
+            process_single_file(file)
+        }, error = function(e) {
+            log_error("Failed to process:", basename(file))
+            log_error("Error:", e$message)
+            NULL
+        })
+    })
+}
+
+process_single_file <- function(file_path) {
+    log_info("Processing file:", basename(file_path))
+    
+    # Read data
+    data <- read_feature_file(file_path)
+    
+    # Process data
+    processed <- process_feature_data(data, basename(file_path))
+    
+    # Convert to GRanges
+    granges <- convert_to_granges(processed, basename(file_path))
+    
+    # Output results
+    output_results(granges, file_path)
+    
+    granges
+}
+#' Feature Data Processing Functions
+process_downloaded_file <- function(file_path,
+                                  source_name,
+                                  type) {
+    log_info("Processing file:", basename(file_path))
+    
+    # Read file
+    data <- read_feature_file(file_path)
+    
+    # Process based on type
+    processed <- switch(type,
+        "features" = process_sgd_features(data),
+        "acs" = process_eaton_acs(data, file_path),
+        process_generic_features(data)
+    )
+    
+    validate_processed_data(processed, type)
+}
+
+process_sgd_features <- function(data) {
+    log_info("Processing SGD features")
+    
+    if (ncol(data) != length(CONFIG$SGD_COLUMNS)) {
+        log_error("Invalid column count in SGD features")
+        return(NULL)
+    }
+    
+    names(data) <- CONFIG$SGD_COLUMNS
+    data
+}
+
+process_eaton_acs <- function(data, file_path) {
+    log_info("Processing Eaton ACS data")
+    
+    # Fix coordinates
+    fixed <- fix_coordinates(data)
+    
+    # Save fixed version if needed
+    if (has_invalid_coordinates(data)) {
+        save_fixed_coordinates(fixed, file_path)
+    }
+    
+    fixed
+}
+
+fix_coordinates <- function(data) {
+    log_info("Fixing coordinates")
+    
+    data %>%
+        mutate(
+            width = end - start,
+            temp = ifelse(start > end, start, end),
+            start = ifelse(start > end, end, start),
+            end = temp
+        ) %>%
+        select(-temp, -width)
+}
diff --git a/R/modules/file_operations.R b/R/modules/file_operations.R
new file mode 100644
index 0000000..873c93c
--- /dev/null
+++ b/R/modules/file_operations.R
@@ -0,0 +1,58 @@
+load_sample_table <- function(directory_path) {
+    cat("Loading sample_table from", directory_path, "\n")
+    documentation_dir_path <- file.path(directory_path, "documentation")
+    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
+    if(length(sample_table_path) == 0){
+        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path))
+        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n")
+        stop()
+    } else if(length(sample_table_path) > 1){
+        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path))
+        cat(sprintf("Files found in %s\n", documentation_dir_path))
+        print(sample_table)
+        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n")
+        stop()
+    }
+    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
+    cat(sprintf("Reading %s\n", sample_table_path))
+    cat("Head of sample_table\n")
+    print(head(sample_table))
+    return(sample_table)
+}
+
+determine_sample_id <- function(directory_path) {
+    fastq_directory_path <- file.path(directory_path, "fastq")
+    fastq_file_paths <- list.files(fastq_directory_path, pattern = "*.fastq", full.names = TRUE)
+    if(length(fastq_file_paths) == 0) {
+        cat(sprintf("No fastq files found in %s\n", fastq_directory_path))
+        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project and download files from BMC\n")
+        stop()
+    } else {
+        cat(sprintf("Found %s files in %s.\n", length(fastq_file_paths), fastq_directory_path))
+    }
+    fastq_file_names <- basename(fastq_file_paths)
+    ID_regex <- "\\d{5,6}"
+    fastq_split_string_list <- strsplit(fastq_file_names, "_|-")
+    sample_IDs <- lapply(fastq_split_string_list, function(fastq_split_string_list) {
+        for(split_string in fastq_split_string_list) {
+            if(grepl(ID_regex, split_string)) {
+                return(split_string)
+            }
+        }
+    })
+    if(!all(unlist(lapply(sample_IDs, length)) == 1)) {
+        cat("At least one of the files did not extract exactly one sample ID.\n")
+        cat("Files with problems:\n")
+        print(fastq_file_names[unlist(lapply(sample_IDs, length)) != 1])
+        cat("Verify sample names. Redownload from BMC if necessary.\n")
+        cat(sprintf("Regex pattern used %s:\n", ID_regex))
+        stop()
+    } else {
+        sample_IDs <- unlist(sample_IDs)
+        cat(sprintf("Found %s sample_IDs.\n", length(sample_IDs)))
+        cat(sprintf("First sample_ID: %s\n",sample_IDs[1]))
+        cat(sprintf("Last sample_ID: %s\n", sample_IDs[length(sample_IDs)]))
+        cat("Returning sample_ID array.\n")
+        return(sample_IDs)
+    }
+}
diff --git a/R/modules/genome_processor.R b/R/modules/genome_processor.R
new file mode 100644
index 0000000..f607063
--- /dev/null
+++ b/R/modules/genome_processor.R
@@ -0,0 +1,57 @@
+#' Genome Loading Functions
+load_reference_genome <- function(genome_dir = CONFIG$PATHS$GENOME_DIR, 
+                                genome_pattern = CONFIG$PATTERNS$GENOME) {
+    log_info("Loading reference genome")
+    
+    directory_path <- file.path(CONFIG$PATHS$BASE_DIR, genome_dir)
+    
+    if (!dir.exists(directory_path)) {
+        log_error("Genome directory not found:", directory_path)
+        stop("Missing genome directory")
+    }
+    
+    genome_files <- list.files(directory_path, pattern = genome_pattern, 
+                             full.names = TRUE, recursive = TRUE)
+    
+    if (length(genome_files) != 1) {
+        log_error("Invalid number of genome files found:", length(genome_files))
+        stop("Genome file error")
+    }
+    
+    genome <- readFasta(genome_files[1])
+    
+    genome_df <- data.frame(
+        chrom = names(as(genome, "DNAStringSet")),
+        basePairSize = width(genome)
+    ) %>% 
+        filter(chrom != "chrM")
+    
+    return(genome_df)
+}
+
+#' Chromosome Name Processing Functions
+normalize_chr_names <- function(chr_names, target_style) {
+    log_info("Normalizing chromosome names")
+    
+    if (!target_style %in% CONFIG$CHROMOSOME_MAPPING$STYLES) {
+        log_error("Invalid chromosome style:", target_style)
+        stop("Invalid style")
+    }
+    
+    chr_names <- gsub(paste0("^", CONFIG$CHROMOSOME_MAPPING$PREFIX), "", 
+                     chr_names)
+    
+    normalized <- switch(
+        target_style,
+        "UCSC" = paste0(CONFIG$CHROMOSOME_MAPPING$PREFIX, chr_names),
+        "Roman" = paste0(CONFIG$CHROMOSOME_MAPPING$PREFIX, 
+                        mapvalues(chr_names, 
+                                from = as.character(1:16),
+                                to = CONFIG$CHROMOSOME_MAPPING$ROMAN)),
+        "Numeric" = mapvalues(chr_names,
+                            from = CONFIG$CHROMOSOME_MAPPING$ROMAN,
+                            to = as.character(1:16))
+    )
+    
+    return(unname(normalized))
+}
diff --git a/R/modules/granges_converter.R b/R/modules/granges_converter.R
new file mode 100644
index 0000000..9b95f1f
--- /dev/null
+++ b/R/modules/granges_converter.R
@@ -0,0 +1,44 @@
+#' GRanges Conversion Functions
+convert_granges_style <- function(gr,
+                                target_style,
+                                verbose = FALSE) {
+    log_info("Converting GRanges object to style:", target_style)
+    
+    validate_granges(gr)
+    
+    # Get current seqnames
+    current_seqnames <- as.character(seqnames(gr))
+    if (verbose) {
+        log_info("Original seqnames:",
+                paste(unique(current_seqnames), collapse = ", "))
+    }
+    
+    # Convert names
+    new_seqnames <- convert_chromosome_names(
+        current_seqnames,
+        target_style
+    )
+    
+    if (verbose) {
+        log_info("Converted seqnames:",
+                paste(unique(new_seqnames), collapse = ", "))
+        log_info("Changes made:",
+                sum(new_seqnames != current_seqnames))
+    }
+    
+    # Update GRanges object
+    update_granges_seqnames(gr, new_seqnames)
+}
+
+validate_granges <- function(gr) {
+    if (!is(gr, "GRanges")) {
+        log_error("Invalid input type")
+        stop("Input must be a GenomicRanges object")
+    }
+}
+
+update_granges_seqnames <- function(gr, new_names) {
+    seqlevels(gr) <- unique(new_names)
+    seqnames(gr) <- new_names
+    gr
+}
diff --git a/R/modules/mapping_calculator.R b/R/modules/mapping_calculator.R
new file mode 100644
index 0000000..9c92a0e
--- /dev/null
+++ b/R/modules/mapping_calculator.R
@@ -0,0 +1,66 @@
+#' Mapping Statistics Functions
+calculate_mapping_stats <- function(flagstat_files,
+                                  sample_info,
+                                  genome_names) {
+    log_info("Calculating mapping statistics")
+    
+    # Initialize results matrix
+    results <- initialize_results_matrix(genome_names)
+    
+    # Process each sample
+    sample_ids <- get_sample_ids(sample_info)
+    
+    for (sample_id in sample_ids) {
+        stats <- process_sample_stats(
+            sample_id,
+            flagstat_files,
+            genome_names
+        )
+        
+        results <- rbind(results, stats)
+    }
+    
+    results
+}
+
+process_sample_stats <- function(sample_id,
+                               flagstat_files,
+                               genome_names) {
+    log_info("Processing sample:", sample_id)
+    
+    # Find relevant flagstat files
+    sample_files <- find_sample_flagstats(
+        sample_id,
+        flagstat_files
+    )
+    
+    # Calculate percentages
+    stats <- sapply(sample_files, function(file) {
+        calculate_mapping_percentage(file)
+    })
+    
+    # Create result row
+    result <- as.data.frame(t(stats))
+    colnames(result) <- genome_names
+    
+    result
+}
+
+calculate_mapping_percentage <- function(flagstat_file) {
+    log_info("Calculating mapping percentage for:", 
+             basename(flagstat_file))
+    
+    # Read flagstat data
+    data <- read.table(flagstat_file, sep = "\t")
+    
+    # Find relevant rows
+    patterns <- CONFIG$BAM_QC$METRICS
+    is_relevant <- Reduce("|", lapply(patterns, grepl, data[,3]))
+    subset_data <- data[is_relevant, ]
+    
+    # Calculate percentage
+    mapped <- as.numeric(subset_data[2,1])
+    total <- as.numeric(subset_data[1,1])
+    
+    (mapped / total) * 100
+}
diff --git a/R/modules/output_operations.R b/R/modules/output_operations.R
new file mode 100644
index 0000000..4b6f099
--- /dev/null
+++ b/R/modules/output_operations.R
@@ -0,0 +1,19 @@
+output_tables_in_list <- function(experiment_directory, list_of_tables, OUTPUT_TABLE = FALSE){
+        experiment_name <- basename(experiment_directory)
+        if (!(typeof(list_of_tables) == "list")){
+            stop("Argument must be a list.")
+        }
+        names_of_tables <- names(list_of_tables)
+        for (name_of_table in names_of_tables){
+            output_table <- sample_config_output[[name_of_table]]
+            cat("============\n")
+            print(head(output_table))
+            output_file_path <- file.path(experiment_directory, "documentation", paste(experiment_name, "_", name_of_table, ".tsv", sep = ""))
+            cat(sprintf("Outputting to %s: \n", output_file_path))
+            if(OUTPUT_TABLE) {
+                write.table(output_table, file = output_file_path, sep = "\t", row.names = FALSE)
+            } else {
+                cat("Skip writing table. MODIFY OUTPUT_TABLE value to output.\n")
+            }
+        }
+}
diff --git a/R/modules/package_installer.R b/R/modules/package_installer.R
new file mode 100644
index 0000000..59e967d
--- /dev/null
+++ b/R/modules/package_installer.R
@@ -0,0 +1,56 @@
+#' Package Installation Functions
+install_package_group <- function(packages,
+                                manager = "BiocManager") {
+    log_info("Installing package group using:", manager)
+    
+    results <- switch(manager,
+        "BiocManager" = install_bioc_packages(packages),
+        "renv" = install_renv_packages(packages),
+        "github" = install_github_packages(packages),
+        stop("Unknown package manager:", manager)
+    )
+    
+    failed <- names(results)[!results]
+    if (length(failed) > 0) {
+        log_warning("Failed to install packages:",
+                   paste(failed, collapse = ", "))
+    }
+    
+    invisible(results)
+}
+
+install_bioc_packages <- function(packages) {
+    sapply(packages, function(pkg) {
+        tryCatch({
+            BiocManager::install(pkg, update = FALSE)
+            TRUE
+        }, error = function(e) {
+            log_error("Failed to install:", pkg)
+            FALSE
+        })
+    })
+}
+
+install_renv_packages <- function(packages) {
+    sapply(packages, function(pkg) {
+        tryCatch({
+            renv::install(pkg)
+            TRUE
+        }, error = function(e) {
+            log_error("Failed to install:", pkg)
+            FALSE
+        })
+    })
+}
+
+install_github_packages <- function(packages) {
+    sapply(packages, function(pkg) {
+        tryCatch({
+            remotes::install_github(pkg)
+            TRUE
+        }, error = function(e) {
+            log_error("Failed to install:", pkg)
+            FALSE
+        })
+    })
+}
diff --git a/R/modules/package_manager.R b/R/modules/package_manager.R
new file mode 100644
index 0000000..4706019
--- /dev/null
+++ b/R/modules/package_manager.R
@@ -0,0 +1,104 @@
+#' Package Management Functions
+load_required_packages <- function(packages = CONFIG$PACKAGES$REQUIRED) {
+    log_info("Loading required packages")
+    
+    suppressPackageStartupMessages({
+        results <- sapply(packages, require, character.only = TRUE)
+    })
+    
+    missing <- packages[!results]
+    if (length(missing) > 0) {
+        log_error("Missing packages:", paste(missing, collapse = ", "))
+        stop("Required packages not available")
+    }
+    
+    return(TRUE)
+}
+#' Package Management Functions
+initialize_environment <- function(config = CONFIG) {
+    log_info("Initializing R environment")
+    
+    # Initialize renv
+    setup_renv()
+    
+    # Setup package management
+    setup_package_manager()
+    
+    # Install and load packages
+    install_required_packages(config$PACKAGES)
+    
+    # Snapshot environment
+    snapshot_environment()
+    
+    log_info("Environment initialization complete")
+}
+
+setup_renv <- function() {
+    log_info("Setting up renv")
+    
+    if (!requireNamespace("renv", quietly = TRUE)) {
+        log_info("Installing renv")
+        install.packages("renv")
+    }
+    
+    renv::init()
+}
+
+setup_package_manager <- function() {
+    log_info("Setting up BiocManager")
+    
+    if (!requireNamespace("BiocManager", quietly = TRUE)) {
+        log_info("Installing BiocManager")
+        install.packages("BiocManager")
+    }
+}
+
+install_required_packages <- function(package_lists) {
+    log_info("Installing required packages")
+    
+    # Combine all packages
+    all_packages <- unique(unlist(package_lists))
+    
+    # Install packages
+    result <- tryCatch({
+        BiocManager::install(all_packages)
+        TRUE
+    }, error = function(e) {
+        log_error("Package installation failed:", e$message)
+        FALSE
+    })
+    
+    if (!result) {
+        stop("Package installation failed")
+    }
+}
+
+load_packages <- function(packages) {
+    log_info("Loading packages")
+    
+    results <- sapply(packages, function(pkg) {
+        tryCatch({
+            library(pkg, character.only = TRUE)
+            TRUE
+        }, error = function(e) {
+            log_warning("Failed to load package:", pkg)
+            FALSE
+        })
+    })
+    
+    if (!all(results)) {
+        failed <- names(results)[!results]
+        log_warning("Failed to load packages:", 
+                   paste(failed, collapse = ", "))
+    }
+}
+
+snapshot_environment <- function() {
+    log_info("Creating environment snapshot")
+    
+    tryCatch({
+        renv::snapshot()
+    }, error = function(e) {
+        log_error("Failed to create snapshot:", e$message)
+    })
+}
diff --git a/R/modules/plot_generator.R b/R/modules/plot_generator.R
new file mode 100644
index 0000000..c96be32
--- /dev/null
+++ b/R/modules/plot_generator.R
@@ -0,0 +1,83 @@
+#' Plot Generation Functions
+generate_track_plot <- function(tracks,
+                              title,
+                              chromosome,
+                              output_file = NULL) {
+    log_info("Generating track plot:", title)
+    
+    if (!is.null(output_file)) {
+        svg(output_file)
+        on.exit(dev.off())
+    }
+    
+    plotTracks(
+        tracks,
+        main = title,
+        chromosome = chromosome
+    )
+}
+
+create_output_filename <- function(base_dir,
+                                 chromosome,
+                                 pattern,
+                                 comparison,
+                                 time_id = NULL) {
+    if (is.null(time_id)) {
+        time_id <- format(Sys.time(), CONFIG$PLOT$DATE_FORMAT)
+    }
+    
+    pattern_clean <- gsub("_|\\.bw", "", pattern)
+    comparison_clean <- gsub("_", "", comparison)
+    
+    file.path(
+        base_dir,
+        sprintf(
+            "%s_%s_%s_%s_%s.svg",
+            time_id,
+            chromosome,
+            pattern_clean,
+            comparison_clean,
+            format(Sys.time(), "%H%M%S")
+        )
+    )
+}
+#' Plot Generation Functions
+generate_sample_plot <- function(tracks,
+                               chromosome,
+                               title,
+                               output_file = NULL,
+                               config = CONFIG$VISUALIZATION) {
+    log_info("Generating plot:", title)
+    
+    if (!is.null(output_file)) {
+        svg(output_file)
+        on.exit(dev.off())
+    }
+    
+    plotTracks(
+        tracks,
+        main = title,
+        chromosome = chromosome,
+        ylim = c(config$LIMITS$Y_MIN, config$LIMITS$Y_MAX)
+    )
+}
+
+generate_output_filename <- function(base_dir,
+                                   sample_name,
+                                   chromosome,
+                                   config = CONFIG$VISUALIZATION) {
+    log_info("Generating output filename")
+    
+    date_str <- format(Sys.time(), config$OUTPUT$DATE_FORMAT)
+    
+    file.path(
+        base_dir,
+        sprintf(
+            "%s_%s_%s_WithInputAndHighlights.%s",
+            date_str,
+            chromosome,
+            sample_name,
+            config$OUTPUT$FORMAT
+        )
+    )
+}
diff --git a/R/modules/plot_manager.R b/R/modules/plot_manager.R
new file mode 100644
index 0000000..cf1303b
--- /dev/null
+++ b/R/modules/plot_manager.R
@@ -0,0 +1,97 @@
+#' Main Plot Management Function
+plot_all_sample_tracks <- function(sample_table,
+                                 directory_path,
+                                 chromosome_to_plot = 10,
+                                 genomeRange_to_get,
+                                 control_track,
+                                 annotation_track,
+                                 highlight_gr) {
+    log_info("Starting sample track plotting")
+    
+    # Setup
+    setup_result <- setup_plotting_environment(
+        directory_path,
+        chromosome_to_plot
+    )
+    
+    # Process samples
+    for (sample_index in seq_len(nrow(sample_table))) {
+        if (sample_index == 1) {  # Only process first sample for now
+            process_sample(
+                sample_table[sample_index, ],
+                sample_table,
+                setup_result,
+                control_track,
+                annotation_track,
+                highlight_gr
+            )
+        }
+    }
+    
+    log_info("Plotting completed")
+}
+
+#' Helper Functions
+setup_plotting_environment <- function(directory_path,
+                                     chromosome) {
+    log_info("Setting up plotting environment")
+    
+    list(
+        plot_dir = file.path(directory_path, CONFIG$PATHS$SUBDIRS$PLOTS),
+        bigwig_dir = file.path(directory_path, CONFIG$PATHS$SUBDIRS$BIGWIG),
+        chromosome = paste0("chr", as.roman(chromosome)),
+        title = sprintf("Complete View of Chrom %s", chromosome)
+    )
+}
+
+process_sample <- function(sample_row,
+                         sample_table,
+                         setup,
+                         control_track,
+                         annotation_track,
+                         highlight_gr) {
+    log_info("Processing sample:", sample_row$short_name)
+    
+    # Get sample data
+    sample_data <- get_sample_data(
+        sample_row,
+        setup$bigwig_dir,
+        genomeRange_to_get
+    )
+    
+    if (is.null(sample_data)) {
+        log_warning("No data for sample:", sample_row$short_name)
+        return(NULL)
+    }
+    
+    # Get control data
+    control_data <- get_control_data(
+        sample_row,
+        sample_table,
+        setup$bigwig_dir,
+        genomeRange_to_get
+    )
+    
+    # Create tracks
+    tracks <- create_track_set(
+        sample_data,
+        control_data,
+        annotation_track,
+        highlight_gr,
+        setup$chromosome
+    )
+    
+    # Generate plot
+    output_file <- generate_output_filename(
+        setup$plot_dir,
+        sample_row$short_name,
+        setup$chromosome
+    )
+    
+    generate_sample_plot(
+        tracks,
+        setup$chromosome,
+        setup$title,
+        output_file
+    )
+}
diff --git a/R/modules/range_handler.R b/R/modules/range_handler.R
new file mode 100644
index 0000000..9d1397d
--- /dev/null
+++ b/R/modules/range_handler.R
@@ -0,0 +1,60 @@
+#' Genome Range Management Functions
+create_chromosome_range <- function(genome_data,
+                                  config = CONFIG$GENOME) {
+    log_info("Creating chromosome range")
+    
+    validate_genome_data(genome_data)
+    
+    GRanges(
+        seqnames = genome_data$chrom,
+        ranges = IRanges(
+            start = 1,
+            end = genome_data$basePairSize
+        ),
+        strand = config$DEFAULT_STRAND
+    )
+}
+
+load_control_range <- function(control_dir,
+                             identifier,
+                             chromosome,
+                             genome_range) {
+    log_info("Loading control range data")
+    
+    bigwig_file <- find_control_bigwig(
+        control_dir,
+        identifier
+    )
+    
+    if (is.null(bigwig_file)) {
+        log_error("Control bigwig file not found")
+        return(NULL)
+    }
+    
+    import_control_data(
+        bigwig_file,
+        chromosome,
+        genome_range
+    )
+}
+
+import_control_data <- function(file_path,
+                              chromosome,
+                              genome_range) {
+    log_info("Importing control data")
+    
+    control_style <- determine_chr_style(
+        seqlevels(import(file_path))
+    )
+    
+    chromosome_name <- normalize_chr_names(
+        chromosome,
+        control_style
+    )
+    
+    subset_range <- genome_range[
+        seqnames(genome_range) == chromosome_name
+    ]
+    
+    import(file_path, which = subset_range)
+}
diff --git a/R/modules/sample_labeler.R b/R/modules/sample_labeler.R
new file mode 100644
index 0000000..e242c28
--- /dev/null
+++ b/R/modules/sample_labeler.R
@@ -0,0 +1,83 @@
+#' Sample Labeling Functions
+unique_labeling <- function(table,
+                          categories_for_label,
+                          config = CONFIG$LABEL_CONFIG) {
+    log_info("Creating unique labels for samples")
+    
+    # Validate inputs
+    validate_labeling_inputs(table, categories_for_label)
+    
+    # Ensure required categories
+    categories_for_label <- ensure_required_categories(categories_for_label)
+    
+    # Get unique values
+    unique_values <- get_unique_category_values(table, categories_for_label)
+    
+    # Create labels
+    labels <- create_sample_labels(table,
+                                 categories_for_label,
+                                 unique_values,
+                                 config)
+    
+    return(labels)
+}
+
+validate_labeling_inputs <- function(table, categories) {
+    if (!is.data.frame(table)) {
+        log_error("Invalid input table type")
+        stop("Table must be a data frame")
+    }
+    
+    if (!is.character(categories) || length(categories) == 0) {
+        log_error("Invalid categories specification")
+        stop("Categories must be non-empty character vector")
+    }
+    
+    missing_cats <- setdiff(categories, colnames(table))
+    if (length(missing_cats) > 0) {
+        log_error("Missing categories:", paste(missing_cats, collapse = ", "))
+        stop("Missing categories in table")
+    }
+}
+
+ensure_required_categories <- function(categories) {
+    required <- CONFIG$REQUIRED_CATEGORIES$BASE
+    if (!required %in% categories) {
+        log_info("Adding required category:", required)
+        categories <- c(required, categories)
+    }
+    return(categories)
+}
+
+create_sample_labels <- function(table,
+                               categories,
+                               unique_values,
+                               config) {
+    log_info("Constructing sample labels")
+    
+    labels <- apply(table, 1, function(sample) {
+        # Get relevant category values
+        values <- sapply(categories, function(cat) {
+            if (length(unique_values[[cat]]) > 1 || 
+                cat == CONFIG$REQUIRED_CATEGORIES$BASE) {
+                return(sample[cat])
+            }
+            return(NULL)
+        })
+        
+        # Filter and combine values
+        values <- values[!sapply(values, is.null)]
+        label <- paste(values, collapse = config$SEPARATOR)
+        
+        # Truncate if necessary
+        if (nchar(label) > config$MAX_LENGTH) {
+            label <- paste0(substr(label, 1,
+                                 config$MAX_LENGTH - nchar(config$TRUNCATE_SUFFIX)),
+                          config$TRUNCATE_SUFFIX)
+        }
+        
+        return(label)
+    })
+    
+    return(unlist(labels))
+}
diff --git a/R/modules/sample_matcher.R b/R/modules/sample_matcher.R
new file mode 100644
index 0000000..872a0e4
--- /dev/null
+++ b/R/modules/sample_matcher.R
@@ -0,0 +1,87 @@
+#' Sample and Control Matching Functions
+find_matching_samples <- function(sample_table,
+                                directory_path,
+                                config = CONFIG) {
+    log_info("Finding matching samples and controls")
+    
+    # Setup directories
+    directories <- setup_directories(directory_path)
+    
+    # Get matching factors
+    factors <- get_factors_to_match(sample_table)
+    
+    # Process each sample
+    results <- process_all_samples(
+        sample_table,
+        directories,
+        factors
+    )
+    
+    return(results)
+}
+
+setup_directories <- function(base_path) {
+    list(
+        bam = file.path(base_path, CONFIG$PATHS$SUBDIRS$ALIGNMENT),
+        bigwig = file.path(base_path, CONFIG$PATHS$SUBDIRS$BIGWIG)
+    )
+}
+
+process_all_samples <- function(sample_table,
+                              directories,
+                              factors) {
+    log_info("Processing all samples")
+    
+    results <- list()
+    
+    for (i in seq_len(nrow(sample_table))) {
+        result <- process_single_sample(
+            sample_table[i, ],
+            sample_table,
+            directories,
+            factors
+        )
+        
+        if (!is.null(result)) {
+            results[[i]] <- result
+        }
+    }
+    
+    return(results)
+}
+
+process_single_sample <- function(sample_row,
+                                sample_table,
+                                directories,
+                                factors) {
+    log_info("Processing sample:", sample_row$short_name)
+    
+    # Find control
+    control_info <- find_control_sample(
+        sample_row,
+        sample_table,
+        directories$bam,
+        factors
+    )
+    
+    if (is.null(control_info)) {
+        log_warning("No valid control found for:", sample_row$short_name)
+        return(NULL)
+    }
+    
+    # Find sample BAM
+    sample_bam <- find_sample_bam(
+        sample_row,
+        directories$bam
+    )
+    
+    if (is.null(sample_bam)) {
+        log_warning("No BAM file found for:", sample_row$short_name)
+        return(NULL)
+    }
+    
+    list(
+        sample = sample_bam,
+        control = control_info$file
+    )
+}
diff --git a/R/modules/sample_processor.R b/R/modules/sample_processor.R
new file mode 100644
index 0000000..a59068e
--- /dev/null
+++ b/R/modules/sample_processor.R
@@ -0,0 +1,47 @@
+#' Sample Table Processing Functions
+process_control_factors <- function(sample_table) {
+    log_info("Processing control factors")
+    
+    cf_cols <- grep(CONFIG$PATTERNS$CONTROL_FACTOR_PREFIX, names(sample_table), 
+                   value = TRUE)
+    
+    if (length(cf_cols) == 0) {
+        log_error("No control factor columns found")
+        stop("Invalid sample table format")
+    }
+    
+    control_factors <- lapply(sample_table[cf_cols], function(x) {
+        strsplit(x[1], ",")[[1]]
+    })
+    
+    names(control_factors) <- sub(CONFIG$PATTERNS$CONTROL_FACTOR_PREFIX, "", 
+                                cf_cols)
+    
+    sample_table[cf_cols] <- NULL
+    attr(sample_table, "control_factors") <- control_factors
+    
+    return(sample_table)
+}
+
+#' Control Sample Matching Functions
+determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
+    log_info("Finding matching control sample")
+    
+    comparison_row <- sample_row[factors_to_match]
+    matches <- apply(sample_table[, factors_to_match], 1, function(row) {
+        all(row == comparison_row)
+    })
+    
+    control_indices <- which(matches & sample_table$antibody == "Input")
+    
+    if (length(control_indices) == 0) {
+        log_warning("No matching control found")
+        return(1)
+    }
+    
+    if (length(control_indices) > 1) {
+        log_warning("Multiple controls found, using first")
+    }
+    
+    return(control_indices[1])
+}
diff --git a/R/modules/script_info.R b/R/modules/script_info.R
new file mode 100644
index 0000000..ea44165
--- /dev/null
+++ b/R/modules/script_info.R
@@ -0,0 +1,49 @@
+#' Script Information Functions
+get_script_info <- function() {
+    log_info("Getting script information")
+    
+    script_path <- get_script_path()
+    
+    list(
+        path = script_path,
+        dir = dirname(script_path),
+        name = get_script_name(script_path),
+        config = get_script_config(script_path)
+    )
+}
+
+get_script_path <- function() {
+    # Try command line args first
+    cmd_args <- commandArgs(trailingOnly = FALSE)
+    file_arg <- grep("--file=", cmd_args, value = TRUE)
+    
+    if (length(file_arg) > 0) {
+        return(normalizePath(sub("--file=", "", file_arg)))
+    }
+    
+    # Try sys.frames for sourced scripts
+    if (!is.null(sys.frames()[[1]]$ofile)) {
+        return(normalizePath(sys.frames()[[1]]$ofile))
+    }
+    
+    log_warning("Unable to determine script path")
+    return("interactive")
+}
+
+get_script_name <- function(script_path) {
+    if (script_path == "interactive") {
+        return("interactive")
+    }
+    
+    tools::file_path_sans_ext(basename(script_path))
+}
+
+get_script_config <- function(script_path) {
+    script_name <- get_script_name(script_path)
+    
+    if (script_name == "interactive") {
+        return(NULL)
+    }
+    
+    CONFIG$SCRIPTS[[script_name]]
+}
diff --git a/R/modules/script_loader.R b/R/modules/script_loader.R
new file mode 100644
index 0000000..1ffead2
--- /dev/null
+++ b/R/modules/script_loader.R
@@ -0,0 +1,106 @@
+#' Script Loading Functions
+load_project_scripts <- function(config = CONFIG$INITIALIZATION) {
+    log_info("Loading project scripts")
+    
+    # Load in specific order
+    load_priority_scripts(config)
+    load_remaining_scripts(config)
+    
+    log_info("Script loading completed")
+}
+
+load_priority_scripts <- function(config) {
+    log_info("Loading priority scripts")
+    
+    base_dir <- get_project_root()
+    
+    for (script in config$LOAD_ORDER$PRIORITY) {
+        script_path <- find_script(script, base_dir)
+        if (!is.null(script_path)) {
+            source_script_safely(script_path)
+        }
+    }
+}
+
+load_remaining_scripts <- function(config) {
+    log_info("Loading remaining scripts")
+    
+    # Get all script directories
+    dirs <- get_script_directories(config)
+    
+    # Load scripts from each directory
+    for (dir in dirs) {
+        load_directory_scripts(dir, config)
+    }
+}
+
+source_script_safely <- function(script_path) {
+    log_info("Sourcing:", basename(script_path))
+    
+    tryCatch({
+        source(script_path)
+        TRUE
+    }, error = function(e) {
+        log_error("Failed to source:", script_path)
+        log_error("Error:", e$message)
+        FALSE
+    })
+}
+#' Script Loading Functions
+load_directory_scripts <- function(dir_path) {
+    log_info("Loading scripts from:", dir_path)
+    
+    if (!dir.exists(dir_path)) {
+        log_warning("Directory not found:", dir_path)
+        return(FALSE)
+    }
+    
+    # Get all R scripts
+    scripts <- list.files(
+        dir_path,
+        pattern = CONFIG$PATTERNS$R_FILES,
+        full.names = TRUE
+    )
+    
+    # Filter excluded patterns
+    scripts <- filter_scripts(scripts)
+    
+    # Load each script
+    for (script in scripts) {
+        source_script(script)
+    }
+    
+    TRUE
+}
+
+filter_scripts <- function(scripts) {
+    # Remove excluded patterns
+    for (pattern in CONFIG$PATTERNS$EXCLUDE) {
+        scripts <- scripts[!grepl(pattern, basename(scripts))]
+    }
+    scripts
+}
+
+source_script <- function(script_path) {
+    log_info("Sourcing:", basename(script_path))
+    
+    tryCatch({
+        source(script_path)
+        TRUE
+    }, error = function(e) {
+        log_error("Failed to source:", script_path)
+        log_error("Error:", e$message)
+        FALSE
+    })
+}
+
+load_priority_scripts <- function() {
+    log_info("Loading priority scripts")
+    
+    for (script in CONFIG$LOAD_ORDER$PRIORITY) {
+        script_path <- file.path(CONFIG$PATHS$FUNCTIONS, script)
+        if (file.exists(script_path)) {
+            source_script(script_path)
+        }
+    }
+}
diff --git a/R/modules/sync_handler.R b/R/modules/sync_handler.R
new file mode 100644
index 0000000..5932a24
--- /dev/null
+++ b/R/modules/sync_handler.R
@@ -0,0 +1,11 @@
+#' Sync Command Generation Functions
+generate_sync_command <- function(directory,
+                                config = CONFIG$SYNC) {
+    log_info("Generating sync command")
+    
+    sprintf(
+        config$COMMAND,
+        directory,
+        directory
+    )
+}
diff --git a/R/modules/track_assembler.R b/R/modules/track_assembler.R
new file mode 100644
index 0000000..28b3307
--- /dev/null
+++ b/R/modules/track_assembler.R
@@ -0,0 +1,58 @@
+#' Track Assembly Functions
+create_track_set <- function(sample_data,
+                           control_data,
+                           annotation_data,
+                           highlight_data,
+                           chromosome,
+                           config = CONFIG$VISUALIZATION) {
+    log_info("Creating track set")
+    
+    # Create base tracks
+    tracks <- list(
+        create_genome_axis_track(chromosome),
+        create_control_track(control_data, chromosome),
+        create_sample_track(sample_data, chromosome),
+        create_annotation_track(annotation_data, chromosome)
+    )
+    
+    # Add highlights if present
+    if (!is.null(highlight_data)) {
+        tracks <- create_highlight_track(
+            tracks,
+            highlight_data,
+            chromosome
+        )
+    }
+    
+    return(tracks)
+}
+
+create_sample_track <- function(data,
+                              chromosome,
+                              name,
+                              config = CONFIG$VISUALIZATION) {
+    log_info("Creating sample track:", name)
+    
+    DataTrack(
+        data,
+        type = config$TRACK_TYPES$LINE,
+        name = name,
+        col = config$COLORS$SAMPLE,
+        chromosome = chromosome
+    )
+}
+
+create_control_track <- function(data,
+                               chromosome,
+                               name,
+                               config = CONFIG$VISUALIZATION) {
+    log_info("Creating control track:", name)
+    
+    DataTrack(
+        data,
+        type = config$TRACK_TYPES$LINE,
+        name = name,
+        col = config$COLORS$CONTROL,
+        chromosome = chromosome
+    )
+}
diff --git a/R/modules/track_generator.R b/R/modules/track_generator.R
new file mode 100644
index 0000000..8fd94f9
--- /dev/null
+++ b/R/modules/track_generator.R
@@ -0,0 +1,44 @@
+#' Track Generation Functions
+create_genome_axis_track <- function(chromosome, name = NULL) {
+    log_info("Creating genome axis track")
+    
+    GenomeAxisTrack(
+        name = name %||% sprintf("Chr %s Axis", chromosome)
+    )
+}
+
+create_data_track <- function(data,
+                            chromosome,
+                            name,
+                            config = CONFIG$VISUALIZATION) {
+    log_info("Creating data track:", name)
+    
+    if (!is(data, "GRanges")) {
+        data <- convert_to_granges(data, chromosome)
+    }
+    
+    DataTrack(
+        data,
+        type = config$TRACKS$TYPES$DATA,
+        name = name,
+        col = config$TRACKS$COLORS$PRIMARY,
+        chromosome = chromosome
+    )
+}
+
+create_highlight_track <- function(track_list,
+                                 highlights,
+                                 chromosome) {
+    log_info("Creating highlight track")
+    
+    if (!is(highlights, "GRanges")) {
+        stop("Highlights must be a GRanges object")
+    }
+    
+    HighlightTrack(
+        trackList = track_list,
+        start = start(highlights),
+        end = end(highlights),
+        chromosome = as.character(seqnames(highlights))
+    )
+}
diff --git a/R/modules/track_manager.sh b/R/modules/track_manager.sh
new file mode 100644
index 0000000..a1e69ae
--- /dev/null
+++ b/R/modules/track_manager.sh
@@ -0,0 +1,37 @@
+#' Track Creation and Management Functions
+create_genome_track <- function(chromosome) {
+    log_info("Creating genome axis track")
+    
+    GenomeAxisTrack(
+        name = sprintf("Chr %s Axis", chromosome)
+    )
+}
+
+create_data_track <- function(bigwig_data,
+                            name,
+                            chromosome,
+                            config = CONFIG$PLOT) {
+    log_info("Creating data track:", name)
+    
+    DataTrack(
+        bigwig_data,
+        type = config$TRACK_TYPE,
+        name = name,
+        col = config$COLORS$DEFAULT,
+        chromosome = chromosome
+    )
+}
+
+create_control_track <- function(bigwig_data,
+                               chromosome,
+                               config = CONFIG$PLOT) {
+    log_info("Creating control track")
+    
+    DataTrack(
+        bigwig_data,
+        type = config$TRACK_TYPE,
+        name = "Input",
+        col = config$COLORS$CONTROL,
+        chromosome = chromosome
+    )
+}
diff --git a/R/modules/utilities.R b/R/modules/utilities.R
new file mode 100644
index 0000000..f0c05bc
--- /dev/null
+++ b/R/modules/utilities.R
@@ -0,0 +1,35 @@
+#' Package Loading Functions
+load_required_packages <- function(packages = CONFIG$REQUIRED_PACKAGES) {
+    log_info("Loading required packages")
+    
+    suppressPackageStartupMessages({
+        results <- sapply(packages, require, character.only = TRUE)
+    })
+    
+    if (!all(results)) {
+        missing_packages <- packages[!results]
+        log_error("Failed to load packages:", paste(missing_packages, collapse = ", "))
+        stop("Missing required packages")
+    }
+    
+    return(TRUE)
+}
+
+#' Input Validation Functions
+validate_input <- function(args) {
+    log_info("Validating input arguments")
+    
+    if (length(args) != 1) {
+        log_error("Invalid number of arguments")
+        stop("Usage: Rscript script.R <directory_path>")
+    }
+    
+    directory_path <- file.path(CONFIG$PATHS$BASE_DIR, args[1])
+    
+    if (!dir.exists(directory_path)) {
+        log_error("Directory not found:", directory_path)
+        stop("Invalid directory")
+    }
+    
+    return(directory_path)
+}
diff --git a/R/modules/validation_utils.R b/R/modules/validation_utils.R
new file mode 100644
index 0000000..ffbf26b
--- /dev/null
+++ b/R/modules/validation_utils.R
@@ -0,0 +1,124 @@
+#!/usr/bin/env Rscript
+
+#' Validation Utilities
+#' @description Core validation functions for project
+#' @export
+
+#' Check Project Configuration
+#' @param throw_error Logical Whether to throw error if config missing
+#' @return Logical TRUE if config exists
+check_project_config <- function(throw_error = TRUE) {
+    config_exists <- exists("PROJECT_CONFIG", envir = .GlobalEnv)
+    if (!config_exists && throw_error) {
+        stop("PROJECT_CONFIG not loaded. Source project_config.R first.")
+    }
+    invisible(config_exists)
+}
+
+#' Validate Function Arguments
+#' @param args List Arguments to validate
+#' @param spec List Argument specifications
+#' @return List Validated arguments
+validate_function_args <- function(args, spec) {
+    check_project_config()
+    
+    for (arg_name in names(spec)) {
+        arg_spec <- spec[[arg_name]]
+        arg_value <- args[[arg_name]]
+        
+        # Check required arguments
+        if (is.null(arg_value)) {
+            if (arg_spec$required) {
+                stop(sprintf("Required argument missing: %s", arg_name))
+            }
+            args[[arg_name]] <- arg_spec$default
+            next
+        }
+        
+        # Type checking
+        if (!inherits(arg_value, arg_spec$type)) {
+            stop(sprintf(
+                "Invalid type for %s: expected %s, got %s",
+                arg_name, arg_spec$type, class(arg_value)[1]
+            ))
+        }
+        
+        # Custom validation
+        if (!is.null(arg_spec$validation)) {
+            if (!arg_spec$validation(arg_value)) {
+                stop(arg_spec$error_message)
+            }
+        }
+    }
+    
+    invisible(args)
+}
+
+#' Validate NGS File
+#' @param file_path Character Path to file
+#' @param type Character File type
+#' @param config List Configuration settings
+#' @return Logical TRUE if valid
+validate_ngs_file <- function(
+    file_path,
+    type = c("bam", "fastq", "bigwig", "bed", "narrowpeak", "motif"),
+    config = PROJECT_CONFIG
+) {
+    type <- match.arg(type)
+    
+    # Basic existence check
+    if (!file.exists(file_path)) {
+        log_error(sprintf("File not found: %s", file_path))
+        return(FALSE)
+    }
+    
+    # Extension check
+    pattern <- config$FILE_TYPES$NGS$EXTENSIONS[[toupper(type)]]
+    if (!grepl(pattern, file_path)) {
+        log_error(sprintf("Invalid file extension for %s: %s", type, file_path))
+        return(FALSE)
+    }
+    
+    # Index check if required
+    if (type %in% names(config$FILE_TYPES$NGS$REQUIRED_INDEX)) {
+        index_pattern <- config$FILE_TYPES$NGS$REQUIRED_INDEX[[toupper(type)]]
+        index_file <- sub(pattern, index_pattern, file_path)
+        if (!file.exists(index_file)) {
+            log_warning(sprintf("Index file missing: %s", index_file))
+            return(FALSE)
+        }
+    }
+    
+    TRUE
+}
+
+#' Validate BAM File
+#' @param file_path Character Path to BAM file
+#' @return Logical TRUE if valid
+validate_bam_file <- function(file_path) {
+    # Basic checks
+    if (!grepl("\\.bam$", file_path)) {
+        log_error("Not a BAM file:", file_path)
+        return(FALSE)
+    }
+    
+    # Check index
+    if (!file.exists(paste0(file_path, ".bai"))) {
+        log_warning("BAM index missing:", file_path)
+        return(FALSE)
+    }
+    
+    TRUE
+}
+
+#' Example Usage Configuration
+VALIDATION_CONFIG <- list(
+    TYPES = c(
+        "bam", "fastq", "bigwig", 
+        "bed", "bedgraph", "narrowPeak"
+    ),
+    LIMITS = list(
+        CHROMOSOME = c(min = 1, max = 16),
+        READ_LENGTH = c(min = 20, max = 150)
+    )
+)
diff --git a/R/modules/visualization_config.R b/R/modules/visualization_config.R
new file mode 100644
index 0000000..6cf4f1e
--- /dev/null
+++ b/R/modules/visualization_config.R
@@ -0,0 +1,93 @@
+#' Add to existing configurations
+CONFIG <- list(
+    PLOT = list(
+        COLORS = list(
+            DEFAULT = "#fd0036",
+            CONTROL = "#808080"
+        ),
+        TRACK_TYPE = "l",
+        DATE_FORMAT = "%Y%m%d%H%M%S"
+    ),
+    
+    PATHS = list(
+        PLOTS = "plots",
+        BIGWIG = "bigwig"
+    ),
+    
+    PATTERNS = list(
+        COMPARISON_PREFIX = "^comp_",
+        DEFAULT_BIGWIG = "S288C_log2ratio",
+        TIMECOURSE = "comp_timecourse1108"
+    ),
+    
+    CATEGORIES = list(
+        DEFAULT = c(
+            "strain_source",
+            "rescue_allele",
+            "mcm_tag",
+            "antibody",
+            "timepoint_after_release"
+        )
+    )
+)
+#' Add to existing configurations
+CONFIG <- list(
+    PACKAGES = list(
+        REQUIRED = c(
+            "QuasR",
+            "GenomicAlignments",
+            "Gviz",
+            "rtracklayer",
+            "ShortRead",
+            "tidyverse",
+            "gtools"
+        )
+    ),
+    
+    DEFAULTS = list(
+        CHROMOSOME = 10,
+        GENOME_DIR = "REFGENS",
+        GENOME_PATTERN = "S288C_refgenome.fna",
+        FEATURE_PATTERN = "eaton_peaks",
+        BIGWIG_PATTERN = "_bamcomp.bw"
+    ),
+    
+    SYNC = list(
+        COMMAND = "rsync -nav username@domain:~/data/%s/plots/* /local/dir/%s/plots/",
+        REMOTE_USER = "username",
+        REMOTE_DOMAIN = "domain"
+    )
+)
+#' Add to existing visualization configurations
+CONFIG <- list(
+    VISUALIZATION = list(
+        COLORS = list(
+            SAMPLE = "#E41A1C",
+            CONTROL = "#377EB8",
+            HIGHLIGHT = list(
+                FILL = "#FFE3E6",
+                BORDER = "#FF0000",
+                ALPHA = 0.3
+            )
+        ),
+        TRACK_TYPES = list(
+            LINE = "l",
+            HIGHLIGHT = "highlight"
+        ),
+        LIMITS = list(
+            Y_MIN = 0,
+            Y_MAX = 100000
+        ),
+        OUTPUT = list(
+            FORMAT = "svg",
+            DATE_FORMAT = "%Y%m%d%H%M%S"
+        )
+    ),
+    
+    PATHS = list(
+        SUBDIRS = list(
+            PLOTS = "plots",
+            BIGWIG = "bigwig"
+        )
+    )
+)
diff --git a/R/project_init.R b/R/project_init.R
new file mode 100644
index 0000000..36bfa0f
--- /dev/null
+++ b/R/project_init.R
@@ -0,0 +1,115 @@
+#!/usr/bin/env Rscript
+# R/project_init.R
+
+#' Project Initialization System
+#' @description Initialize project environment and dependencies
+#' @export
+
+#' Load Project Configuration
+#' @param config_path Character Path to configuration file
+#' @return List Configuration object
+load_project_config <- function(
+    config_path = "~/lab_utils/R/config/project_config.R"
+) {
+    if (!file.exists(config_path)) {
+        stop("Configuration file not found: ", config_path)
+    }
+    source(config_path)
+    if (!exists("PROJECT_CONFIG")) {
+        stop("PROJECT_CONFIG not defined in configuration file")
+    }
+    invisible(PROJECT_CONFIG)
+}
+
+#' Check R Version Compatibility
+#' @param R_system_version package_version Current R version
+#' @param R_project_version package_version Required R version
+#' @return Logical TRUE if version check passes
+check_r_version <- function(
+    R_system_version,
+    R_project_version
+) {
+    if (R_system_version < R_project_version) {
+        stop(sprintf(
+            "R version %s required, current version is %s",
+            R_project_version,
+            R_system_version
+        ))
+    }
+    invisible(TRUE)
+}
+
+#' Load Core Utilities
+#' @param base_path Character Base path for utilities
+#' @param script_list Character vector List of scripts to load
+#' @param verbose Logical Enable detailed logging
+#' @return Logical TRUE if loading successful
+load_core_utilities <- function(
+    base_path,
+    script_list,
+    verbose = TRUE
+) {
+    for (script in script_list) {
+        script_path <- file.path(base_path, "functions", script)
+        if (!file.exists(script_path)) {
+            stop("Required utility not found: ", script_path)
+        }
+        if (verbose) message("Loading: ", script_path)
+        source(script_path)
+    }
+    invisible(TRUE)
+}
+
+#' Project Initialization Main Function
+#' @param config_path Character Path to configuration file
+#' @param verbose Logical Enable detailed logging
+#' @return Logical TRUE if initialization successful
+#' @export
+initialize_project_main <- function(
+    config_path = "~/lab_utils/R/config/project_config.R",
+    verbose = TRUE
+) {
+    tryCatch({
+        # Load configuration first
+        config <- load_project_config(config_path)
+        
+        # Check R version
+        check_r_version(
+            R_system_version = getRversion(),
+            R_project_version = package_version(config$SYSTEM$R_MIN_VERSION)
+        )
+        
+        # Ensure log directory exists
+        if (!dir.exists("~/logs")) {
+            dir.create("~/logs", recursive = TRUE)
+        }
+        
+        # Load core utilities in order
+        load_core_utilities(
+            base_path = config$PATHS$R_BASE,
+            script_list = config$LOAD_SEQUENCE$CRITICAL,
+            verbose = verbose
+        )
+        
+        # Initialize logging
+        log_file <- initialize_logging(script_name = "project_init")
+        log_info("Project initialization started", log_file)
+        
+        # Log system information
+        log_system_info(log_file)
+        
+        log_info("Project initialization completed successfully", log_file)
+        invisible(TRUE)
+        
+    }, error = function(e) {
+        message("Critical initialization error: ", e$message)
+        if (interactive()) {
+            recover()
+        } else {
+            quit(status = 1)
+        }
+    })
+}
+
+# Execute if run as script
+if (!interactive()) initialize_project_main()
diff --git a/R/scripts/002_R_node_readPlotFCS.R b/R/scripts/002_R_node_readPlotFCS.R
new file mode 100644
index 0000000..08c8fa8
--- /dev/null
+++ b/R/scripts/002_R_node_readPlotFCS.R
@@ -0,0 +1,102 @@
+# functions moved
+# revisit
+#library(flowCore)
+#library(tidyverse)
+#library(svglite)
+#library(ggplot2)
+#library(ggridges)
+#
+#path_to_fcs_files <- "/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Experiments/Yeast Genetics/2023_10_04 Cdc6 Overexpression/CL4_Cdc6Oe_CarbonSourceShift_01"
+##=======
+#
+##' Extract FITC Width Data from an FCS File
+##'
+##' This function reads an FCS file and extracts the fluorescence data
+##' corresponding to the FITC width of individual cells.
+##'
+##' @param file_path The path to the FCS file.
+##' @return A data frame with one column, FITC_Width, containing the extracted data.
+##' @examples
+##' extract_fitc_width_from_fcs("path/to/your/file.fcs")
+##' @export
+#extract_fitc_width_from_fcs <- function(file_path) {
+#	    if (!file.exists(file_path)) {
+#		            stop("File does not exist: ", file_path)
+#    }
+#    
+#    fcs_data <- tryCatch({
+#	            read.FCS(file_path, transformation = FALSE)
+#		        }, error = function(e) {
+#				        stop("Failed to read FCS file: ", e$message)
+#		        })
+#        
+#        if (!'FITC-Width' %in% colnames(exprs(fcs_data))) {
+#		        stop("FITC-Width data not found in the file.")
+#	    }
+#        
+#        data_frame <- data.frame(FITC_Width = exprs(fcs_data[,'FITC-Width']))
+#	    return(data_frame)
+#}
+#
+##' Generate a Current Datetime String for File Naming
+##'
+##' This function returns a string representing the current date and time,
+##' formatted for use in file names. The format used is "YYYY-MM-DD-HH-MM-SS",
+##' and the system's local time zone is assumed.
+##'
+##' @return A character string of the current date and time.
+##' @examples
+##' get_current_datetime_string()
+##' @export
+#get_current_datetime_string <- function() {
+#	          return(format(Sys.time(), "%Y-%m-%d-%H-%M-%S"))
+#}
+######
+#
+#windows_user <- list.files("/mnt/c/Users")[grepl(Sys.info()[["user"]], list.files("/mnt/c/Users"), ignore.case = TRUE)]
+#path_to_fcs_files <- sprintf("/mnt/c/Users/%s/Dropbox (MIT)/Lab/Experiments/Yeast Genetics/2023_10_04 Cdc6 Overexpression/CL4_Cdc6Oe_CarbonSourceShift_01", windows_user)
+#fcs_file_paths <- list.files(path_to_fcs_files, pattern = "\\.fcs$", full.names = TRUE)
+##fcs_data <- read.FCS(fcs_file_paths[1])
+#
+##TODO: Initialize variables. for each one, use length to assign module class. then maye use switch or some other tidyverse to replace values with variables.
+##REMINDER: Calculated the types of samples that would account for my FACS samples. Seems like I had 4 genotypes, 2 carbon sources and 7 timepoints.
+#genotypes <- c("ORC4", "orc4-R267A", "ORC1", "orc1-K485T")
+#carbon_sources <- c("GLU", "GAL")
+#time_points <- c(seq(0, 150, by = 30))
+#df_sample_info <- as.data.frame(expand.grid(genotype_Orc4 = genotypes, 
+#					    carbonSources = carbon_sources, 
+#					    timePoints = time_points)) %>% arrange(genotype_Orc4,carbonSources,timePoints)
+#df_sample_info$filePath <- gtools::mixedsort(fcs_file_paths)#
+#is_WTandGLU <- df_sample_info$genotype_Orc4 == "ORC4" & df_sample_info$carbonSources == "GLU"
+#subset_df <- df_sample_info %>% filter(is_WTandGLU)
+#for (i in 1:nrow(subset_df)) {
+#
+##Check the first six to ensure creation went well. 
+#for (i in 1:6) {
+##	flow_data <- read.FCS(df_sample_info$filePath[i])
+##	exprs(flow_data[,'FITC-Width'])
+#	formatted_output <- sprintf("%s | %s ", i, basename(subset_df$filePath[i]))
+#	print(formatted_output)
+#}
+#
+#subset_df$fitcData <- map(subset_df$filePath, extract_fitc_width_from_fcs)
+#combined_data <- bind_rows(subset_df$fitcData, .id = "sample_id")
+#
+#head(combined_data)
+#dim(combined_data)
+#str(combined_data)
+#summary(combined_data)
+##Plot the data on the same plot and use color gradient to see. 
+#output_directory <- sprintf("/mnt/c/Users/%s/Dropbox (MIT)/Lab/Experiments/Yeast Genetics/2023_10_04 Cdc6 Overexpression/", windows_user)
+#plot_output <- paste0(output_directory, get_current_datetime_string(), "_densityPlot.svg")
+##svglite(plot_output, width = 6, height = 4)
+#ggplot(combined_data, aes(x = `FITC.Width`, y = after_stat(density), fill = sample_id)) +
+#	geom_density(alpha = .2) +
+#	xlim(0, median(combined_data$FITC.Width) + 1500) + 
+#	labs(title = "Fluorescence Intensity Distribution", x = "Fluorescence Intensity", y = "Density")
+##dev.off()
+#
+#
+## Plot a staggered version using an offset of the samples on the same plot. 
+#combined_data$offset <- as.numeric(as.factor(combined_data$sample_id)) * 100 # Create the offset by multiplying sample id with 10
+#plot_output <- paste0(output_directory, get_current_datetime_string(), "_plot_count.svg")
diff --git a/R/scripts/analyze_bam_qc.R b/R/scripts/analyze_bam_qc.R
new file mode 100644
index 0000000..555194d
--- /dev/null
+++ b/R/scripts/analyze_bam_qc.R
@@ -0,0 +1,34 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/bam_qc_analyzer.R")
+source("../functions/mapping_calculator.R")
+
+#' Main BAM QC analysis function
+main <- function() {
+    args <- commandArgs(trailingOnly = TRUE)
+    
+    if (length(args) == 0) {
+        log_error("No directory specified")
+        stop("Usage: Rscript analyze_bam_qc.R <directory>")
+    }
+    
+    # Find directory
+    directory <- find_data_directory(args[1])
+    
+    if (is.null(directory)) {
+        log_error("Directory not found:", args[1])
+        stop("Invalid directory")
+    }
+    
+    # Analyze QC
+    results <- analyze_bam_qc(directory)
+    
+    if (!is.null(results)) {
+        print(results)
+    }
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/R/scripts/determine_controls.R b/R/scripts/determine_controls.R
new file mode 100644
index 0000000..4b46da1
--- /dev/null
+++ b/R/scripts/determine_controls.R
@@ -0,0 +1,69 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/file_validator.R")
+source("../functions/control_manager.R")
+
+#' Main control determination function
+determine_sample_controls <- function(directory,
+                                    task_id,
+                                    reference_pattern = CONFIG$PATTERNS$REFERENCE_GENOME) {
+    log_info("Starting control determination")
+    
+    # Load and validate sample table
+    sample_table <- load_sample_table(directory)
+    
+    # Get matching factors
+    factors <- get_factors_to_match(sample_table)
+    
+    # Setup directories
+    bam_dir <- file.path(directory, CONFIG$PATHS$SUBDIRS$ALIGNMENT)
+    
+    # Get sample BAM
+    sample_bam <- find_matching_bam(
+        sample_table$sample_ID[task_id],
+        bam_dir
+    )
+    
+    if (is.null(sample_bam)) {
+        log_error("Sample BAM not found")
+        return(NULL)
+    }
+    
+    # Find control
+    control_bam <- find_valid_control(
+        sample_table[task_id, ],
+        sample_table,
+        bam_dir,
+        factors
+    )
+    
+    if (is.null(control_bam)) {
+        log_error("No valid control found")
+        return(NULL)
+    }
+    
+    # Return paths
+    cat(sample_bam, control_bam, sep = "\n")
+}
+
+#' Main entry point
+main <- function() {
+    if (!interactive()) {
+        args <- commandArgs(trailingOnly = TRUE)
+        tryCatch({
+            arg_list <- validate_input(args)
+            determine_sample_controls(
+                arg_list$directory_path,
+                arg_list$slurm_array_task_id
+            )
+        }, error = function(e) {
+            log_error("Execution failed:", e$message)
+            quit(status = 1)
+        })
+    }
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/R/scripts/download_features.R b/R/scripts/download_features.R
new file mode 100644
index 0000000..08f58a8
--- /dev/null
+++ b/R/scripts/download_features.R
@@ -0,0 +1,33 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/data_downloader.R")
+source("../functions/feature_processor.R")
+
+#' Main feature download function
+main <- function() {
+    log_info("Starting feature download")
+    
+    # Setup
+    timestamp <- format(Sys.time(), "%y%m%d")
+    base_dir <- file.path(Sys.getenv("HOME"), "data", "feature_files")
+    
+    ensure_directory(base_dir)
+    
+    # Download and process data
+    results <- download_feature_data(
+        CONFIG$SOURCES,
+        base_dir,
+        timestamp
+    )
+    
+    # Validate results
+    validate_results(results)
+    
+    log_info("Feature download complete")
+    log_info("Cleanup hint: rm *_eaton_acs.bed if not needed")
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/R/scripts/example_plot_sample_tracks.R b/R/scripts/example_plot_sample_tracks.R
new file mode 100644
index 0000000..d814b97
--- /dev/null
+++ b/R/scripts/example_plot_sample_tracks.R
@@ -0,0 +1,26 @@
+#!/usr/bin/env Rscript
+
+source("../functions/input_validator.R")
+
+#' Main plotting function
+main <- function() {
+    # Get command line arguments
+    args <- commandArgs(trailingOnly = TRUE)
+    
+    # Validate inputs
+    tryCatch({
+        validate_script_args(
+            directory = args[1],
+            chromosome = if(length(args) > 1) as.integer(args[2]) else NULL
+        )
+    }, error = function(e) {
+        log_error("Validation failed:", e$message)
+        quit(status = 1)
+    })
+    
+    # Continue with script execution...
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/001_setupR/001_packageInstallation.R b/R/scripts/fcs_package_installation.R
similarity index 68%
rename from 001_setupR/001_packageInstallation.R
rename to R/scripts/fcs_package_installation.R
index d4c1a00..a5108c5 100644
--- a/001_setupR/001_packageInstallation.R
+++ b/R/scripts/fcs_package_installation.R
@@ -1,3 +1,19 @@
+# May need to install run in command line because of systemfonts, textshaping, and ragg:
+# sudo apt-get install libharfbuzz-dev libfribidi-dev libfontconfig1-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev libboost-all-dev build-essential
+renv::install(c("tidyverse", "R.utils", "ggplot2", "BiocManager", "remotes","devtools", "svglite"))
+repository <- "github::"
+user <- "RGLab/"
+packages <- c("RProtoBufLib", "cytolib", "flowCore")
+packages_to_install <- paste0(repository, user, packages)
+renv::install(c(packages_to_install, "ggcyto"))
+renv::install(c("tidyverse", "R.utils", "ggplot2", "BiocManager", "remotes","devtools"))
+
+options(repos = BiocManager::repositories())
+
+library(flowCore)
+
+renv::snapshot()
+
 # Installs R libraries to library_location.
 # Inside R source the file. 
 # This should be setup in 001_setupR/000_installingR4.2.0.sh
@@ -11,8 +27,7 @@ install.packeges(c("tidyverse", "R.utils", "ggplot2", "BiocManager", "renv"),
 
 library(BiocManager)
 bioconductor_packages_to_install <- c("QuasR", "GenomicAlignments", "Gviz", "rtracklayer", "ShortRead")
-BiocManager::install(bioconductor_packages_to_install
-        lib = library_location)
+BiocManager::install(bioconductor_packages_to_install, lib = library_location)
 #Renv has to be installed overall before it can be used to install 
 #install.packeges("renv")
 renv::init(bioconductor = "3.16")
diff --git a/R/scripts/find_sample_inputs.R b/R/scripts/find_sample_inputs.R
new file mode 100644
index 0000000..7274f39
--- /dev/null
+++ b/R/scripts/find_sample_inputs.R
@@ -0,0 +1,46 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/sample_matcher.R")
+source("../functions/bam_finder.R")
+
+#' Main sample input determination function
+determine_input_for_all_samples <- function(sample_table,
+                                          directory_path,
+                                          reference_pattern = CONFIG$FILES$PATTERNS$REFERENCE) {
+    log_info("Starting sample input determination")
+    
+    # Validate inputs
+    validate_inputs(sample_table, directory_path)
+    
+    # Find matching samples
+    results <- find_matching_samples(
+        sample_table,
+        directory_path
+    )
+    
+    # Output results
+    output_results(results)
+    
+    log_info("Sample input determination completed")
+}
+
+validate_inputs <- function(sample_table, directory_path) {
+    if (!all(CONFIG$VALIDATION$REQUIRED_COLUMNS %in% colnames(sample_table))) {
+        log_error("Missing required columns in sample table")
+        stop("Invalid sample table")
+    }
+    
+    if (!dir.exists(directory_path)) {
+        log_error("Directory not found:", directory_path)
+        stop("Invalid directory")
+    }
+}
+
+output_results <- function(results) {
+    for (result in results) {
+        if (!is.null(result)) {
+            cat(result$sample, result$control, sep = "\n")
+        }
+    }
+}
diff --git a/R/scripts/genome_plot_tracks.R b/R/scripts/genome_plot_tracks.R
new file mode 100644
index 0000000..dc531a2
--- /dev/null
+++ b/R/scripts/genome_plot_tracks.R
@@ -0,0 +1,35 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/utils.R")
+source("../functions/sample_processor.R")
+source("../functions/genome_processor.R")
+
+main <- function() {
+    log_info("Starting genome track plotting")
+    
+    # Load required packages
+    load_required_packages()
+    
+    # Process command line arguments
+    args <- commandArgs(trailingOnly = TRUE)
+    directory_path <- validate_input(args)
+    
+    # Load and process sample table
+    sample_table <- load_sample_table(directory_path)
+    
+    # Set up genome visualization
+    options(ucscChromosomeNames = FALSE)
+    refGenome <- load_reference_genome()
+    
+    # Create genome ranges
+    genomeRange <- create_chromosome_GRange(refGenome)
+    
+    # Additional visualization logic here
+    
+    log_info("Genome track plotting completed")
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/R/scripts/install_packages.R b/R/scripts/install_packages.R
new file mode 100644
index 0000000..ce3e3c2
--- /dev/null
+++ b/R/scripts/install_packages.R
@@ -0,0 +1,46 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/package_installer.R")
+source("../functions/environment_manager.R")
+
+#' Main installation function
+install_all_packages <- function(config = CONFIG) {
+    log_info("Starting package installation")
+    
+    # Clean environment
+    clean_environment()
+    
+    # Install bioinformatics packages
+    for (group in names(config$BIOINFORMATICS)) {
+        log_info("Installing", group, "packages")
+        install_package_group(
+            config$BIOINFORMATICS[[group]],
+            "BiocManager"
+        )
+    }
+    
+    # Install development packages
+    for (group in names(config$DEVELOPMENT)) {
+        log_info("Installing", group, "packages")
+        install_package_group(
+            config$DEVELOPMENT[[group]],
+            "renv"
+        )
+    }
+    
+    # Install GitHub packages
+    install_package_group(
+        config$GITHUB_PACKAGES,
+        "github"
+    )
+    
+    # Create snapshot
+    renv::snapshot()
+    
+    log_info("Package installation completed")
+}
+
+if (!interactive()) {
+    install_all_packages()
+}
diff --git a/R/scripts/plot_genome_tracks.R b/R/scripts/plot_genome_tracks.R
new file mode 100644
index 0000000..00af57e
--- /dev/null
+++ b/R/scripts/plot_genome_tracks.R
@@ -0,0 +1,18 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/track_manager.R")
+source("../functions/bigwig_processor.R")
+source("../functions/control_handler.R")
+source("../functions/plot_generator.R")
+
+#' Main plotting function with improved structure
+plot_all_sample_tracks <- function(sample_table,
+                                 directory_path,
+                                 chromosome_to_plot = 10,
+                                 genomeRange_to_get,
+                                 annotation_track,
+                                 highlight_gr = NULL,
+                                 pattern_for_bigwig = CONFIG$PATTERNS$DEFAULT_BIGWIG) {
+    # ... (implementation using the above functions)
+}
diff --git a/R/scripts/process_features.R b/R/scripts/process_features.R
new file mode 100644
index 0000000..448a1c8
--- /dev/null
+++ b/R/scripts/process_features.R
@@ -0,0 +1,27 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/feature_processor.R")
+source("../functions/data_converter.R")
+
+#' Main feature processing function
+main <- function() {
+    log_info("Starting feature processing")
+    
+    # Process features
+    results <- process_feature_files()
+    
+    if (is.null(results)) {
+        log_error("Feature processing failed")
+        quit(status = 1)
+    }
+    
+    # Output transfer command
+    log_info("Processing complete")
+    log_info("To transfer files:")
+    log_info("scp -r user@server:from_dir to_dir")
+}
+
+if (!interactive()) {
+    main()
+}
diff --git a/R/scripts/visualization_example.R b/R/scripts/visualization_example.R
new file mode 100644
index 0000000..57f3408
--- /dev/null
+++ b/R/scripts/visualization_example.R
@@ -0,0 +1,53 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/track_generator.R")
+source("../functions/range_handler.R")
+
+#' Generate example visualization
+create_example_visualization <- function(chromosome = CONFIG$GENOME$DEFAULT_CHROMOSOME,
+                                      start = 1000,
+                                      end = 5000) {
+    log_info("Creating example visualization")
+    
+    # Create example data
+    highlights <- create_example_highlights(chromosome, start, end)
+    tracks <- create_example_tracks(chromosome, start, end)
+    
+    # Create highlight track
+    highlight_track <- create_highlight_track(
+        tracks,
+        highlights,
+        chromosome
+    )
+    
+    # Generate plot
+    plotTracks(
+        highlight_track,
+        from = start,
+        to = end,
+        chromosome = chromosome
+    )
+}
+
+#' Example data generation functions
+create_example_highlights <- function(chromosome, start, end) {
+    GRanges(
+        seqnames = chromosome,
+        ranges = IRanges(
+            start = c(start + 500, start + 2000),
+            end = c(start + 1000, start + 2500)
+        )
+    )
+}
+
+create_example_tracks <- function(chromosome, start, end) {
+    length <- end - start + 1
+    
+    list(
+        create_genome_axis_track(chromosome),
+        create_data_track(runif(length), chromosome, "Random"),
+        create_data_track(rnorm(length), chromosome, "Normal"),
+        create_annotation_track(chromosome, start, end)
+    )
+}
diff --git a/R/scripts/visualize_genome_tracks.R b/R/scripts/visualize_genome_tracks.R
new file mode 100644
index 0000000..02ae005
--- /dev/null
+++ b/R/scripts/visualize_genome_tracks.R
@@ -0,0 +1,185 @@
+#!/usr/bin/env Rscript
+# functions moved
+
+source("../functions/package_manager.R")
+source("../functions/data_preparer.R")
+source("../functions/sync_handler.R")
+source("../functions/plot_generator.R")  # From previous analysis
+
+#' Main execution function
+run_visualization <- function(directory,
+                            chromosome = CONFIG$DEFAULTS$CHROMOSOME,
+                            bigwig_pattern = CONFIG$DEFAULTS$BIGWIG_PATTERN) {
+    log_info("Starting genome visualization")
+    
+    # Load required packages
+    load_required_packages()
+    
+    # Prepare data
+    data <- prepare_visualization_data(
+        directory,
+        chromosome
+    )
+    
+    # Prepare feature track
+    feature_track <- prepare_feature_track(
+        chromosome,
+        data$range
+    )
+    
+    # Generate visualization
+    plot_all_sample_tracks(
+        sample_table = data$samples,
+        directory_path = data$directory,
+        chromosome_to_plot = chromosome,
+        genomeRange_to_get = data$range,
+        annotation_track = feature_track,
+        highlight_gr = feature_track@range,
+        pattern_for_bigwig = bigwig_pattern
+    )
+    
+    log_info("Visualization complete")
+    
+    # Generate sync command
+    sync_cmd <- generate_sync_command(directory)
+    log_info("Sync command:", sync_cmd)
+}
+
+#' Main entry point
+main <- function() {
+    if (!interactive()) {
+        tryCatch({
+            run_visualization("240819Bel")
+        }, error = function(e) {
+            log_error("Visualization failed:", e$message)
+            quit(status = 1)
+        })
+    } else {
+        run_visualization(
+            directory = "240819Bel",
+            chromosome = 10,
+            bigwig_pattern = "_bamcomp.bw"
+        )
+    }
+}
+
+if (!interactive()) {
+    main()
+}
+#!/usr/bin/env Rscript
+
+#' Load all required components
+source("../functions/track_generator.R")
+source("../functions/feature_processor.R")
+source("../functions/sample_processor.R")
+source("../functions/visualization_manager.R")
+
+#' Main execution function with mode handling
+main <- function(interactive_mode = FALSE) {
+    log_info("Starting genome visualization")
+    
+    if (interactive_mode) {
+        run_interactive_mode()
+    } else {
+        run_batch_mode()
+    }
+}
+
+#' Batch mode execution
+run_batch_mode <- function() {
+    tryCatch({
+        args <- commandArgs(trailingOnly = TRUE)
+        process_visualization(args)
+        
+        # Output sync command
+        log_info("To sync results:")
+        log_info("rsync -nav username@domain:~/data/<dir>/plots/* /local/dir/<dir>/plots/")
+        
+    }, error = function(e) {
+        log_error("Visualization failed:", e$message)
+        quit(status = 1)
+    })
+}
+
+#' Interactive mode execution
+run_interactive_mode <- function() {
+    tryCatch({
+        # Default settings for interactive mode
+        directory_path <- "240808Bel"
+        chromosome <- 10
+        
+        # Process visualization
+        process_visualization(directory_path)
+        
+    }, error = function(e) {
+        log_error("Interactive visualization failed:", e$message)
+    })
+}
+
+#' Main visualization process
+process_visualization <- function(args) {
+    # Setup
+    setup <- initialize_visualization(args)
+    
+    # Load data
+    data <- load_visualization_data(setup)
+    
+    # Generate tracks
+    tracks <- generate_visualization_tracks(data)
+    
+    # Create plots
+    create_visualization_plots(tracks, setup)
+}
+
+#' Initialize visualization environment
+initialize_visualization <- function(args) {
+    # Validate input
+    directory <- validate_input(args)
+    
+    # Load required packages
+    load_required_packages()
+    
+    # Setup visualization parameters
+    list(
+        directory = directory,
+        chromosome = CONFIG$VISUALIZATION$DEFAULT_CHROMOSOME,
+        options = list(
+            ucscChromosomeNames = FALSE
+        )
+    )
+}
+
+#' Load required data
+load_visualization_data <- function(setup) {
+    # Load sample table
+    sample_table <- load_sample_table(setup$directory)
+    
+    # Load reference genome
+    ref_genome <- load_reference_genome(
+        CONFIG$GENOME$DIR,
+        CONFIG$GENOME$PATTERN
+    )
+    
+    # Create genome range
+    genome_range <- create_chromosome_range(ref_genome)
+    
+    # Load features
+    features <- load_feature_data(
+        setup$directory,
+        genome_range
+    )
+    
+    list(
+        samples = sample_table,
+        genome = ref_genome,
+        range = genome_range,
+        features = features
+    )
+}
+
+#' Main entry point
+if (!interactive()) {
+    main(FALSE)
+} else {
+    main(TRUE)
+}
diff --git a/R/tests/core/test_core.R b/R/tests/core/test_core.R
new file mode 100644
index 0000000..012d1e6
--- /dev/null
+++ b/R/tests/core/test_core.R
@@ -0,0 +1,37 @@
+# tests/test_core.R
+
+#' Test Core Functionality
+#' @return Logical TRUE if all tests pass
+test_core <- function() {
+    # Test logging
+    log_file <- initialize_logging(
+        context = "test",
+        log_dir = tempdir()
+    )
+    
+    stopifnot(
+        "Log file creation failed" = file.exists(log_file),
+        "Log message failed" = write_log_message(
+            level = "INFO",
+            message = "Test message",
+            log_file = log_file
+        )
+    )
+    
+    # Test locking
+    test_file <- tempfile()
+    stopifnot(
+        "Lock acquisition failed" = acquire_lock(test_file),
+        "Lock release failed" = release_lock(test_file)
+    )
+    
+    return(TRUE)
+}
+
+# Quick test
+if (!interactive()) {
+    if (!test_core()) {
+        stop("Core validation failed")
+    }
+    cat("Core validation successful\n")
+}
diff --git a/R/tests/modules/test_chromosome_converter.R b/R/tests/modules/test_chromosome_converter.R
new file mode 100644
index 0000000..1b01e9e
--- /dev/null
+++ b/R/tests/modules/test_chromosome_converter.R
@@ -0,0 +1,51 @@
+#!/usr/bin/env Rscript
+
+source("../functions/chromosome_converter.R")
+source("../functions/granges_converter.R")
+
+#' Test chromosome conversion functionality
+test_chromosome_conversion <- function() {
+    log_info("Starting chromosome conversion tests")
+    
+    # Create test data
+    test_gr <- GRanges(
+        seqnames = c("1", "X", "chrII", "20", "chrY", "M",
+                    "chr10", "III", "chrMT"),
+        ranges = IRanges(
+            start = sample(1:1000, 9),
+            width = 100
+        )
+    )
+    
+    # Test conversions
+    test_conversions(test_gr)
+    
+    # Test error handling
+    test_error_handling()
+    
+    log_info("Tests completed")
+}
+
+test_conversions <- function(gr) {
+    log_info("Testing style conversions")
+    
+    for (style in names(CONFIG$NAMING$STYLES)) {
+        log_info("Testing conversion to:", style)
+        result <- convert_granges_style(gr, style, verbose = TRUE)
+        print(result)
+    }
+}
+
+test_error_handling <- function() {
+    log_info("Testing error handling")
+    
+    tryCatch({
+        convert_granges_style(data.frame(), "UCSC")
+    }, error = function(e) {
+        log_info("Expected error caught:", conditionMessage(e))
+    })
+}
+
+if (!interactive()) {
+    test_chromosome_conversion()
+}
diff --git a/R/tests/test_core.R b/R/tests/test_core.R
new file mode 100644
index 0000000..76a738a
--- /dev/null
+++ b/R/tests/test_core.R
@@ -0,0 +1,36 @@
+# tests/test_core.R
+
+#' Test Core Functionality
+#' @return Logical TRUE if all tests pass
+test_core <- function() {
+    # Test logging
+    log_file <- initialize_logging("test")
+    stopifnot(
+        "Log file creation failed" = file.exists(log_file),
+        "Log directory incorrect" = grepl(CORE_CONFIG$PATHS$LOGS, log_file)
+    )
+    
+    # Test message writing
+    success <- write_log_message("INFO", "Test message", log_file)
+    stopifnot(
+        "Log writing failed" = success,
+        "Message not written" = any(grepl("Test message", readLines(log_file)))
+    )
+    
+    # Test locking
+    test_file <- tempfile()
+    stopifnot(
+        "Lock acquisition failed" = acquire_lock(test_file),
+        "Lock release failed" = release_lock(test_file)
+    )
+    
+    return(TRUE)
+}
+
+# Run tests
+if (!interactive()) {
+    if (!test_core()) {
+        stop("Core validation failed")
+    }
+    cat("Core validation successful\n")
+}
diff --git a/README.md b/README.md
index da272f4..3879ea3 100644
--- a/README.md
+++ b/README.md
@@ -40,11 +40,12 @@ The analysis are done locally or in a linux computing cluster. The linux cluster
 
 1. R 4.2.0
 2. Command line utils
-3. bowtie2
-4. fastp 
-5. fastqc 
-6. deeptools ( cluster version requires python 2.7)
+3. bowtie2/2.3.5.1
+4. fastp/0.20.0
+5. fastqc/0.11.5
+6. deeptools/3.0.1
 7. gatk
+8. python 2.7
 ## Installation
 ```{bash}
 git clone https://github.com/luised94/lab_utils.git
diff --git a/STICKY_NOTES.md b/STICKY_NOTES.md
index a25b448..b0e98e8 100644
--- a/STICKY_NOTES.md
+++ b/STICKY_NOTES.md
@@ -17,17 +17,17 @@ I have separated it into dates.
 ## 2024 04 22
 - Was using assign for variable assignment in my previous code assignment, contributed to uninterpretability even though it is a way to assign variables programmatically
 - Will implement a for loop or lapply probably for the track assignment and plotting, easy to understand
-- Think I need to rerun the alignments because there may have been an ordering problem. Not sure since variable naming is done on basis of the file. 
+- Think I need to rerun the alignments because there may have been an ordering problem. Not sure since variable naming is done on basis of the file.
 
 ## 2024 04 23
 SPB comments
 Overnight, Temp, Amount of antibody, same conditions as Kate from winston lab (zotero: @miller_winston23)
-Dont think I have to repeat ORC two more times but probably will have to... 
+Dont think I have to repeat ORC two more times but probably will have to...
 
 ## 2024 04 27
-- Had some trouble with the text processing and used awk to make more robust. 
+- Had some trouble with the text processing and used awk to make more robust.
 - Could come up with script that lets me know if the assumptions I make in my script are met. On the other hand, could just use same name instead of processing.
-- Definitely thinking a little about design up front is worth it pero esto vale cuando haga otro proyecto. 
+- Definitely thinking a little about design up front is worth it pero esto vale cuando haga otro proyecto.
 
 ## 2024 04 28
 - Creating a new branch for flow_cytomery since I am also attempting to use renv for package management.
@@ -38,7 +38,7 @@ git branch feature/flow_cytometry
 #Procede with feature development. In this case, creating the module for analysing flow_cytometry data.
 git checkout feature/flow_cytometry
 # Return to main
-git checkout main 
+git checkout main
 
 '''
 
@@ -54,7 +54,7 @@ Combined for loop with globbing and a multi-line sed command.
 ## 20240826
 Have a decent set of features that I can use to compare to my data. Also have a good amount of reference to use for further comparisons and factor analysis.
 Need to update the genome track plotting script and create all of the downstream analysis.
-Still need to do quality control. 
+Still need to do quality control.
 Must work faster.
 
 ## 2024-09-06
@@ -94,6 +94,66 @@ See results using: find /home/luised94/data/240819Bel/logs -type f -name 2024092
 Run 002_plotUserDefinedExperiments.R after bigwig files are generated.
 
 ## 2024-10-01
-Create a comprehensive set of bigwig files by modifying the bamCompare and bamCoverage files to see the best way to visualize. 
+Create a comprehensive set of bigwig files by modifying the bamCompare and bamCoverage files to see the best way to visualize.
 Need to start working of peak calling.
 Need to potentially add timeid or use distinct tags.
+
+## 2024-10-18
+I think I should implement logging and then perform the reorganization into functions and scripts with updated logging.
+Not super sure about the tests for each function or file but I guess that means I dont understand it enough.
+Maybe quick runs using repl to get it to usable spot and then systematic test construction.
+
+## 2024-10-23
+Debating about where load and output type functions. I think load should go in file operations whole modify and output should go in table operations (or anything else that is being outputted.). This references the object that is being acted on. You load the file. You output the table.
+If there is confusion about this because it requires, two things then it is likely that we should refactor it.
+Will mark scripts files at the top to denote that I moved the functions to their respective files.
+Hmm. It seems that separation of concerns also applies to testing.
+Will further distinguish operations on filetype. For example, if file is tsv, fastq etc will be in tsv_operations, fastq_operations etc.
+Need to break up functions in my config into functions and scripts and configuration.
+May move xml creator into my config.
+Didnt include the uninstallR script in the reorganization even tho I processed it to see. Not quite sure if I need it.
+Reorganized till determineInputForArrayId.R
+
+## 2024-10-24
+Retain the FCS files but deal with them in the future.
+
+## 2024-10-28
+Currently have setup_bmc_experiment.R following a similar protocol to before. Initialize the samples in bmc_sample_grid_config.R. No longer output with comp_ columns. Instead they are found in the experiment config.
+See 002__20241025_Task_Personal/Work_R/bash/Spaces/labutils_XXX: Using perplexity spaces to effectively update my configuration. Revisit. __project.
+Search for current repository state one.
+Have to add print statements for efficient debugging when of the bmc_sample_grid_config.
+
+## 2024-10-29
+Currently setup till transfer of fastq to bmc.
+Mostly just run setup_bmc_experiment then transfer_bmc_experiment_directory_to_luria.
+Havent granted authority to run as script. Run from repl but most source logging_utils.R and project_init.R, and project_config.R
+Adjust bmc_sample_grid_config to setup the particular bmc experiment.
+Then run  download_bmc_fastq_... .
+
+## 2024-10-30
+Remove and clean the directories 241007Bel and 241010Bel before redownloading.
+Update run_alignment.
+Clean up, then test slurm_wrapper with run_alignment.
+Need to determine how to have log_file be empty to not mess up logging. Should be detected in the if statement at the bottom of log_message.
+After testing download_bmc_fastq_to_user_bel_directory.sh, have to remove files referenced in current version: bmc_fastq_file_manager.sh,
+Need to update BMC_CONFIG and its occurrences and calling of PROJECT_CONFIG.
+I dont think verify_filesystem_path will work expected_fs is not equal to real_path.
+
+## 2024-11-01
+Consolidate the two core testing files.
+Create the random lock files in bash and try to normalize the R and bash logging, lock and init.R.
+
+## 2024-11-03
+9641496
+vim /home/luised94/logs/2024-11/bamcoverage/job_9641582/task_*/*.log
+vim /home/luised94/logs/2024-11/bamcoverage/job_9641648/task_*/*.log
+
+Need to doublecheck that samples are in the correct order than I submitted to the bmc.
+Need to test that input sample name is being added correctly.
+
+## 2024-11-07
+Input samples could still be messed up. Ensure everything lines up during alignment, coverage calculations and plotting.
+
+## 2024-11-11
+Need to update the output directory to not nest plot directories. See script to plot all samples.
+
diff --git a/archive/node_refgenomes.sh b/archive/node_refgenomes.sh
deleted file mode 100755
index 2034cfd..0000000
--- a/archive/node_refgenomes.sh
+++ /dev/null
@@ -1,23 +0,0 @@
-#!/bin/bash
-#USAGE: From ~/data/REFGENS/, run './node_ bt2build_refgenomes.sh'
-#
-refgenome_dir=~/data/REFGENS
-exec > "${refgenome_dir}/logs/$(date '+%Y-%m-%d-%M-%S')_indexing.out" 2>&1
-echo "START TIME: $(date '+%Y-%m-%d-%M-%S')"
-echo "From dir ${refgenome_dir}"
-
-module purge
-module load gnu/5.4.0
-module load bowtie2/2.3.5.1
-
-mapfile -t genome_paths < <(find "$refgenome_dir" -type f -name "*_refgenome.fna")
-
-#genome_path=${genome_paths[$SLURM_ARRAY_TASK_ID-1]}
-for genome_path in "${genome_paths[@]}"; do
-	echo "Starting indexing for $genome_path"
-	echo "$genome_path" "${genome_path%_refgenome.fna}_index"
-	bowtie2-build "$genome_path" "${genome_path%_refgenome.fna}_index"
-	echo "Indexing completed"
-done 
-
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
diff --git a/bash/config/core_config.sh b/bash/config/core_config.sh
new file mode 100644
index 0000000..fb302e6
--- /dev/null
+++ b/bash/config/core_config.sh
@@ -0,0 +1,56 @@
+#!/bin/bash
+# bash/config/core_config.sh
+
+declare -A CORE_CONFIG=(
+    # Version Control
+    [VERSION]="1.0.0"
+    [MIN_BASH_VERSION]="4.2.0"
+    # Logging
+    [LOG_LEVELS]="TRACE DEBUG INFO WARNING ERROR FATAL"
+    [DEFAULT_LOG_ROOT]="$HOME/logs"
+    [LOG_FORMAT]="[%s] [%s] [%s] %s\n"  # timestamp, level, context, message
+    # Locking
+    [LOCK_TIMEOUT]="30"
+    [LOCK_RETRY]="3"
+    [LOCK_BASE_DIR]="/tmp/lab_utils_locks"
+    # Paths
+    [PROJECT_ROOT]="$HOME/lab_utils"
+    [MODULE_PATH]="$HOME/lab_utils/bash/modules"
+    [VERBOSE]="false"
+    [RUN_SEPARATOR]="=== New Run ==="
+    [TIMESTAMP_FORMAT]="%Y-%m-%d %H:%M:%S"
+    [ENTRY_FORMAT]="\n%s (#%d) === %s ===\n"
+    [FIRST_RUN_FORMAT]="%s (#1) === %s ===\n"
+    [BUFFER_SIZE]="4096"         # Write buffer size
+    [MAX_MESSAGE_LENGTH]="1024"   # Maximum message length
+)
+
+#declare -A PATHS=(
+#    ["BASE_DIR"]="$HOME/lab_utils"
+#    ["FUNCTIONS_DIR"]="bash/functions"
+#    ["CONFIG_DIR"]="bash/config"
+#    ["SCRIPTS_DIR"]="bash/scripts"
+#)
+#
+#declare -A FILE_PATTERNS=(
+#    ["FUNCTIONS"]="*.sh"
+#    ["CONFIG"]="*.sh"
+#    ["EXCLUDE_PATTERNS"]=(".*" "_*" "test_*")
+#)
+#
+#declare -A LOAD_ORDER=(
+#    ["PRIORITY"]=(
+#        "logging.sh"
+#        "lock_utils.sh"
+#        h"
+#    )
+#    ["OPTIONAL"]=(
+#        "experimental.sh"
+#        "deprecated.sh"
+#    )
+#)
+# Step marker
+#  Continuation
+# Final step
+# Success
+#? Failure
diff --git a/bash/config/modules/bam_comparison_config.sh b/bash/config/modules/bam_comparison_config.sh
new file mode 100644
index 0000000..874808b
--- /dev/null
+++ b/bash/config/modules/bam_comparison_config.sh
@@ -0,0 +1,30 @@
+#!/bin/bash
+
+# Add to existing or create new configuration
+declare -A BAMCOMPARE_PARAMS=(
+    ["BIN_SIZE"]=10
+    ["NORMALIZATION"]="CPM"
+    ["SCALE_METHOD"]="readCount"
+    ["GENOME_SIZE"]=12157105
+    ["MIN_MAPPING_QUALITY"]=20
+    ["OPERATION"]="ratio"
+    ["IGNORE_REGIONS"]=("chrXII")
+    ["PSEUDOCOUNT"]=1
+)
+
+declare -A OUTPUT_DIRS=(
+    ["BIGWIG"]="bigwig"
+    ["LOGS"]="logs"
+)
+
+declare -A R_CONFIG=(
+    ["SCRIPT_PATH"]="$HOME/lab_utils/next_generation_sequencing/004_bamProcessing/determineInputForArrayId.R"
+)
+
+# Add to existing module configurations
+declare -A REQUIRED_MODULES=(
+    ["GNU"]="gnu/5.4.0"
+    ["PYTHON"]="python/2.7.13"
+    ["DEEPTOOLS"]="deeptools"
+    ["R"]="r/4.2.0"
+)
diff --git a/bash/config/modules/bmc_config.sh b/bash/config/modules/bmc_config.sh
new file mode 100644
index 0000000..a7b974f
--- /dev/null
+++ b/bash/config/modules/bmc_config.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+declare -A BMC_CONFIG=(
+    [SOURCE_FS]="/net/bmc-pub17/data/bmc/public/Bell"
+    [TARGET_FS]="/net/bmc-pub14/data/bell/users/luised94"  # More specific
+    [RSYNC_OPTIONS]="-av"
+    [CLEANUP_DIRS]="*D24* infosite* *_fastqc *_stats"
+    [CLEANUP_FILES]="*unmapped*.fastq *.html *.zip"
+    [MIN_SPACE_GB]=50
+    
+)
+## Cleanup patterns
+## BMC-specific settings
+##[BMC_BASE_PATH]="/net/%s/data/bmc/public/Bell/%s"
+##[BMC_FASTQ_DIR]="fastq"
+##[BMC_DEFAULT_SERVER]="bmc-pub17"
diff --git a/bash/config/modules/coverage_config.sh b/bash/config/modules/coverage_config.sh
new file mode 100644
index 0000000..4fec647
--- /dev/null
+++ b/bash/config/modules/coverage_config.sh
@@ -0,0 +1,26 @@
+#!/bin/bash
+
+# Add to existing or create new configuration
+declare -A COVERAGE_PARAMS=(
+    ["BIN_SIZE"]=10
+    ["NORMALIZATION"]="CPM"
+    ["MIN_MAPPING_QUALITY"]=20
+    ["IGNORE_DUPLICATES"]=true
+)
+#--normalizeUsing {RPKM,CPM,BPM,RPGC}
+declare -A FILE_PATTERNS=(
+    ["BAM_SUFFIX"]="S288C.bam"
+    ["BIGWIG_SUFFIX"]="_indivNorm.bw"
+)
+
+declare -A OUTPUT_DIRS=(
+    ["BIGWIG"]="bigwig"
+    ["LOGS"]="logs"
+)
+
+# Add to existing module configurations
+declare -A REQUIRED_MODULES=(
+    ["GNU"]="gnu/5.4.0"
+    ["PYTHON"]="python/2.7.13"
+    ["DEEPTOOLS"]="deeptools"
+)
diff --git a/bash/config/modules/data_sources_config.sh b/bash/config/modules/data_sources_config.sh
new file mode 100644
index 0000000..e741404
--- /dev/null
+++ b/bash/config/modules/data_sources_config.sh
@@ -0,0 +1,16 @@
+#!/bin/bash
+
+# Add to existing or create new configuration
+declare -A FEATURE_DATA=(
+    ["BASE_DIR"]="$HOME/data/feature_files"
+    ["DEFAULT_DEPTH"]=1
+)
+
+declare -A REPOSITORIES=(
+    ["ROSSI_2021"]={
+        "url"="https://github.com/CEGRcode/2021-Rossi_Nature.git"
+        "branch"="main"
+        "depth"=1
+        "dir"="rossi_2021"
+    }
+)
diff --git a/bash/config/modules/experiment_config.sh b/bash/config/modules/experiment_config.sh
new file mode 100644
index 0000000..7e9a22d
--- /dev/null
+++ b/bash/config/modules/experiment_config.sh
@@ -0,0 +1,13 @@
+#!/bin/bash
+
+# File patterns and formats
+export EXPERIMENT_FILE_PATTERN="*.sh"
+export EXPERIMENT_NUMBER_FORMAT="%03d"
+export DATE_FORMAT="%Y%m%d"
+
+# File naming
+export FILENAME_TEMPLATE="${DATE}_${EXPERIMENT_INDEX}_experiment.md"
+
+# Paths
+export TEMPLATE_DIR="templates"
+export TEMPLATE_FILE="experiment_template.md"
diff --git a/bash/config/modules/fastq_processing_config.sh b/bash/config/modules/fastq_processing_config.sh
new file mode 100644
index 0000000..d303d56
--- /dev/null
+++ b/bash/config/modules/fastq_processing_config.sh
@@ -0,0 +1,36 @@
+#!/bin/bash
+
+# Add to existing FASTQ configurations
+declare -A FASTP_PARAMS=(
+    ["BASE_PARAMS"]=(
+        "--cut_window_size 4"
+        "--cut_mean_quality 20"
+        "--n_base_limit 5"
+        "--average_qual 20"
+        "--qualified_quality_phred 20"
+        "--unqualified_percent_limit 50"
+        "--html /dev/null"
+    )
+    ["STANDARD"]=(
+        "--length_required 50"
+    )
+    ["EATON"]=(
+        "--length_required 20"
+    )
+)
+
+declare -A FILE_PATTERNS=(
+    ["INPUT"]="*.fastq"
+    ["EXCLUDE_PATTERNS"]="*unmapped* processed_*"
+    ["OUTPUT_PREFIX"]="processed_"
+)
+
+declare -A OUTPUT_DIRS=(
+    ["PROCESSED"]="processedFastq"
+    ["LOGS"]="logs"
+)
+
+# Add to existing module configurations
+declare -A REQUIRED_MODULES=(
+    ["FASTP"]="fastp/0.20.0"
+)
diff --git a/bash/config/modules/genome_config.sh b/bash/config/modules/genome_config.sh
new file mode 100644
index 0000000..4b628f9
--- /dev/null
+++ b/bash/config/modules/genome_config.sh
@@ -0,0 +1,71 @@
+#!/bin/bash
+
+declare -A GENOME_CONFIG=(
+    ["BASE_DIR"]="$HOME/data/REFGENS"
+    ["LOG_DIR"]="$HOME/data/REFGENS/logs"
+    ["GENOME_PATTERN"]="*_refgenome.fna"
+    ["INDEX_SUFFIX"]="_index"
+)
+
+declare -A SLURM_CONFIG=(
+    ["NODES"]=1
+    ["TASKS"]=1
+    ["MEM_PER_CPU"]="20G"
+    ["EXCLUDE_NODES"]="c[5-22]"
+    ["MAIL_TYPE"]="ALL"
+    ["MAIL_USER"]="luised94@mit.edu"
+)
+
+declare -A MODULES=(
+    ["GNU"]="gnu/5.4.0"
+    ["BOWTIE2"]="bowtie2/2.3.5.1"
+)
+
+# Add NCBI-specific configurations
+declare -A NCBI_CONFIG=(
+    ["DOWNLOAD_INCLUDES"]="genome,rna,cds,protein,gff3,gtf"
+    ["ACCESSIONS"]=(
+        "GCF_000146045.2"  # S. cerevisiae S288C
+        "GCF_000001405.40" # Human
+        "GCF_000005845.2"  # E. coli
+        "GCA_002163515.1"  # S cerevisaie W303
+    )
+    ["GENOME_PATTERNS"]=("GCF*.fna" "GCA*.fna")
+)
+
+declare -A GENOME_NAMING=(
+    ["CDS_FILE"]="cds.fna"
+    ["REFGENOME_SUFFIX"]="_refgenome.fna"
+    ["ORIGINAL_CDS"]="cds_from_genomic.fna"
+    ["GENOMIC_PATTERN"]="*_genomic.fna"
+)
+
+declare -A GENOME_PATHS=(
+    ["NCBI_DATA"]="ncbi_dataset/data"
+    ["ASSEMBLY_REPORT"]="assembly_data_report.jsonl"
+)
+
+declare -A CHROMOSOME_NAMING=(
+    ["PATTERN_OLD"]="chromosome"
+    ["PATTERN_NEW"]="chr"
+    ["SEPARATOR"]=","
+)
+
+# Add to existing genome configurations
+declare -A HEADER_PATTERNS=(
+    ["OLD_PREFIX"]="chromosome"
+    ["NEW_PREFIX"]="chr"
+    ["DELIMITER"]=","
+)
+
+declare -A GENOME_FILES=(
+    ["S288C_PATTERN"]="*S288C_refgenome.fna"
+    ["BACKUP_SUFFIX"]="_backup.fna"
+)
+
+# Add to existing file patterns if they exist
+declare -A FILE_OPERATIONS=(
+    ["BACKUP"]=true
+    ["VERIFY"]=true
+    ["MAX_HEADER_LENGTH"]=100
+)
diff --git a/bash/config/modules/project_config.sh b/bash/config/modules/project_config.sh
new file mode 100644
index 0000000..0696bb7
--- /dev/null
+++ b/bash/config/modules/project_config.sh
@@ -0,0 +1,42 @@
+#!/bin/bash
+# bash/config/project_config.sh
+
+declare -A PROJECT_CONFIG=(
+    [REMOTE_HOST]="luria.mit.edu"
+    [REMOTE_USER]="luised94"
+    [REMOTE_PATH]="$HOME/data"
+    [REQUIRED_DIRS]="documentation fastq"
+    # File patterns
+    [FASTQ_PATTERN]="*.fastq"
+    [SAMPLE_PATTERN]="[0-9]{6}Bel"
+
+    # Safety settings
+    [REQUIRE_CONFIRMATION]="true"
+    [STAGING_DIR_PREFIX]="bmc_staging"
+    #
+    # FASTQ Processing
+    [FASTP_BASE_PARAMS]="--cut_window_size 4 --cut_mean_quality 20 --n_base_limit 5 --average_qual 20 --qualified_quality_phred 20 --unqualified_percent_limit 50 --html /dev/null"
+    [FASTP_STANDARD_PARAMS]="--length_required 50"
+    [FASTP_EATON_PARAMS]="--length_required 20"
+    
+    # File Patterns
+    [FASTQ_INPUT_PATTERN]="*.fastq"
+    [FASTQ_EXCLUDE_PATTERNS]="*unmapped* processed_*"
+    [FASTQ_OUTPUT_PREFIX]="processed_"
+
+    # FASTQ Processing
+    [FASTQ_PATTERN]="*.fastq"
+    [BMC_FASTQ_ID_PATTERN]="[-_]"  # Pattern for splitting
+    [BMC_ID_REGEX]="[0-9]{5,6}"    # Pattern for matching ID
+    [FASTQ_EXCLUDE]="*unmapped* processed_*"
+    [FASTQ_PREFIX]="processed_"
+    [FASTQ_SUFFIX]=".fastq"
+
+    # Directory Structure
+    [FASTQ_DIR]="fastq"
+    [PROCESSED_FASTQ_DIR]="processedFastq"
+    [DOC_DIR]="documentation"
+    
+    # Module Requirements
+    [REQUIRED_MODULES]="fastp/0.20.0"
+)
diff --git a/bash/config/modules/quality_control_config.sh b/bash/config/modules/quality_control_config.sh
new file mode 100644
index 0000000..4593137
--- /dev/null
+++ b/bash/config/modules/quality_control_config.sh
@@ -0,0 +1,41 @@
+#!/bin/bash
+
+# Add to existing QC configurations
+declare -A QC_PATHS=(
+    ["BASE_DIR"]="$HOME/data"
+    ["QC_SUBDIR"]="qualityControl"
+    ["MAX_DEPTH"]=1
+)
+
+declare -A FILE_PATTERNS=(
+    ["FASTQC_ZIP"]="*.zip"
+)
+
+declare -A OPERATION_DEFAULTS=(
+    ["CONFIRM_TIMEOUT"]=30
+    ["UNZIP_BATCH_SIZE"]=10
+    ["PRESERVE_ZIP"]=true
+)
+
+declare -A BAM_QC_OUTPUTS=(
+    ["FLAGSTAT"]="_bamFlagstat.txt"
+    ["QUICKCHECK"]="_bamQuickcheck.txt"
+    ["STATS"]="_bamStats.txt"
+)
+
+declare -A SAMTOOLS_PARAMS=(
+    ["FLAGSTAT"]="-O tsv"
+    ["STATS"]=""
+)
+
+declare -A QC_DIRS=(
+    ["OUTPUT"]="qualityControl"
+    ["LOGS"]="logs"
+)
+
+# Add to existing module configurations
+declare -A REQUIRED_MODULES=(
+    ["GNU"]="gnu/5.4.0"
+    ["SAMTOOLS"]="samtools/1.10"
+    ["FASTQC"]="fastqc/0.11.5"
+)
diff --git a/bash/config/modules/slurm_config.sh b/bash/config/modules/slurm_config.sh
new file mode 100644
index 0000000..ee64e01
--- /dev/null
+++ b/bash/config/modules/slurm_config.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+# bash/config/slurm_config.sh
+
+#' SLURM Configuration Settings
+#' @description Centralized SLURM configuration for the project
+
+# Load project configuration
+source "$HOME/lab_utils/bash/config/project_config.sh"
+
+#' Core SLURM Settings
+declare -A SLURM_CONFIG=(
+    # Resource Management
+    [NODES]="1"
+    [TASKS]="1"
+    [CPUS_PER_TASK]="4"
+    [MEM_PER_CPU]="50G"
+    [MAX_ARRAY_SIZE]="16"
+    [ARRAY_FORMAT]="1-%d%%16"
+    
+    # Job Control
+    [NICE]="10000"
+    [EXCLUDE_NODES]="c[5-22]"
+    
+    # Notification
+    [MAIL_TYPE]="ALL"
+    [MAIL_USER]="luised94@mit.edu"
+    
+    # Paths
+    [LOG_DIR]="slurm_logs"
+    [MAX_LOG_AGE]="30"
+    
+)
+
+#' Module Management
+declare -A SLURM_MODULES=(
+    # Core Modules
+    [GNU]="gnu/5.4.0"
+    [SAMTOOLS]="samtools/1.10"
+    [BOWTIE2]="bowtie2/2.3.5.1"
+    [FASTQC]="fastqc/0.11.5"
+    
+    # Module Loading Order
+    [LOAD_ORDER]="GNU BOWTIE2 SAMTOOLS FASTQC"
+)
+
+#' Resource Configurations
+declare -A SLURM_RESOURCES=(
+    # Alignment Settings
+    [ALIGN_MEM]="50G"
+    [ALIGN_CPUS]="4"
+    [ALIGN_TIME]="24:00:00"
+    
+    # QC Settings
+    [QC_MEM]="20G"
+    [QC_CPUS]="2"
+    [QC_TIME]="4:00:00"
+    
+    # BigWig Settings
+    [BW_MEM]="30G"
+    [BW_CPUS]="2"
+    [BW_TIME]="8:00:00"
+)
+
+#' Reference Genome Settings
+declare -A SLURM_GENOMES=(
+    [BASE_DIR]="$HOME/data/REFGENS"
+    [DEFAULT_GENOME]="SaccharomycescerevisiaeS288C"
+    [GENOME_PATTERN]="*_refgenome.fna"
+    [INDEX_SUFFIX]="_index"
+)
+
+#' Validation Functions
+validate_slurm_config() {
+    local log_file="$1"
+    
+    # Verify required directories
+    if [[ ! -d "${SLURM_GENOMES[BASE_DIR]}" ]]; then
+        log_error "Reference genome directory not found: ${SLURM_GENOMES[BASE_DIR]}" "$log_file"
+        return 1
+    fi
+    
+    # Verify module availability
+    for module in ${SLURM_MODULES[LOAD_ORDER]}; do
+        if ! module avail "${SLURM_MODULES[$module]}" 2>/dev/null; then
+            log_error "Required module not available: ${SLURM_MODULES[$module]}" "$log_file"
+            return 1
+        fi
+    done
+    
+    return 0
+}
+
+#' Get SLURM Options
+#' @param job_type Character Type of job (ALIGN, QC, BW)
+#' @return String SLURM options
+get_slurm_options() {
+    local job_type="$1"
+    
+    echo "--nodes=${SLURM_CONFIG[NODES]} \
+          --cpus-per-task=${SLURM_RESOURCES[${job_type}_CPUS]} \
+          --mem-per-cpu=${SLURM_RESOURCES[${job_type}_MEM]} \
+          --time=${SLURM_RESOURCES[${job_type}_TIME]} \
+          --exclude=${SLURM_CONFIG[EXCLUDE_NODES]} \
+          --nice=${SLURM_CONFIG[NICE]} \
+          --mail-type=${SLURM_CONFIG[MAIL_TYPE]} \
+          --mail-user=${SLURM_CONFIG[MAIL_USER]}"
+}
diff --git a/bash/config/modules/sra_config.sh b/bash/config/modules/sra_config.sh
new file mode 100644
index 0000000..f0a26e6
--- /dev/null
+++ b/bash/config/modules/sra_config.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+
+declare -A SRA_CONFIG=(
+    ["BASE_URL"]="ftp://ftp.sra.ebi.ac.uk/vol1/fastq/"
+    ["DATA_DIR"]="$HOME/data"
+)
+
+# Study-specific configuration
+declare -A EATON_2010=(
+    ["BIOPROJECT"]="PRJNA117641"
+    ["DESCRIPTION"]="ORC precisely positions nucleosomes at origins of replication"
+    ["CONDITION"]="WT-G2-ORC"
+    ["OUTPUT_FILE"]="nnNnH.fastq"
+)
+
+# Sample configuration
+declare -A SAMPLES=(
+    ["WT-G2-ORC-rep1.fastq.gz"]="SRR034475"
+    ["WT-G2-ORC-rep2.fastq.gz"]="SRR034476"
+)
diff --git a/bash/core/config_export.sh b/bash/core/config_export.sh
new file mode 100644
index 0000000..5719fdb
--- /dev/null
+++ b/bash/core/config_export.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+# bash/core/config_export.sh
+
+export_core_config() {
+    local serialized=""
+    for key in "${!CORE_CONFIG[@]}"; do
+        serialized+="${key}=${CORE_CONFIG[$key]};"
+    done
+    export LAB_UTILS_CONFIG_SERIALIZED="${serialized}"
+    
+    # Export individual keys
+    for key in "${!CORE_CONFIG[@]}"; do
+        export "LAB_UTILS_${key}=${CORE_CONFIG[$key]}"
+    done
+}
+
+import_core_config() {
+    declare -g -A CORE_CONFIG
+    while IFS='=' read -r key value; do
+        [[ -n "$key" ]] && CORE_CONFIG[$key]="$value"
+    done < <(echo "$LAB_UTILS_CONFIG_SERIALIZED" | tr ';' '\n')
+}
diff --git a/code_management/001_reassignFiles.sh b/bash/core/file_ops.sh
similarity index 100%
rename from code_management/001_reassignFiles.sh
rename to bash/core/file_ops.sh
diff --git a/bash/core/initialize_lab_environment.sh b/bash/core/initialize_lab_environment.sh
new file mode 100755
index 0000000..6d450c1
--- /dev/null
+++ b/bash/core/initialize_lab_environment.sh
@@ -0,0 +1,203 @@
+#!/bin/bash
+# bash/core/initialize_lab_environment.sh
+
+#' Initialize Lab Utils Environment
+#' @description Core initialization script for lab utilities
+#' @export LAB_UTILS_ROOT, LAB_UTILS_INITIALIZED
+
+# Guard against multiple inclusion
+[[ -n "$LAB_UTILS_INITIALIZED" ]] && return
+
+#' Discover Lab Utils Root Using Git
+#' @description Find repository root using git
+#' @return String Absolute path to repository root
+discover_lab_utils_root() {
+    local repo_root
+    
+    # Check if we're in a git repository
+    if ! git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
+        echo "? Not in a git repository" >&2
+        return 1
+    fi
+    
+    # Get repository root (compatible with git 1.8+)
+    repo_root="$(git rev-parse --show-toplevel 2>/dev/null)" || {
+        echo "? Failed to determine repository root" >&2
+        return 1
+    }
+    
+    # Verify it's the correct repository
+    if [[ ! -d "$repo_root/bash/core" ]]; then
+        echo "? Invalid repository structure" >&2
+        return 1
+    fi
+    
+    echo "$repo_root"
+}
+
+# bash/core/initialize_lab_environment.sh
+
+#' Load Module Configuration
+#' @param module_name Character Name of module
+#' @return Integer 0 if successful
+load_module_config() {
+    local module_name="$1"
+    local config_path="${LAB_UTILS_ROOT}/bash/config/modules/${module_name}_config.sh"
+    
+    log_debug "Loading module configuration: ${module_name}" "${LAB_UTILS_LOG_FILE:-/dev/null}"
+    
+    if [[ ! -f "$config_path" ]]; then
+        log_error "Module configuration not found: ${module_name}_config.sh"
+        return 1
+    fi
+    
+    source "$config_path" || {
+        log_error "Failed to load module configuration: ${module_name}"
+        return 1
+    }
+    
+    return 0
+}
+
+#' Load Laboratory Module
+#' @param module_path Character Path relative to modules directory
+#' @return Integer 0 if successful
+load_lab_module() {
+    local module_path="$1"
+    local module_name="${module_path%%/*}"  # Extract base module name
+    
+    # Load module configuration first
+    load_module_config "$module_name" || {
+        log_error "Failed to load configuration for module: $module_name"
+        return 1
+    }
+    
+    # Then load module implementation
+    local full_path="${LAB_UTILS_ROOT}/bash/modules/${module_path}.sh"
+    if [[ ! -f "$full_path" ]]; then
+        log_error "Module not found: $module_path"
+        return 1
+    fi
+    
+    source "$full_path" || {
+        log_error "Failed to load module: $module_path"
+        return 1
+    }
+    
+    return 0
+}
+#' Load Laboratory Module
+#' @description Load module from lab utils repository
+#' @param module_path Character Path relative to modules directory
+#' @return Integer 0 if successful
+load_lab_module() {
+    local module_path="$1"
+    local full_path="${LAB_UTILS_ROOT}/bash/modules/${module_path}.sh"
+    
+    # Debug output
+    log_debug "Loading module: $module_path" "${LAB_UTILS_LOG_FILE:-/dev/null}"
+    
+    # Validate path
+    if [[ ! -f "$full_path" ]]; then
+        log_error "Module not found: $module_path" "${LAB_UTILS_LOG_FILE:-/dev/null}"
+        return 1
+    fi
+    
+    # Source module with error handling
+    if ! source "$full_path"; then
+        log_error "Failed to load module: $module_path" "${LAB_UTILS_LOG_FILE:-/dev/null}"
+        return 1
+    fi
+    
+    log_debug "Successfully loaded: $module_path" "${LAB_UTILS_LOG_FILE:-/dev/null}"
+    return 0
+}
+
+# Set root directory
+if [[ -z "$LAB_UTILS_ROOT" ]]; then
+    LAB_UTILS_ROOT="$(discover_lab_utils_root)"
+    echo "DEBUG: LAB_UTILS_ROOT set to: $LAB_UTILS_ROOT"
+    echo "DEBUG: Looking for config in: $LAB_UTILS_ROOT/bash/config"
+    export LAB_UTILS_ROOT
+fi
+
+# Verify critical directory exists
+if [[ ! -d "$LAB_UTILS_ROOT" ]]; then
+    echo "ERROR: Lab utils directory not found: $LAB_UTILS_ROOT" >&2
+    return 1
+fi
+
+# Core configuration files (order matters)
+readonly CORE_CONFIG_FILES=(
+    "core_config.sh"      # Base configuration with logging and lock settings.
+)
+
+# Core module files (order matters)
+readonly CORE_MODULES=(
+    "logging.sh"          # Must be first
+    "lock.sh"
+    "path_utils.sh"
+)
+
+# Initialize core configuration
+for config in "${CORE_CONFIG_FILES[@]}"; do
+    config_path="${LAB_UTILS_ROOT}/bash/config/${config}"
+    if [[ -f "$config_path" ]]; then
+        source "$config_path"
+    else
+        echo "ERROR: Required configuration not found: ${config}" >&2
+        return 1
+    fi
+done
+
+# Initialize core modules
+for module in "${CORE_MODULES[@]}"; do
+    module_path="${LAB_UTILS_ROOT}/bash/core/${module}"
+    if [[ -f "$module_path" ]]; then
+        source "$module_path"
+    else
+        echo "ERROR: Required module not found: ${module}" >&2
+        return 1
+    fi
+done
+
+# Export common paths
+export LAB_UTILS_CONFIG_DIR="${LAB_UTILS_ROOT}/bash/config"
+export LAB_UTILS_CORE_DIR="${LAB_UTILS_ROOT}/bash/core"
+export LAB_UTILS_MODULES_DIR="${LAB_UTILS_ROOT}/bash/modules"
+export LAB_UTILS_SCRIPTS_DIR="${LAB_UTILS_ROOT}/bash/scripts"
+export LAB_UTILS_TESTS_DIR="${LAB_UTILS_ROOT}/bash/tests"
+
+
+source "${LAB_UTILS_CORE_DIR}/config_export.sh" || {
+    echo "[ERROR] Failed to load config export functions" >&2
+    return 1
+}
+
+# After loading config
+export_core_config || {
+    echo "[ERROR] Failed to export configuration" >&2
+    return 1
+}
+
+# Verify export worked
+if [[ -z "$LAB_UTILS_CONFIG_SERIALIZED" ]]; then
+    echo "[ERROR] Configuration export failed" >&2
+    return 1
+fi
+# Export critical functions
+export -f import_core_config
+export -f export_core_config
+
+# Initialize logging
+if [[ -z "$LAB_UTILS_LOG_FILE" ]]; then
+    initialize_logging "lab_utils_init"
+fi
+
+# Cleanup stale locks on initialization
+#cleanup_stale_locks 2>/dev/null
+
+# Mark as initialized
+readonly LAB_UTILS_INITIALIZED=1
+
+log_info "Lab environment initialized successfully"
diff --git a/bash/core/lock.sh b/bash/core/lock.sh
new file mode 100644
index 0000000..6724774
--- /dev/null
+++ b/bash/core/lock.sh
@@ -0,0 +1,178 @@
+#!/bin/bash
+# bash/functions/lock_utils.sh
+
+# Source core_config and logging or through initialization.
+#source "$HOME/lab_utils/bash/config/logging_config.sh"
+
+# Shared validation for both acquire and release
+#' Validate Lock Path
+#' @param lock_path Character Path to lock file
+#' @return Integer 0 if valid
+validate_lock_path() {
+    local lock_path="$1"
+    local normalized_path
+    
+    # Basic checks
+    [[ -z "$lock_path" ]] && { 
+        log_error "Empty lock path"
+        return 1
+    }
+    
+    # Ensure path is under user's lock directory
+    normalized_path=$(readlink -f "$lock_path" 2>/dev/null) || {
+        log_error "Cannot resolve path: $lock_path"
+        return 1
+    }
+    
+    local user_lock_dir="${CORE_CONFIG[LOCK_BASE_DIR]}/${USER}"
+    if [[ ! "$normalized_path" =~ ^"$user_lock_dir" ]]; then
+        log_error "Lock must be in user directory: $user_lock_dir"
+        return 1
+    fi
+    
+    # Check parent directory permissions
+    local parent_dir=$(dirname "$normalized_path")
+    if [[ ! -w "$parent_dir" ]]; then
+        log_error "Parent directory not writable: $parent_dir"
+        return 1
+    fi
+    
+    return 0
+}
+
+#' Acquire Lock
+#' @param lock_name Character Lock identifier
+#' @param timeout Integer Seconds to wait
+acquire_lock() {
+    local lock_name="$1"
+    local timeout="${2:-${CORE_CONFIG[LOCK_TIMEOUT]}}"
+    local user_specific_dir="${CORE_CONFIG[LOCK_BASE_DIR]}/${USER}"
+    local lock_file="$user_specific_dir/${lock_name}.lock"
+    
+    # Ensure user lock directory exists
+    create_lock_directory
+    
+    # Attempt to acquire lock
+    local start_time=$(date +%s)
+    while (( $(date +%s) - start_time < timeout )); do
+        if mkdir "$lock_file" 2>/dev/null; then
+            echo $$ > "$lock_file/pid"
+            chmod 700 "$lock_file"  # Only owner can access
+            return 0
+        fi
+        sleep 0.1
+    done
+    
+    return 1
+}
+
+#' Cleanup Stale Locks
+cleanup_stale_locks() {
+    local lock_dir="${CORE_CONFIG[LOCK_BASE_DIR]}"
+    local max_age=3600  # 1 hour
+    local user_specific_dir="${lock_dir}/${USER}"
+    
+    # Only clean user-specific locks
+    if [[ ! -d "$user_specific_dir" ]]; then
+        return 0
+    fi
+    
+    # Find only locks owned by current user
+    find "$user_specific_dir" -type d -name "*.lock" -user "$USER" -mmin +60 | \
+    while read -r lock; do
+        if validate_lock_path "$lock"; then
+            local pid_file="$lock/pid"
+            if [[ -f "$pid_file" ]]; then
+                local stored_pid=$(cat "$pid_file")
+                # Check if process still exists
+                if ! kill -0 "$stored_pid" 2>/dev/null; then
+                    rm -rf "$lock"
+                    log_debug "Removed stale lock: $lock (PID: $stored_pid)"
+                fi
+            else
+                rm -rf "$lock"
+                log_debug "Removed invalid lock: $lock (no PID file)"
+            fi
+        fi
+    done
+    
+    log_info "Cleaned up stale locks for user: $USER"
+}
+
+#' Create Lock Directory
+#' @description Ensure user-specific lock directory exists
+create_lock_directory() {
+    local lock_base="${CORE_CONFIG[LOCK_BASE_DIR]}"
+    local user_dir="$lock_base/$USER"
+    
+    # Create with strict permissions
+    mkdir -p "$lock_base"
+    chmod 1777 "$lock_base"  # Like /tmp
+    
+    mkdir -p "$user_dir"
+    chmod 700 "$user_dir"    # Only user can access
+    
+    return 0
+}
+
+#' Release Lock
+#' @param lock_name Character Lock identifier
+#' @param force Logical Force release even if not owner
+#' @return Integer 0 if successful
+release_lock() {
+    local lock_name="$1"
+    local force="${2:-false}"
+    local user_specific_dir="${CORE_CONFIG[LOCK_BASE_DIR]}/${USER}"
+    local lock_file="$user_specific_dir/${lock_name}.lock"
+    local verbose="${3:-${CORE_CONFIG[VERBOSE]:-false}}"  #  Add verbose parameter
+    
+    # Validate lock path
+    if ! validate_lock_path "$lock_file"; then
+        log_error "Invalid or protected lock path: $lock_file"
+        return 1
+    fi
+    
+    # Check if lock exists
+    if [[ ! -d "$lock_file" ]]; then
+        log_debug "Lock already released: $lock_file"
+        return 0
+    fi
+    
+    # Verify ownership
+    if [[ ! -O "$lock_file" ]]; then
+        log_error "Lock owned by different user: $(stat -c %U "$lock_file")"
+        return 1
+    fi
+    
+    # Check PID
+    local pid_file="$lock_file/pid"
+    if [[ -f "$pid_file" ]]; then
+        local stored_pid
+        stored_pid=$(cat "$pid_file" 2>/dev/null)
+        
+        if [[ "$stored_pid" != "$$" && "$force" != "true" ]]; then
+            # Additional check: see if process is still running
+            if kill -0 "$stored_pid" 2>/dev/null; then
+                log_error "Lock owned by running process: $stored_pid"
+                return 1
+            elif [[ "$force" != "true" ]]; then
+                log_warning "Found stale lock from dead process: $stored_pid"
+            fi
+        fi
+    fi
+    
+    # Safe removal with error checking
+    {
+        rm -f "${lock_file:?}/pid" 2>/dev/null
+        rm -rf "${lock_file:?}"/* 2>/dev/null
+        rmdir "${lock_file:?}" 2>/dev/null
+    } || {
+        log_error "Failed to remove lock: $lock_file"
+        return 1
+    }
+
+    if [[ "$verbose" == "true" ]]; then
+        log_debug "Released lock: $lock_name"  #  Only log if verbose
+    fi
+    return 0
+}
diff --git a/bash/core/logging.sh b/bash/core/logging.sh
new file mode 100644
index 0000000..07a44e9
--- /dev/null
+++ b/bash/core/logging.sh
@@ -0,0 +1,211 @@
+
+#!/bin/bash
+
+# Load required settings for logging_utils
+# Advanced Logging Functions for Bash Scripts
+#
+# Script: 002_logging_functions.sh
+# Description: A set of functions for consistent logging across Bash scripts
+# Author: Your Name
+# Date: 2024-10-18
+
+# Function: extract_script_path
+# Purpose: Extract the full path of the current script
+# Parameters: None
+# Return: Script path
+extract_script_path() {
+  echo "$(readlink -f "${BASH_SOURCE[0]}")"
+}
+
+# Function: get_script_dir
+# Purpose: Extract the directory of the current script
+# Parameters: None
+# Return: Script directory
+get_script_dir() {
+  echo "$(dirname "$(get_script_name)")"
+}
+
+# Function: get_script_basename
+# Purpose: Extract the basename of the current script without extension
+# Parameters: None
+# Return: Script basename
+get_script_basename() {
+  echo "$(basename "$(get_script_name)" .sh)"
+}
+
+
+#' Write Log Entry Atomically
+#' @param entry Character Log entry
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful
+write_log_atomic() {
+    local entry="$1"
+    local log_file="$2"
+    local lock_name="log_$(basename "$log_file")"
+    local retry_count=0
+
+    # Ensure lock directory exists
+    mkdir -p "${CORE_CONFIG[LOCK_BASE_DIR]}" 2>/dev/null
+    mkdir -p "${CORE_CONFIG[DEFAULT_LOG_ROOT]}" 2>/dev/null
+
+    while [[ $retry_count -lt ${CORE_CONFIG[LOCK_RETRY]} ]]; do
+        if acquire_lock "$lock_name"; then
+            echo "$entry" >> "$log_file"
+            local status=$?
+            release_lock "$lock_name"
+            return $status
+        fi
+        ((retry_count++))
+        sleep 1
+    done
+    
+    echo "ERROR: Failed to acquire log lock after ${CORE_CONFIG[LOCK_RETRY]} attempts" >&2
+    return 1
+}
+
+#' Get Run Count
+#' @param log_file Character Path to log file
+#' @return Integer Run count
+get_run_count() {
+    local log_file="$1"
+    local lock_name="log_$(basename "$log_file")"
+    local count=0
+    
+    # Acquire lock for counting
+    if ! acquire_lock "$lock_name"; then
+        echo "0"
+        return 1
+    fi
+    
+    # Count runs with error handling
+    if [[ -f "$log_file" ]]; then
+        count=$(grep -c "${CORE_CONFIG[RUN_SEPARATOR]}" "$log_file" 2>/dev/null || echo "0")
+    fi
+    
+    release_lock "$lock_name"
+    echo "$count"
+}
+
+#' Format Run Entry
+#' @param count Integer Run count
+#' @return String Formatted entry
+format_run_entry() {
+    local count="$1"
+    local timestamp
+    timestamp=$(date +"${CORE_CONFIG[TIMESTAMP_FORMAT]}")
+    
+    if [[ $count -eq 0 ]]; then
+        printf "${CORE_CONFIG[FIRST_RUN_FORMAT]}" \
+            "${CORE_CONFIG[RUN_SEPARATOR]}" \
+            "$timestamp"
+    else
+        printf "${CORE_CONFIG[ENTRY_FORMAT]}" \
+            "${CORE_CONFIG[RUN_SEPARATOR]}" \
+            "$((count + 1))" \
+            "$timestamp"
+    fi
+}
+#' Initialize Logging
+#' @param script_name Character Name of the calling script
+#' @param log_dir Character Optional log directory
+#' @return String Path to log file
+
+initialize_logging() {
+    local script_name="${1:-$(basename "${BASH_SOURCE[1]}" )}"
+    local log_dir="${2:-${CORE_CONFIG[DEFAULT_LOG_ROOT]}}"
+    local log_file
+    
+    # Setup log file
+    log_file="$log_dir/$(date +%Y-%m)/$(date +%Y-%m-%d)_${script_name}.log"
+    mkdir -p "$(dirname "$log_file")"
+    
+    # Get run count atomically
+    local run_count
+    run_count=$(get_run_count "$log_file")
+    
+    # Write header atomically
+    local entry
+    entry=$(format_run_entry "$run_count")
+    write_log_atomic "$entry" "$log_file"
+    log_system_info "$log_file"
+    log_git_info "$log_file"
+    echo "$log_file"
+}
+
+#' Enhanced Log Message
+#' @param level Character Log level
+#' @param message Character Message to log
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful
+log_message() {
+    local level="$1"
+    local message="$2"
+    local log_file="$3"
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    local task_id="${SLURM_ARRAY_TASK_ID:-standalone}"
+    local job_id="${SLURM_JOB_ID:-local}"
+    
+    # Improved level validation
+    local valid_levels=(${CORE_CONFIG[LOG_LEVELS]})
+    local is_valid=0
+    for valid_level in "${valid_levels[@]}"; do
+        if [[ "$level" == "$valid_level" ]]; then
+            is_valid=1
+            break
+        fi
+    done
+    
+    if [[ $is_valid -eq 0 ]]; then
+        echo "Invalid log level: $level (Valid levels: ${CORE_CONFIG[LOG_LEVELS]})" >&2
+        return 1
+    fi
+    
+    # Truncate long messages
+    if [[ ${#message} -gt ${CORE_CONFIG[MAX_MESSAGE_LENGTH]} ]]; then
+        message="${message:0:${CORE_CONFIG[MAX_MESSAGE_LENGTH]}}..."
+    fi
+    # Format entry
+    local log_entry="[${timestamp}] [${level}] [Job:${job_id}] [Task:${task_id}] ${message}"
+    # Console output
+    echo "$log_entry" >&2
+    # File output with atomic writes
+    if [[ -n "$log_file" ]]; then
+        write_log_atomic "$log_entry" "$log_file"
+        return $?
+    fi
+    
+    return 0
+}
+
+#' Log System Information
+#' @param log_file Character Path to log file
+#' @return None
+log_system_info() {
+    local log_file="$1"
+    log_message "INFO" "System Information:" "$log_file"
+    log_message "INFO" "  Bash: $BASH_VERSION" "$log_file"
+    log_message "INFO" "  Host: $(hostname)" "$log_file"
+    log_message "INFO" "  User: $USER" "$log_file"
+    log_message "INFO" "  PWD:  $PWD" "$log_file"
+}
+
+#' Log Git Information
+#' @param log_file Character Path to log file
+#' @return None
+log_git_info() {
+    local log_file="$1"
+    
+    if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
+        log_message "INFO" "Git Information:" "$log_file"
+        log_message "INFO" "  Branch: $(git rev-parse --abbrev-ref HEAD)" "$log_file"
+        log_message "INFO" "  Commit: $(git rev-parse HEAD)" "$log_file"
+    else
+        log_message "WARNING" "Not in a git repository" "$log_file"
+    fi
+}
+
+#' Convenience Logging Functions
+for level in ${CORE_CONFIG[LOG_LEVELS]}; do
+    level_lower=$(echo "$level" | tr '[:upper:]' '[:lower:]')
+    eval "log_${level_lower}() { log_message \"$level\" \"\$1\" \"\$2\"; }"
+done
diff --git a/bash/core/path_utils.sh b/bash/core/path_utils.sh
new file mode 100644
index 0000000..0620979
--- /dev/null
+++ b/bash/core/path_utils.sh
@@ -0,0 +1,80 @@
+#!/bin/bash
+source "$HOME/lab_utils/bash/config/bmc_config.sh"
+
+remove_files_safely() {
+    local pattern="$1"
+    local log_file="$2"
+    
+    log_info "Removing items matching: $pattern" "$log_file"
+    
+    # Find matching items with sizes
+    local items=$(find . -name "$pattern" -exec du -sh {} \; | \
+                 sort -hr)
+    
+    if [[ -z "$items" ]]; then
+        log_info "No items found matching: $pattern" "$log_file"
+        return 0
+    fi
+    
+    # Log items to be removed
+    log_warning "Will remove:" "$log_file"
+    echo "$items" | tee -a "$log_file"
+    
+    # Interactive confirmation if running interactively
+    if [[ -t 0 ]]; then
+        read -p "Proceed with deletion? (y/n) " -r
+        [[ ! $REPLY =~ ^[Yy]$ ]] && return 0
+    fi
+    
+    # Remove items
+    echo "$items" | while read size item; do
+        if rm -rf "${item#./}"; then
+            log_info "Removed: $item ($size)" "$log_file"
+        else
+            log_error "Failed to remove: $item" "$log_file"
+        fi
+    done
+}
+
+#' Check Available Filesystem Space
+#' @param path Character Path to check
+#' @param log_file Character Log file path
+#' @param min_space Numeric Minimum required space in GB
+#' @return Integer 0 if sufficient space
+check_filesystem_space() {
+    local path="$1"
+    local log_file="$2"
+    local min_space="${3:-${CORE_CONFIG[MIN_SPACE_GB]:-10}}"  # Default 10GB if not set
+    
+    # Move to CORE_CONFIG
+    # [MIN_SPACE_GB]="10"
+    
+    local available_gb=$(df -P "$path" | awk 'NR==2 {print $4/1024/1024}')
+    
+    log_info "Available space: ${available_gb}GB" "$log_file"
+    
+    if (( $(echo "$available_gb < $min_space" | bc -l) )); then
+        log_error "Insufficient space: need ${min_space}GB, have ${available_gb}GB" "$log_file"
+        return 1
+    fi
+    return 0
+}
+
+#' Verify Path is Under Expected Filesystem
+#' @param path Character Path to verify
+#' @param expected_fs Character Expected filesystem root
+#' @param log_file Character Log file path
+#' @return Integer 0 if path is valid
+verify_filesystem_path() {
+    local path="$1"
+    local expected_fs="$2"
+    local log_file="$3"
+    
+    local real_path=$(readlink -f "$path")
+    if [[ "$real_path" != "$expected_fs"* ]]; then
+        log_error "Path not under expected filesystem: $path" "$log_file"
+        log_error "Expected root: $expected_fs" "$log_file"
+        return 1
+    fi
+    return 0
+}
diff --git a/bash/modules/alignment_handler.sh b/bash/modules/alignment_handler.sh
new file mode 100644
index 0000000..010113a
--- /dev/null
+++ b/bash/modules/alignment_handler.sh
@@ -0,0 +1,57 @@
+#!/bin/bash
+
+source "../config/slurm_config.sh"
+
+function calculate_indices() {
+    local task_id="$1"
+    local fastq_count="$2"
+    
+    local genome_index=$(( (task_id - 1) / fastq_count ))
+    local fastq_index=$(( (task_id - 1) % fastq_count ))
+    
+    echo "${genome_index}:${fastq_index}"
+}
+
+function get_output_names() {
+    local fastq_path="$1"
+    local genome_path="$2"
+    
+    local fastq_id=$(basename "${fastq_path%.fastq}")
+    local genome_name=$(basename "$genome_path" | cut -d_ -f1)
+    
+    echo "${fastq_id}:${genome_name}"
+}
+
+function perform_alignment() {
+    local genome_path="$1"
+    local fastq_path="$2"
+    local output_dir="$3"
+    local threads="$4"
+    
+    local index_base="${genome_path%_refgenome.fna}${FILE_PATTERNS[INDEX_SUFFIX]}"
+    local names=$(get_output_names "$fastq_path" "$genome_path")
+    local fastq_id=${names%:*}
+    local genome_name=${names#*:}
+    local output_bam="${output_dir}/${fastq_id}_${genome_name}.bam"
+    
+    log_info "Starting alignment: $fastq_id to $genome_name"
+    
+    # Alignment pipeline
+    if ! bowtie2 -x "$index_base" \
+                 -U "$fastq_path" \
+                 -p "$threads" \
+                 ${ALIGNMENT_CONFIG[BOWTIE_PARAMS]} |
+         samtools view -@ "$threads" -b - |
+         samtools sort -@ "$threads" -o "$output_bam" -; then
+        log_error "Alignment failed for: $fastq_id to $genome_name"
+        return 1
+    fi
+    
+    if ! samtools index "$output_bam"; then
+        log_error "Index creation failed for: $output_bam"
+        return 1
+    fi
+    
+    log_info "Completed alignment: $output_bam"
+    return 0
+}
diff --git a/bash/modules/archive_handler.sh b/bash/modules/archive_handler.sh
new file mode 100644
index 0000000..e8b1c3a
--- /dev/null
+++ b/bash/modules/archive_handler.sh
@@ -0,0 +1,80 @@
+#!/bin/bash
+
+source "../config/quality_control_config.sh"
+
+function find_zip_files() {
+    local base_dir="$1"
+    local pattern="${FILE_PATTERNS[FASTQC_ZIP]}"
+    
+    log_info "Searching for ZIP files in: $base_dir"
+    
+    local files=$(find "$base_dir" -type f -name "$pattern")
+    
+    if [ -z "$files" ]; then
+        log_warning "No ZIP files found"
+        return 1
+    fi
+    
+    echo "$files"
+}
+
+function verify_zip_file() {
+    local zip_file="$1"
+    
+    if ! unzip -t "$zip_file" >/dev/null 2>&1; then
+        log_error "Invalid or corrupted ZIP file: $zip_file"
+        return 1
+    fi
+    
+    return 0
+}
+
+function unzip_in_place() {
+    local zip_file="$1"
+    local preserve="${2:-${OPERATION_DEFAULTS[PRESERVE_ZIP]}}"
+    
+    local dir=$(dirname "$zip_file")
+    local filename=$(basename "$zip_file")
+    
+    log_info "Unzipping: $filename"
+    
+    (
+        cd "$dir" || {
+            log_error "Failed to change to directory: $dir"
+            return 1
+        }
+        
+        if ! unzip -o "$filename"; then
+            log_error "Failed to unzip: $filename"
+            return 1
+        fi
+        
+        if [ "$preserve" = false ]; then
+            rm "$filename" || log_warning "Failed to remove ZIP file: $filename"
+        fi
+    )
+}
+
+function process_zip_files() {
+    local files=("$@")
+    local total=${#files[@]}
+    local success=0
+    local failed=0
+    
+    log_info "Processing $total ZIP files"
+    
+    for file in "${files[@]}"; do
+        if verify_zip_file "$file"; then
+            if unzip_in_place "$file"; then
+                ((success++))
+            else
+                ((failed++))
+            fi
+        else
+            ((failed++))
+        fi
+    done
+    
+    log_info "Processed files - Success: $success, Failed: $failed"
+    return $((failed > 0))
+}
diff --git a/bash/modules/bam_comparer.sh b/bash/modules/bam_comparer.sh
new file mode 100644
index 0000000..ba8276a
--- /dev/null
+++ b/bash/modules/bam_comparer.sh
@@ -0,0 +1,69 @@
+#!/bin/bash
+
+function generate_output_name() {
+    local sample_bam="$1"
+    local input_bam="$2"
+    local time_id="$3"
+    local output_dir="$4"
+    
+    local sample_name=$(basename "${sample_bam%.bam}" | awk -F'_' '{print $1}')
+    local input_name=$(basename "${input_bam%.bam}" | awk -F'_' '{print $1}')
+    
+    echo "${output_dir}/${time_id}_${sample_name}_${input_name}_bamcomp.bw"
+}
+
+function build_bamcompare_command() {
+    local sample_bam="$1"
+    local input_bam="$2"
+    local output_file="$3"
+    local threads="$4"
+    
+    local cmd="bamCompare"
+    cmd+=" -b1 \"$sample_bam\""
+    cmd+=" -b2 \"$input_bam\""
+    cmd+=" -o \"$output_file\""
+    cmd+=" --binSize ${BAMCOMPARE_PARAMS[BIN_SIZE]}"
+    cmd+=" --normalizeUsing ${BAMCOMPARE_PARAMS[NORMALIZATION]}"
+    cmd+=" --scaleFactorsMethod ${BAMCOMPARE_PARAMS[SCALE_METHOD]}"
+    cmd+=" --effectiveGenomeSize ${BAMCOMPARE_PARAMS[GENOME_SIZE]}"
+    cmd+=" --minMappingQuality ${BAMCOMPARE_PARAMS[MIN_MAPPING_QUALITY]}"
+    cmd+=" --operation ${BAMCOMPARE_PARAMS[OPERATION]}"
+    cmd+=" --numberOfProcessors $threads"
+    
+    if [ "${BAMCOMPARE_PARAMS[IGNORE_DUPLICATES]:-true}" = true ]; then
+        cmd+=" --ignoreDuplicates"
+    fi
+    
+    for region in "${BAMCOMPARE_PARAMS[IGNORE_REGIONS][@]}"; do
+        cmd+=" --ignoreForNormalization $region"
+    done
+    
+    echo "$cmd"
+}
+
+function run_comparison() {
+    local sample_bam="$1"
+    local input_bam="$2"
+    local output_file="$3"
+    local threads="$4"
+    
+    log_info "Running BAM comparison"
+    log_info "Sample: $sample_bam"
+    log_info "Input: $input_bam"
+    log_info "Output: $output_file"
+    
+    local start_time=$(date +%s)
+    
+    local cmd=$(build_bamcompare_command "$sample_bam" "$input_bam" "$output_file" "$threads")
+    
+    if ! eval "$cmd"; then
+        log_error "Comparison failed"
+        return 1
+    }
+    
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    
+    log_info "Comparison completed in $duration seconds"
+    return 0
+}
diff --git a/bash/modules/bam_processor.sh b/bash/modules/bam_processor.sh
new file mode 100644
index 0000000..d9221b2
--- /dev/null
+++ b/bash/modules/bam_processor.sh
@@ -0,0 +1,111 @@
+#!/bin/bash
+
+source "../config/quality_control_config.sh"
+
+function setup_qc_directories() {
+    local base_dir="$1"
+    
+    log_info "Setting up QC directories"
+    
+    for dir in "${QC_DIRS[@]}"; do
+        local full_path="$base_dir/$dir"
+        mkdir -p "$full_path" || {
+            log_error "Failed to create directory: $full_path"
+            return 1
+        }
+    done
+    
+    return 0
+}
+
+function find_bam_files() {
+    local base_dir="$1"
+    
+    log_info "Finding BAM files"
+    
+    local bam_files=$(find "$base_dir" -type f -name "*.bam")
+    
+    if [ -z "$bam_files" ]; then
+        log_error "No BAM files found in: $base_dir"
+        return 1
+    }
+    
+    echo "$bam_files"
+}
+
+function get_output_basename() {
+    local bam_path="$1"
+    
+    echo "$(basename "${bam_path%.bam}")"
+}
+
+function run_flagstat() {
+    local bam_file="$1"
+    local output_file="$2"
+    
+    log_info "Running flagstat on: $bam_file"
+    
+    if ! samtools flagstat ${SAMTOOLS_PARAMS[FLAGSTAT]} "$bam_file" > "$output_file"; then
+        log_error "Flagstat failed for: $bam_file"
+        return 1
+    }
+    
+    return 0
+}
+
+function run_quickcheck() {
+    local bam_file="$1"
+    local output_file="$2"
+    
+    log_info "Running quickcheck on: $bam_file"
+    
+    if samtools quickcheck "$bam_file"; then
+        echo -e 'QUICKCHECK\tTRUE' > "$output_file"
+    else
+        echo -e 'QUICKCHECK\tFALSE' > "$output_file"
+        log_warning "Quickcheck failed for: $bam_file"
+    fi
+    
+    return 0
+}
+
+function run_stats() {
+    local bam_file="$1"
+    local output_file="$2"
+    
+    log_info "Running stats on: $bam_file"
+    
+    if ! samtools stats ${SAMTOOLS_PARAMS[STATS]} "$bam_file" > "$output_file"; then
+        log_error "Stats failed for: $bam_file"
+        return 1
+    }
+    
+    return 0
+}
+
+function process_bam_file() {
+    local bam_file="$1"
+    local qc_dir="$2"
+    
+    local basename=$(get_output_basename "$bam_file")
+    local success=true
+    
+    # Run QC tools
+    for tool in "${!BAM_QC_OUTPUTS[@]}"; do
+        local output_file="${qc_dir}/${basename}${BAM_QC_OUTPUTS[$tool]}"
+        
+        case "$tool" in
+            "FLAGSTAT")
+                run_flagstat "$bam_file" "$output_file" || success=false
+                ;;
+            "QUICKCHECK")
+                run_quickcheck "$bam_file" "$output_file" || success=false
+                ;;
+            "STATS")
+                run_stats "$bam_file" "$output_file" || success=false
+                ;;
+        esac
+    done
+    
+    $success
+}
diff --git a/bash/modules/coverage_processor.sh b/bash/modules/coverage_processor.sh
new file mode 100644
index 0000000..d4a717a
--- /dev/null
+++ b/bash/modules/coverage_processor.sh
@@ -0,0 +1,81 @@
+#!/bin/bash
+
+source "../config/coverage_config.sh"
+
+function setup_coverage_directories() {
+    local base_dir="$1"
+    
+    log_info "Setting up coverage directories"
+    
+    for dir in "${OUTPUT_DIRS[@]}"; do
+        local full_path="$base_dir/$dir"
+        mkdir -p "$full_path" || {
+            log_error "Failed to create directory: $full_path"
+            return 1
+        }
+    done
+    
+    return 0
+}
+
+function find_bam_files() {
+    local base_dir="$1"
+    local pattern="${FILE_PATTERNS[BAM_SUFFIX]}"
+    
+    log_info "Finding BAM files"
+    
+    local bam_files=$(find "$base_dir" -type f -name "*${pattern}" | sort)
+    
+    if [ -z "$bam_files" ]; then
+        log_error "No BAM files found matching pattern: $pattern"
+        return 1
+    }
+    
+    echo "$bam_files"
+}
+
+function generate_output_name() {
+    local bam_path="$1"
+    local time_id="$2"
+    local output_dir="$3"
+    
+    local basename=$(basename "${bam_path%.bam}")
+    echo "${output_dir}/${time_id}_${basename}${FILE_PATTERNS[BIGWIG_SUFFIX]}"
+}
+
+function build_bamcoverage_command() {
+    local input_file="$1"
+    local output_file="$2"
+    
+    local cmd="bamCoverage"
+    cmd+=" -b \"$input_file\""
+    cmd+=" -o \"$output_file\""
+    cmd+=" --binSize ${COVERAGE_PARAMS[BIN_SIZE]}"
+    cmd+=" --normalizeUsing ${COVERAGE_PARAMS[NORMALIZATION]}"
+    cmd+=" --minMappingQuality ${COVERAGE_PARAMS[MIN_MAPPING_QUALITY]}"
+    
+    if [ "${COVERAGE_PARAMS[IGNORE_DUPLICATES]}" = true ]; then
+        cmd+=" --ignoreDuplicates"
+    fi
+    
+    echo "$cmd"
+}
+
+function process_bam_coverage() {
+    local bam_file="$1"
+    local output_file="$2"
+    
+    log_info "Processing coverage for: $bam_file"
+    log_info "Output: $output_file"
+    
+    local cmd=$(build_bamcoverage_command "$bam_file" "$output_file")
+    
+    log_info "Executing: $cmd"
+    if ! eval "$cmd"; then
+        log_error "Coverage generation failed for: $bam_file"
+        return 1
+    fi
+    
+    log_info "Successfully generated coverage for: $bam_file"
+    return 0
+}
diff --git a/bash/modules/fasta_processor.sh b/bash/modules/fasta_processor.sh
new file mode 100644
index 0000000..d92993f
--- /dev/null
+++ b/bash/modules/fasta_processor.sh
@@ -0,0 +1,104 @@
+#!/bin/bash
+
+source "../config/genome_config.sh"
+
+function validate_fasta() {
+    local file="$1"
+    
+    log_info "Validating FASTA file: $file"
+    
+    if [ ! -f "$file" ]; then
+        log_error "File not found: $file"
+        return 1
+    fi
+    
+    if ! grep -q '^>' "$file"; then
+        log_error "Invalid FASTA format: No headers found"
+        return 1
+    fi
+    
+    return 0
+}
+
+function create_backup() {
+    local source_file="$1"
+    local backup_suffix="${2:-${GENOME_FILES[BACKUP_SUFFIX]}}"
+    
+    local backup_file="${source_file%_refgenome.fna}${backup_suffix}"
+    
+    log_info "Creating backup: $backup_file"
+    
+    if [ -f "$backup_file" ]; then
+        log_warning "Backup file already exists: $backup_file"
+        return 0
+    fi
+    
+    if ! cp "$source_file" "$backup_file"; then
+        log_error "Failed to create backup"
+        return 1
+    fi
+    
+    return 0
+}
+
+function reformat_headers_awk() {
+    local input_file="$1"
+    local output_file="$2"
+    local old_prefix="${HEADER_PATTERNS[OLD_PREFIX]}"
+    local new_prefix="${HEADER_PATTERNS[NEW_PREFIX]}"
+    local delimiter="${HEADER_PATTERNS[DELIMITER]}"
+    
+    log_info "Reformatting headers with AWK"
+    
+    awk -v old="$old_prefix" -v new="$new_prefix" -v delim="$delimiter" '
+        /^>/ {
+            gsub(old, new, $6)
+            printf(">%s%s\n", $6, $7)
+            next
+        }
+        {
+            print $0
+        }' "$input_file" | 
+    sed "s/${delimiter}.*//" > "$output_file"
+}
+
+function reformat_headers_while() {
+    local input_file="$1"
+    local output_file="$2"
+    local old_prefix="${HEADER_PATTERNS[OLD_PREFIX]}"
+    local new_prefix="${HEADER_PATTERNS[NEW_PREFIX]}"
+    local delimiter="${HEADER_PATTERNS[DELIMITER]}"
+    
+    log_info "Reformatting headers with while loop"
+    
+    while IFS= read -r line; do
+        if [[ $line == ">"* ]]; then
+            echo "${line/$old_prefix/$new_prefix}" | cut -d"$delimiter" -f1
+        else
+            echo "$line"
+        fi
+    done < "$input_file" > "$output_file"
+}
+
+function verify_conversion() {
+    local original="$1"
+    local converted="$2"
+    
+    log_info "Verifying conversion"
+    
+    local orig_count=$(grep -c '^>' "$original")
+    local conv_count=$(grep -c '^>' "$converted")
+    
+    if [ "$orig_count" -ne "$conv_count" ]; then
+        log_error "Header count mismatch: Original=$orig_count, Converted=$conv_count"
+        return 1
+    fi
+    
+    if grep -q "${HEADER_PATTERNS[OLD_PREFIX]}" "$converted"; then
+        log_error "Old prefix still present in converted file"
+        return 1
+    fi
+    
+    log_info "Conversion verified successfully"
+    return 0
+}
diff --git a/bash/modules/fastq/bmc_handler.sh b/bash/modules/fastq/bmc_handler.sh
new file mode 100644
index 0000000..f04d0d2
--- /dev/null
+++ b/bash/modules/fastq/bmc_handler.sh
@@ -0,0 +1,60 @@
+#!/bin/bash
+# bash/modules/fastq/bmc_handler.sh
+
+#' BMC Data Handler Functions
+#' @description Functions for BMC data management
+
+source "${LAB_UTILS_ROOT}/bash/config/modules/bmc_config.sh"
+
+verify_host() {
+    local current_host=$(hostname)
+    [[ "$current_host" == "luria" ]] || {
+        log_error "Must run on luria.mit.edu (current: $current_host)"
+        return 1
+    }
+}
+
+validate_bmc_paths() {
+    local experiment_id="$1"
+    local log_file="$2"
+
+    # Construct paths using exact locations
+    local bmc_path="${BMC_CONFIG[SOURCE_FS]}/$experiment_id"
+    local local_path="${BMC_CONFIG[TARGET_FS]}/$experiment_id/fastq"
+    
+    # Check source exists
+    if [[ ! -d "$bmc_path" ]]; then
+        log_error "BMC directory not found: $bmc_path" "$log_file"
+        log_error "Please verify the experiment ID and try again" "$log_file"
+        return 1
+    fi
+    
+    # Check space on target filesystem
+    if ! check_filesystem_space "$local_path" "$log_file"; then
+        return 1
+    fi
+    
+    # Create directory
+    if ! mkdir -p "$local_path"; then
+        log_error "Failed to create directory: $local_path" "$log_file"
+        return 1
+    fi
+    
+    echo -n "$bmc_path:$local_path"
+}
+
+download_from_bmc() {
+    local paths="$1"
+    local log_file="$2"
+
+    local bmc_path=${paths%:*}
+    local local_path=${paths#*:}
+    
+    log_info "Starting download from: $bmc_path" "$log_file"
+    
+    if ! srun rsync ${PROJECT_CONFIG[RSYNC_OPTIONS]} "$bmc_path/" "$local_path/"; then
+        log_error "Download failed" "$log_file"
+        return 1
+    fi
+    return 0
+}
diff --git a/bash/modules/fastq/fastq_processor.sh b/bash/modules/fastq/fastq_processor.sh
new file mode 100644
index 0000000..e6c885e
--- /dev/null
+++ b/bash/modules/fastq/fastq_processor.sh
@@ -0,0 +1,57 @@
+#!/bin/bash
+#' Move fastq files inside directories to current working directory.
+#' @param target_dir Character Target directory
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful, 1 otherwise
+move_fastq_files_to_current_directory() {
+    local target_dir="$1"
+    local log_file="$2"
+    log_info "Organizing FASTQ files in: $target_dir" "$log_file"
+    # Store current directory
+    local current_dir=$(pwd)
+    # Change to target directory
+    cd "$target_dir" || {
+        log_error "Failed to access directory: $target_dir" "$log_file"
+        return 1
+    }
+    # Move files to root of target directory
+    find . -type f -name "*.fastq" -exec mv {} . \; || {
+        log_error "Failed to move FASTQ files" "$log_file"
+        cd "$current_dir"
+        return 1
+    }
+    # Return to original directory
+    cd "$current_dir"
+    log_info "FASTQ files organized successfully" "$log_file"
+    return 0
+}
+
+clean_experiment_directory() {
+    local target_dir="$1"
+    local log_file="$2"
+    log_info "Starting cleanup process in: $target_dir" "$log_file"
+    # Store current directory
+    local current_dir=$(pwd)
+    # Verify we're in the correct directory structure
+    if [[ "$target_dir" != "${BMC_CONFIG[TARGET_FS]}"* ]]; then
+        log_error "Invalid target directory: $target_dir" "$log_file"
+        log_error "Must be under: ${BMC_CONFIG[TARGET_FS]}" "$log_file"
+        return 1
+    fi
+    # Change to target directory
+    cd "$target_dir" || {
+        log_error "Failed to access directory: $target_dir" "$log_file"
+        return 1
+    }
+
+    # Process cleanup patterns
+    local -a patterns=(${BMC_CONFIG[CLEANUP_DIRS]} ${BMC_CONFIG[CLEANUP_FILES]})
+    for pattern in "${patterns[@]}"; do
+        remove_files_safely "$pattern" "$log_file"
+    done
+    # Return to original directory
+    cd "$current_dir"
+    
+    log_info "Cleanup process completed" "$log_file"
+    return 0
+}
diff --git a/bash/modules/fastq_consolidator_processor.sh b/bash/modules/fastq_consolidator_processor.sh
new file mode 100644
index 0000000..971c29b
--- /dev/null
+++ b/bash/modules/fastq_consolidator_processor.sh
@@ -0,0 +1,153 @@
+#!/bin/bash
+# bash/functions/fastq_processor.sh
+
+source "$HOME/lab_utils/bash/functions/logging_utils.sh"
+
+#' Validate FASTQ Processing Input
+#' @param experiment_id Character Experiment identifier
+#' @param log_file Character Log file path
+#' @return String Experiment directory path
+validate_fastq_input() {
+    local experiment_id="$1"
+    local log_file="$2"
+    
+    log_info "Validating input for experiment: $experiment_id" "$log_file"
+    
+    # Validate experiment ID format
+    if [[ ! "$experiment_id" =~ ^[0-9]{6}Bel$ ]]; then
+        log_error "Invalid experiment ID format: $experiment_id" "$log_file"
+        return 1
+    fi
+    
+    local experiment_dir="$HOME/data/$experiment_id"
+    if [[ ! -d "$experiment_dir" ]]; then
+        log_error "Experiment directory not found: $experiment_dir" "$log_file"
+        return 1
+    fi
+    
+    echo -n "$experiment_dir"
+}
+
+#' Setup FASTQ Processing Directories
+#' @param experiment_dir Character Experiment directory
+#' @param log_file Character Log file path
+#' @return String Output directory path
+setup_fastq_directories() {
+    local experiment_dir="$1"
+    local log_file="$2"
+    
+
+    local output_dir="${experiment_dir}/fastq"
+    log_info "Setting up output directory: $output_dir" "$log_file"
+    
+    mkdir -p "$output_dir" || {
+        log_error "Failed to create output directory: $output_dir" "$log_file"
+        return 1
+    }
+    
+    echo -n "$output_dir"
+}
+
+#' Find FASTQ Files
+#' @param experiment_dir Character Experiment directory
+#' @param log_file Character Log file path
+#' @return Array FASTQ file paths
+
+find_fastq_files() {
+    local experiment_dir="$1"
+    local log_file="$2"
+    
+    log_info "Searching for FASTQ files in: $experiment_dir/${PROJECT_CONFIG[FASTQ_DIR]}" "$log_file"
+    
+    # Validate directory exists
+    if [[ ! -d "$experiment_dir/${PROJECT_CONFIG[FASTQ_DIR]}" ]]; then
+        log_error "FASTQ directory not found: $experiment_dir/${PROJECT_CONFIG[FASTQ_DIR]}" "$log_file"
+        return 1
+    fi
+    
+    # Build exclude patterns array
+    local -a exclude_patterns=()
+    for pattern in ${PROJECT_CONFIG[FASTQ_EXCLUDE]}; do
+        exclude_patterns+=( "!" "-name" "$pattern" )
+    done
+    
+    # Execute find with error handling
+    local find_output
+    if ! find_output=$(find "$experiment_dir/${PROJECT_CONFIG[FASTQ_DIR]}" -type f \
+        -name "${PROJECT_CONFIG[FASTQ_PATTERN]}" \
+        "${exclude_patterns[@]}" 2>&1 | sort); then
+        log_error "Find command failed: $find_output" "$log_file"
+        return 1
+    fi
+    
+    # Check if any files were found
+    if [[ -z "$find_output" ]]; then
+        log_warning "No FASTQ files found matching pattern: ${PROJECT_CONFIG[FASTQ_PATTERN]}" "$log_file"
+        return 0
+    fi
+    
+    echo -n "$find_output"
+}
+
+#' Process FASTQ Files
+#' @param experiment_dir Character Experiment directory
+#' @param output_dir Character Output directory
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful
+consolidate_fastq_files_by_id() {
+    local experiment_dir="$1"
+    local output_dir="$2"
+    local log_file="$3"
+
+    local -a fastq_files
+    mapfile -t fastq_files < <(find_fastq_files "$experiment_dir" "$log_file")
+
+    local initial_count=${#fastq_files[@]}
+    log_info "Found $initial_count FASTQ files to process" "$log_file"
+
+    for file in "${fastq_files[@]}"; do
+        local basename=$(basename "$file")
+        # Split by both - and _ and look for 5-6 digit pattern
+        local id=""
+        local parts
+        IFS='_-' read -ra parts <<< "$basename"
+        for part in "${parts[@]}"; do
+            if [[ $part =~ ^[0-9]{5,6}$ ]]; then
+                id="$part"
+                break
+            fi
+        done
+
+        if [[ -z "$id" ]]; then
+            log_error "Could not extract ID from filename: $basename" "$log_file"
+            return 1
+        fi
+        log_info "Extract id: $id"
+        local output_file="$output_dir/${id}${PROJECT_CONFIG[FASTQ_SUFFIX]}"
+        log_info "Processing: $basename -> $(basename "$output_file")" "$log_file"
+
+        # Use temporary file for safety
+        local temp_output="${output_file}.tmp"
+        if ! cat "$file" >> "$temp_output"; then
+            log_error "Failed to process file: $file" "$log_file"
+            rm -f "$temp_output"  # Clean up temp file
+            return 1
+        fi
+        
+        # Move temp file to final location
+        if ! mv "$temp_output" "$output_file"; then
+            log_error "Failed to finalize output file: $output_file" "$log_file"
+            return 1
+        fi
+        
+        # Only remove original after successful processing
+        if ! rm "$file"; then
+            log_error "Failed to remove original file: $file" "$log_file"
+            return 1
+        fi
+    
+        log_info "Successfully processed and removed: $basename" "$log_file"
+    done
+    
+    return 0
+}
diff --git a/bash/modules/feature_data_handler.sh b/bash/modules/feature_data_handler.sh
new file mode 100644
index 0000000..1a39ba4
--- /dev/null
+++ b/bash/modules/feature_data_handler.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+
+source "../functions/git_handler.sh"
+
+function setup_feature_directory() {
+    local base_dir="${FEATURE_DATA[BASE_DIR]}"
+    
+    log_info "Setting up feature data directory: $base_dir"
+    
+    if [ ! -d "$base_dir" ]; then
+        if ! mkdir -p "$base_dir"; then
+            log_error "Failed to create directory: $base_dir"
+            return 1
+        fi
+        log_info "Created directory: $base_dir"
+    fi
+    
+    echo "$base_dir"
+}
+
+function download_rossi_data() {
+    local base_dir="$1"
+    local repo_config="${REPOSITORIES[ROSSI_2021]}"
+    local target_dir="${base_dir}/${repo_config[dir]}"
+    
+    log_info "Downloading Rossi 2021 data"
+    
+    if ! clone_repository "${repo_config[url]}" "$target_dir" "${repo_config[depth]}"; then
+        return 1
+    fi
+    
+    if ! verify_repository "$target_dir"; then
+        return 1
+    fi
+    
+    log_info "Successfully downloaded Rossi 2021 data"
+    return 0
+}
diff --git a/code_management/create_script.sh b/bash/modules/file_operations.sh
old mode 100755
new mode 100644
similarity index 97%
rename from code_management/create_script.sh
rename to bash/modules/file_operations.sh
index 5f39321..03461cb
--- a/code_management/create_script.sh
+++ b/bash/modules/file_operations.sh
@@ -1,5 +1,4 @@
-#/bin/bash
-
+#!/bin/bash
 determine_next_experiment_id() {
     #Find all experiment*.md files, extract second element using _ delimiter. If it is three digit number,
     # select the maximum. if none is found, outputs 001.
@@ -12,6 +11,7 @@ determine_next_experiment_id() {
         }
     ')
 }
+
 # Create the name of the file, find the template location (./templates/ relative to the script location)
 # Used sed to replace the tags on the template file with appropriate values. 
 #TODO Have to add descriptive name section that reads in name.
@@ -27,5 +27,3 @@ create_new_experiment() {
     #nvim $filename
     echo $experiment_index
 }
-
-create_new_experiment
diff --git a/bash/modules/filter_fastq_processor.sh b/bash/modules/filter_fastq_processor.sh
new file mode 100644
index 0000000..4f6a823
--- /dev/null
+++ b/bash/modules/filter_fastq_processor.sh
@@ -0,0 +1,82 @@
+
+#!/bin/bash
+
+source "../config/fastq_processing_config.sh"
+
+function setup_output_directories() {
+    local base_dir="$1"
+    
+    log_info "Setting up output directories"
+    
+    for dir in "${OUTPUT_DIRS[@]}"; do
+        local full_path="$base_dir/$dir"
+        mkdir -p "$full_path" || {
+            log_error "Failed to create directory: $full_path"
+            return 1
+        }
+    done
+    
+    return 0
+}
+
+function find_input_files() {
+    local base_dir="$1"
+    
+    log_info "Finding input FASTQ files"
+    
+    local exclude_pattern=""
+    for pattern in "${FILE_PATTERNS[EXCLUDE_PATTERNS][@]}"; do
+        exclude_pattern+=" ! -name \"$pattern\""
+    done
+    
+    eval "find \"$base_dir\" -type f -name \"${FILE_PATTERNS[INPUT]}\" $exclude_pattern"
+}
+
+function get_output_paths() {
+    local input_file="$1"
+    local base_dir="$2"
+    
+    local filename=$(basename "$input_file")
+    local processed_name="${FILE_PATTERNS[OUTPUT_PREFIX]}${filename}"
+    
+    echo "${base_dir}/${OUTPUT_DIRS[PROCESSED]}/${processed_name}:${base_dir}/${OUTPUT_DIRS[LOGS]}/${processed_name%.fastq}.json"
+}
+
+function build_fastp_command() {
+    local input_file="$1"
+    local output_file="$2"
+    local json_file="$3"
+    
+    local params=("${FASTP_PARAMS[BASE_PARAMS][@]}")
+    
+    if [[ $input_file =~ "Eaton" ]]; then
+        log_info "Using Eaton-specific parameters"
+        params+=("${FASTP_PARAMS[EATON][@]}")
+    else
+        params+=("${FASTP_PARAMS[STANDARD][@]}")
+    fi
+    
+    echo "fastp -i \"$input_file\" -o \"$output_file\" --json \"$json_file\" ${params[*]}"
+}
+
+function process_fastq_file() {
+    local input_file="$1"
+    local base_dir="$2"
+    
+    log_info "Processing file: $input_file"
+    
+    local paths=$(get_output_paths "$input_file" "$base_dir")
+    local output_file=${paths%:*}
+    local json_file=${paths#*:}
+    
+    local cmd=$(build_fastp_command "$input_file" "$output_file" "$json_file")
+    
+    log_info "Executing: $cmd"
+    if ! eval "$cmd"; then
+        log_error "Processing failed for: $input_file"
+        return 1
+    fi
+    
+    log_info "Successfully processed: $input_file"
+    return 0
+}
diff --git a/bash/modules/function_loader.sh b/bash/modules/function_loader.sh
new file mode 100644
index 0000000..d791b7c
--- /dev/null
+++ b/bash/modules/function_loader.sh
@@ -0,0 +1,106 @@
+#!/bin/bash
+
+source "../config/environment_config.sh"
+
+function validate_directory() {
+    local dir="$1"
+    
+    log_info "Validating directory: $dir"
+    
+    if [ ! -d "$dir" ]; then
+        log_error "Directory not found: $dir"
+        return 1
+    fi
+    
+    if [ ! -r "$dir" ]; then
+        log_error "Directory not readable: $dir"
+        return 1
+    fi
+    
+    return 0
+}
+
+function find_function_files() {
+    local base_dir="$1"
+    local pattern="${FILE_PATTERNS[FUNCTIONS]}"
+    
+    log_info "Finding function files"
+    
+    local exclude_pattern=""
+    for pattern in "${FILE_PATTERNS[EXCLUDE_PATTERNS][@]}"; do
+        exclude_pattern+=" ! -name \"$pattern\""
+    done
+    
+    eval "find \"$base_dir\" -type f -name \"$pattern\" $exclude_pattern"
+}
+
+function validate_function_file() {
+    local file="$1"
+    
+    log_info "Validating function file: $file"
+    
+    if [ ! -f "$file" ]; then
+        log_error "File not found: $file"
+        return 1
+    fi
+    
+    if [ ! -r "$file" ]; then
+        log_error "File not readable: $file"
+        return 1
+    fi
+    
+    # Optional: Add syntax check
+    if command -v bash > /dev/null; then
+        if ! bash -n "$file"; then
+            log_error "Syntax error in: $file"
+            return 1
+        fi
+    fi
+    
+    return 0
+}
+
+function load_function_file() {
+    local file="$1"
+    
+    log_info "Loading functions from: $file"
+    
+    if ! source "$file" 2>/dev/null; then
+        log_error "Failed to source: $file"
+        return 1
+    fi
+    
+    return 0
+}
+
+function load_priority_functions() {
+    local base_dir="$1"
+    
+    log_info "Loading priority functions"
+    
+    for file in "${LOAD_ORDER[PRIORITY][@]}"; do
+        local full_path="$base_dir/$file"
+        if [ -f "$full_path" ]; then
+            load_function_file "$full_path" || continue
+        else
+            log_warning "Priority file not found: $file"
+        fi
+    done
+}
+
+function load_remaining_functions() {
+    local base_dir="$1"
+    
+    log_info "Loading remaining functions"
+    
+    while IFS= read -r -d $'\0' file; do
+        # Skip priority and optional files
+        local basename=$(basename "$file")
+        if [[ " ${LOAD_ORDER[PRIORITY][@]} ${LOAD_ORDER[OPTIONAL][@]} " =~ " $basename " ]]; then
+            continue
+        fi
+        
+        load_function_file "$file" || continue
+        
+    done < <(find_function_files "$base_dir")
+}
diff --git a/bash/modules/genome_indexer.sh b/bash/modules/genome_indexer.sh
new file mode 100644
index 0000000..11e7835
--- /dev/null
+++ b/bash/modules/genome_indexer.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+
+function find_reference_genomes() {
+    local base_dir="${GENOME_CONFIG[BASE_DIR]}"
+    local pattern="${GENOME_CONFIG[GENOME_PATTERN]}"
+    
+    log_info "Searching for reference genomes in: $base_dir"
+    
+    if [ ! -d "$base_dir" ]; then
+        log_error "Reference genome directory not found: $base_dir"
+        return 1
+    fi
+    
+    find "$base_dir" -type f -name "$pattern"
+}
+
+function build_genome_index() {
+    local genome_path="$1"
+    local index_suffix="${GENOME_CONFIG[INDEX_SUFFIX]}"
+    
+    if [ ! -f "$genome_path" ]; then
+        log_error "Genome file not found: $genome_path"
+        return 1
+    fi
+    
+    local index_base="${genome_path%_refgenome.fna}${index_suffix}"
+    
+    log_info "Building index for: $genome_path"
+    log_info "Output base: $index_base"
+    
+    if ! bowtie2-build "$genome_path" "$index_base"; then
+        log_error "Index building failed for: $genome_path"
+        return 1
+    fi
+    
+    log_info "Successfully built index for: $genome_path"
+    return 0
+}
diff --git a/bash/modules/genome_organizer.sh b/bash/modules/genome_organizer.sh
new file mode 100644
index 0000000..ddc0aa2
--- /dev/null
+++ b/bash/modules/genome_organizer.sh
@@ -0,0 +1,74 @@
+#!/bin/bash
+
+source "../config/genome_config.sh"
+
+function extract_organism_name() {
+    local report_file="$1"
+    
+    if [ ! -f "$report_file" ]; then
+        log_error "Assembly report not found: $report_file"
+        return 1
+    fi
+    
+    log_info "Extracting organism name from: $report_file"
+    
+    local organism_name=$(grep -m 1 -o '"organismName":"[^"]*' "$report_file" | 
+                         cut -d '"' -f4 | 
+                         sed 's/ //g')
+    
+    if [ -z "$organism_name" ]; then
+        log_error "Failed to extract organism name"
+        return 1
+    fi
+    
+    echo "$organism_name"
+}
+
+function reorganize_genome_files() {
+    local source_dir="$1"
+    local organism_name="$2"
+    
+    log_info "Reorganizing files for: $organism_name"
+    
+    # Move all files to root directory
+    find "$source_dir/" -type f -exec mv -v {} "$source_dir/" \; || {
+        log_error "Failed to move files in: $source_dir"
+        return 1
+    }
+    
+    # Rename CDS file
+    local cds_source="${source_dir}/${GENOME_NAMING[ORIGINAL_CDS]}"
+    local cds_target="${source_dir}/${GENOME_NAMING[CDS_FILE]}"
+    if [ -f "$cds_source" ]; then
+        mv -v "$cds_source" "$cds_target" || log_warning "Failed to rename CDS file"
+    fi
+    
+    # Rename genomic file
+    find "$source_dir" -name "${GENOME_NAMING[GENOMIC_PATTERN]}" \
+        -exec mv -v {} "$source_dir/${organism_name}${GENOME_NAMING[REFGENOME_SUFFIX]}" \;
+    
+    # Cleanup
+    rm -rf "$source_dir/${GENOME_PATHS[NCBI_DATA]}"
+    
+    return 0
+}
+
+function standardize_chromosome_names() {
+    local genome_file="$1"
+    local output_file="$2"
+    
+    log_info "Standardizing chromosome names in: $genome_file"
+    
+    awk -v old="${CHROMOSOME_NAMING[PATTERN_OLD]}" \
+        -v new="${CHROMOSOME_NAMING[PATTERN_NEW]}" \
+        -v sep="${CHROMOSOME_NAMING[SEPARATOR]}" \
+        '/^>/ {
+            gsub(old, new, $6)
+            printf(">%s%s\n", $6, $7)
+            next
+        }
+        !/^>/ {
+            print $0
+        }' "$genome_file" | 
+    sed "s/${CHROMOSOME_NAMING[SEPARATOR]}.*//" > "$output_file"
+}
diff --git a/bash/modules/git_handler.sh b/bash/modules/git_handler.sh
new file mode 100644
index 0000000..7a30165
--- /dev/null
+++ b/bash/modules/git_handler.sh
@@ -0,0 +1,48 @@
+#!/bin/bash
+
+source "../config/data_sources_config.sh"
+
+# Add to existing git handler or create new
+function validate_git() {
+    if ! command -v git &>/dev/null; then
+        log_error "Git is not installed"
+        return 1
+    fi
+    return 0
+}
+
+function clone_repository() {
+    local repo_url="$1"
+    local target_dir="$2"
+    local depth="${3:-1}"
+    
+    if [ -d "$target_dir" ]; then
+        log_warning "Directory already exists: $target_dir"
+        return 1
+    fi
+    
+    log_info "Cloning repository: $repo_url"
+    if ! git clone --depth="$depth" "$repo_url" "$target_dir"; then
+        log_error "Failed to clone repository: $repo_url"
+        return 1
+    fi
+    
+    log_info "Successfully cloned to: $target_dir"
+    return 0
+}
+
+function verify_repository() {
+    local repo_dir="$1"
+    
+    if [ ! -d "$repo_dir/.git" ]; then
+        log_error "Not a git repository: $repo_dir"
+        return 1
+    fi
+    
+    if ! git -C "$repo_dir" rev-parse HEAD &>/dev/null; then
+        log_error "Invalid git repository: $repo_dir"
+        return 1
+    fi
+    
+    return 0
+}
diff --git a/bash/modules/ncbi_handler.sh b/bash/modules/ncbi_handler.sh
new file mode 100644
index 0000000..4bc30f9
--- /dev/null
+++ b/bash/modules/ncbi_handler.sh
@@ -0,0 +1,60 @@
+#!/bin/bash
+
+source "../config/genome_config.sh"
+
+function validate_ncbi_tools() {
+    if ! command -v datasets &>/dev/null; then
+        log_error "NCBI datasets tool not found"
+        return 1
+    fi
+    return 0
+}
+
+function download_genome() {
+    local accession="$1"
+    local base_dir="${GENOME_CONFIG[BASE_DIR]}"
+    local timestamp=$(date +"%Y%m%d_%H%M%S")
+    local target_dir="${base_dir}/${accession}_${timestamp}"
+    
+    log_info "Downloading genome: $accession"
+    
+    mkdir -p "$target_dir" || {
+        log_error "Failed to create directory: $target_dir"
+        return 1
+    }
+    
+    if ! datasets download genome accession "$accession" \
+        --include "${NCBI_CONFIG[DOWNLOAD_INCLUDES]}" \
+        --filename "${target_dir}/${accession}.zip"; then
+        log_error "Download failed for accession: $accession"
+        return 1
+    fi
+    
+    log_info "Extracting files for: $accession"
+    if ! unzip -q "${target_dir}/${accession}.zip" -d "$target_dir"; then
+        log_error "Extraction failed for: $accession"
+        return 1
+    fi
+    
+    rm "${target_dir}/${accession}.zip"
+    log_info "Successfully processed: $accession"
+    return 0
+}
+
+function process_genome_batch() {
+    local -a accessions=("$@")
+    local success_count=0
+    local fail_count=0
+    
+    for accession in "${accessions[@]}"; do
+        if download_genome "$accession"; then
+            ((success_count++))
+        else
+            ((fail_count++))
+            log_warning "Failed to process: $accession"
+        fi
+    done
+    
+    log_info "Processing complete. Success: $success_count, Failed: $fail_count"
+    return $((fail_count > 0))
+}
diff --git a/bash/modules/ngs_file_manager.sh b/bash/modules/ngs_file_manager.sh
new file mode 100644
index 0000000..506741b
--- /dev/null
+++ b/bash/modules/ngs_file_manager.sh
@@ -0,0 +1,120 @@
+#!/bin/bash
+
+source "../config/file_management_config.sh"
+
+function build_file_patterns() {
+    local patterns=()
+    for type in "${!FILE_TYPES[@]}"; do
+        for ext in ${FILE_TYPES[$type]}; do
+            patterns+=("-o" "-name" "*.$ext")
+        done
+    done
+    echo "${patterns[@]:1}" # Remove first -o
+}
+
+function check_disk_space() {
+    local target_dir="$1"
+    local min_space="${2:-${DEFAULTS[MIN_FREE_SPACE_GB]}}"
+    
+    local available_space=$(df -BG "$target_dir" | awk 'NR==2 {print $4}' | sed 's/G//')
+    if (( available_space < min_space )); then
+        log_error "Insufficient disk space. Available: ${available_space}GB, Required: ${min_space}GB"
+        return 1
+    if
+    return 0
+}
+
+function validate_directories() {
+    local target_dir="$1"
+    local search_dir="$2"
+    
+    if [[ ! -d "$search_dir" ]]; then
+        log_error "Search directory does not exist: $search_dir"
+        return 1
+    fi
+    
+    if [[ ! -d "$target_dir" ]]; then
+        log_info "Creating target directory: $target_dir"
+        mkdir -p "$target_dir"
+    fi
+    
+    if [[ ! -w "$target_dir" ]]; then
+        log_error "Target directory not writable: $target_dir"
+        return 1
+    fi
+}
+
+function move_ngs_files() {
+    local target_dir="$1"
+    local search_dir="${2:-.}"
+    local max_depth="${3:-${DEFAULTS[MAX_DEPTH]}}"
+    local batch_size="${4:-${DEFAULTS[BATCH_SIZE]}}"
+    
+    log_info "Starting NGS file movement operation"
+    
+    validate_directories "$target_dir" "$search_dir" || return 1
+    check_disk_space "$target_dir" || return 1
+    
+    local file_patterns=($(build_file_patterns))
+    
+    find "$search_dir" \
+        -maxdepth "$max_depth" \
+        -type f \
+        \( "${file_patterns[@]}" \) \
+        \( -path "*/code/*" -o -path "*/script*/*" \) \
+        -print0 | 
+    while IFS= read -r -d '' file; do
+        local file_type=$(determine_file_type "$file")
+        local target_subdir="$target_dir/$file_type"
+        
+        mkdir -p "$target_subdir"
+        log_info "Moving $file to $target_subdir"
+        mv "$file" "$target_subdir/" || log_error "Failed to move: $file"
+    done
+}
+
+function determine_file_type() {
+    local file="$1"
+    local extension="${file##*.}"
+    
+    for type in "${!FILE_TYPES[@]}"; do
+        if [[ " ${FILE_TYPES[$type]} " =~ " $extension " ]]; then
+            echo "$type"
+            return
+        }
+    done
+    echo "OTHER"
+}
+
+function analyze_file_distribution() {
+    local search_dir="${1:-.}"
+    
+    log_info "Analyzing file distribution in $search_dir"
+    
+    find "$search_dir" \
+        -type d \( -path '*/lib/*' -o -path '*/renv/*' -o -path '*/git/*' \) -prune \
+        -o -type f -exec du -a {} + | 
+        sort -nr | 
+        head -n 1000 | 
+        awk -F/ '{print $NF}' | 
+        rev | cut -d. -f1 | rev | 
+        sort | uniq -c | 
+        sort -nr
+}
+
+# Add to existing file management functions
+function find_fastq_files() {
+    local base_dir="$1"
+    local exclude_pattern="${QC_CONFIG[EXCLUDE_PATTERNS]}"
+    
+    log_info "Finding FASTQ files in: $base_dir"
+    find "$base_dir" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \)
+}
+
+function find_processed_fastq() {
+    local base_dir="$1"
+    local pattern="${QC_CONFIG[PROCESSED_PATTERN]}"
+    
+    log_info "Finding processed FASTQ files in: $base_dir"
+    find "$base_dir" -type f -name "$pattern"
+}
diff --git a/bash/modules/quality_control.sh b/bash/modules/quality_control.sh
new file mode 100644
index 0000000..5091a92
--- /dev/null
+++ b/bash/modules/quality_control.sh
@@ -0,0 +1,35 @@
+#!/bin/bash
+
+source "../functions/ngs_file_manager.sh"
+
+function setup_qc_directory() {
+    local base_dir="$1"
+    local qc_dir="${base_dir}/${QC_CONFIG[OUTPUT_DIR]}"
+    
+    log_info "Setting up QC directory: $qc_dir"
+    mkdir -p "$qc_dir" || {
+        log_error "Failed to create QC directory: $qc_dir"
+        return 1
+    }
+    
+    echo "$qc_dir"
+}
+
+function run_fastqc() {
+    local file_path="$1"
+    local output_dir="$2"
+    
+    if [ ! -f "$file_path" ]; then
+        log_error "File not found: $file_path"
+        return 1
+    fi
+    
+    log_info "Running FASTQC on: $file_path"
+    if ! fastqc --outdir="$output_dir" "$file_path"; then
+        log_error "FASTQC failed for: $file_path"
+        return 1
+    fi
+    
+    log_info "FASTQC completed for: $file_path"
+    return 0
+}
diff --git a/bash/modules/r_integration.sh b/bash/modules/r_integration.sh
new file mode 100644
index 0000000..a9e6d38
--- /dev/null
+++ b/bash/modules/r_integration.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+
+source "../config/bam_comparison_config.sh"
+
+function get_bam_pairs() {
+    local base_dir="$1"
+    local task_id="$2"
+    local r_script="${R_CONFIG[SCRIPT_PATH]}"
+    
+    log_info "Getting BAM pairs from R script"
+    
+    if [ ! -f "$r_script" ]; then
+        log_error "R script not found: $r_script"
+        return 1
+    fi
+    
+    local pairs=$(Rscript "$r_script" "$base_dir" "$task_id" | grep -v '^NULL$')
+    
+    if [ -z "$pairs" ]; then
+        log_error "No output from R script"
+        return 1
+    fi
+    
+    echo "$pairs"
+}
+
+function validate_bam_pairs() {
+    local -a pairs=("$@")
+    
+    log_info "Validating BAM pairs"
+    
+    if [ ${#pairs[@]} -ne 2 ]; then
+        log_error "Expected 2 BAM files, got ${#pairs[@]}"
+        return 1
+    fi
+    
+    for bam in "${pairs[@]}"; do
+        if [ ! -f "$bam" ]; then
+            log_error "BAM file not found: $bam"
+            return 1
+        fi
+    done
+    
+    return 0
+}
diff --git a/bash/modules/slurm_file_operations.sh b/bash/modules/slurm_file_operations.sh
new file mode 100644
index 0000000..0b37db8
--- /dev/null
+++ b/bash/modules/slurm_file_operations.sh
@@ -0,0 +1,76 @@
+#!/bin/bash
+
+source "../config/slurm_config.sh"
+# Assuming logging utilities are already sourced
+
+function find_slurm_files() {
+    local search_dir="${1:-.}"
+    local pattern="${2:-$SLURM_OUTPUT_PATTERN}"
+    local max_depth="${3:-$MAX_DEPTH_SEARCH}"
+
+    log_info "Searching for SLURM files in: $search_dir"
+    
+    if [[ ! -d "$search_dir" ]]; then
+        log_error "Directory does not exist: $search_dir"
+        return 1
+    fi
+
+    find "$search_dir" \
+        -maxdepth "$max_depth" \
+        -type f \
+        -name "$pattern" \
+        -printf "%T@ %p\n" | \
+        sort -nr | \
+        cut -d' ' -f2-
+}
+
+function organize_slurm_files() {
+    local source_dir="${1:-.}"
+    local target_dir="${2:-$DEFAULT_SLURM_LOG_DIR}"
+    
+    log_info "Organizing SLURM files from $source_dir to $target_dir"
+    
+    # Create target directory if it doesn't exist
+    mkdir -p "$target_dir"
+    
+    # Move files to organized structure
+    while IFS= read -r file; do
+        local job_id=$(basename "$file" | sed 's/slurm-\([0-9]*\).out/\1/')
+        local date_str=$(stat -c %y "$file" | cut -d' ' -f1)
+        local year_month=$(date -d "$date_str" +%Y/%m)
+        local target_subdir="$target_dir/$year_month"
+        
+        mkdir -p "$target_subdir"
+        mv "$file" "$target_subdir/"
+        log_info "Moved $file to $target_subdir/"
+    done < <(find_slurm_files "$source_dir")
+}
+
+function cleanup_old_logs() {
+    local log_dir="${1:-$DEFAULT_SLURM_LOG_DIR}"
+    local max_age="${2:-$MAX_LOG_AGE_DAYS}"
+    
+    log_info "Cleaning up SLURM logs older than $max_age days"
+    
+    find "$log_dir" \
+        -type f \
+        -name "$SLURM_OUTPUT_PATTERN" \
+        -mtime +"$max_age" \
+        -exec rm {} \;
+}
+
+function check_large_files() {
+    local search_dir="${1:-.}"
+    local max_size="${2:-$MAX_FILE_SIZE_MB}"
+    
+    log_info "Checking for large SLURM output files"
+    
+    find "$search_dir" \
+        -type f \
+        -name "$SLURM_OUTPUT_PATTERN" \
+        -size +"${max_size}M" \
+        -exec ls -lh {} \; | \
+    while read -r line; do
+        log_warning "Large SLURM output file: $line"
+    done
+}
diff --git a/bash/modules/slurm_handler.sh b/bash/modules/slurm_handler.sh
new file mode 100644
index 0000000..2793633
--- /dev/null
+++ b/bash/modules/slurm_handler.sh
@@ -0,0 +1,46 @@
+#!/bin/bash
+
+source "../config/genome_index_config.sh"
+
+function setup_logging() {
+    local log_dir="${GENOME_CONFIG[LOG_DIR]}"
+    local timestamp=$(date "+%Y-%m-%d-%H-%M-%S")
+    local job_id="${SLURM_ARRAY_JOB_ID:-standalone}"
+    
+    mkdir -p "$log_dir" || {
+        echo "Failed to create log directory: $log_dir"
+        return 1
+    }
+    
+    echo "${log_dir}/indexing_${job_id}_${timestamp}"
+}
+
+function validate_slurm_env() {
+    if [ -z "${SLURM_ARRAY_TASK_ID:-}" ]; then
+        log_error "This script must be run as a SLURM array job"
+        return 1
+    fi
+    
+    if [ -z "${SLURM_ARRAY_JOB_ID:-}" ]; then
+        log_error "SLURM_ARRAY_JOB_ID not set"
+        return 1
+    fi
+    
+    return 0
+}
+
+function load_required_modules() {
+    log_info "Loading required modules"
+    
+    module purge || log_warning "Failed to purge modules"
+    
+    for module in "${MODULES[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            return 1
+        fi
+        log_info "Loaded module: $module"
+    done
+    
+    return 0
+}
diff --git a/bash/modules/slurm_job_handler.sh b/bash/modules/slurm_job_handler.sh
new file mode 100644
index 0000000..989fec6
--- /dev/null
+++ b/bash/modules/slurm_job_handler.sh
@@ -0,0 +1,60 @@
+#!/bin/bash
+
+source "../config/slurm_config.sh"
+
+function validate_array_range() {
+    local range="$1"
+    
+    log_info "Validating array range: $range"
+    
+    if [[ ! "$range" =~ ^[0-9]+(|-[0-9]+(%[0-9]+)?|,[0-9]+)*$ ]]; then
+        log_error "Invalid array range format: $range"
+        return 1
+    fi
+    
+    if [[ "$range" =~ %([0-9]+) ]]; then
+        local concurrent="${BASH_REMATCH[1]}"
+        if ((concurrent > ${SLURM_WRAPPER[MAX_ARRAY_SIZE]})); then
+            log_warning "Concurrent jobs ($concurrent) exceeds recommended maximum (${SLURM_WRAPPER[MAX_ARRAY_SIZE]})"
+        }
+        fi
+    
+    return 0
+}
+
+function find_script() {
+    local script_name="$1"
+    local base_dir="${SLURM_WRAPPER[SCRIPT_BASE_DIR]}"
+    
+    log_info "Searching for script: $script_name"
+    
+    local script_path=$(find "$base_dir" -type f -name "$script_name")
+    
+    if [ -z "$script_path" ]; then
+        log_error "Script not found: $script_name"
+        return 1
+    fi
+    
+    if [ ! -x "$script_path" ]; then
+        log_error "Script not executable: $script_path"
+        return 1
+    fi
+    
+    echo "$script_path"
+}
+
+function validate_experiment_dir() {
+    local dir_name="$1"
+    local base_dir="${SLURM_WRAPPER[DATA_BASE_DIR]}"
+    
+    log_info "Validating experiment directory: $dir_name"
+    
+    local full_path=$(find -H "$base_dir" -maxdepth 1 -type d -name "$dir_name")
+    
+    if [ -z "$full_path" ]; then
+        log_error "Directory not found: $dir_name"
+        return 1
+    fi
+    
+    echo "$full_path"
+}
diff --git a/bash/modules/slurm_validator.sh b/bash/modules/slurm_validator.sh
new file mode 100644
index 0000000..787187b
--- /dev/null
+++ b/bash/modules/slurm_validator.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+# bash/functions/slurm_validator.sh
+
+source "$HOME/lab_utils/bash/config/slurm_config.sh"
+source "$HOME/lab_utils/bash/functions/logging_utils.sh"
+
+#' Validate SLURM Environment
+#' @param job_type Character Type of job (ALIGN, QC, BW)
+#' @param log_file Character Log file path
+#' @return Integer 0 if valid
+validate_slurm_environment() {
+    local job_type="$1"
+    local log_file="$2"
+    
+    # Validate resource requirements
+    if [[ ! " ALIGN QC BW " =~ " $job_type " ]]; then
+        log_error "Invalid job type: $job_type" "$log_file"
+        return 1
+    fi
+    
+    # Check SLURM environment
+    if [[ -z "${SLURM_JOB_ID:-}" ]]; then
+        log_error "Not running in SLURM environment" "$log_file"
+        return 1
+    fi
+    
+    # Verify reference directory
+    if [[ ! -d "${SLURM_GENOMES[BASE_DIR]}" ]]; then
+        log_error "Reference genome directory not found: ${SLURM_GENOMES[BASE_DIR]}" "$log_file"
+        return 1
+    fi
+    
+    return 0
+}
+
+#' Validate Module Availability
+#' @param modules Array Required module names
+#' @param log_file Character Log file path
+#' @return Integer 0 if all modules available
+validate_modules() {
+    local -a modules=("$@")
+    local log_file="${modules[-1]}" # Last argument is log file
+    unset 'modules[-1]'            # Remove log file from array
+    
+    for module in "${modules[@]}"; do
+        if ! module avail "$module" 2>/dev/null; then
+            log_error "Required module not available: $module" "$log_file"
+            return 1
+        fi
+    done
+    
+    return 0
+}
+
+#' Format Array Range for MIT SLURM
+#' @param range_input String Input range (e.g., "1-10", "1,2,3", "5")
+#' @return String Formatted range with %16
+format_array_range() {
+    local range_input="$1"
+    
+    # Single number
+    if [[ "$range_input" =~ ^[0-9]+$ ]]; then
+        echo "$range_input"
+        return 0
+    fi
+    
+    # Comma-separated list
+    if [[ "$range_input" =~ ^[0-9]+(,[0-9]+)*$ ]]; then
+        echo "$range_input"
+        return 0
+    fi
+    
+    # Range (add %16 if not present)
+    if [[ "$range_input" =~ ^[0-9]+-[0-9]+$ ]]; then
+        echo "${range_input}%16"
+        return 0
+    fi
+    
+    # Invalid format
+    return 1
+}
+#' Validate SLURM Array Range
+#' @param range_input String Input range
+#' @param log_file Character Log file path
+#' @return Integer 0 if valid
+validate_array_range() {
+    local range_input="$1"
+    local log_file="$2"
+    
+    # Validate format
+    if ! [[ "$range_input" =~ ^([0-9]+|[0-9]+-[0-9]+|[0-9]+(,[0-9]+)*)$ ]]; then
+        log_error "Invalid array range format: $range_input" "$log_file"
+        return 1
+    fi
+    
+    # Format range
+    local formatted_range
+    if ! formatted_range=$(format_array_range "$range_input"); then
+        log_error "Failed to format array range" "$log_file"
+        return 1
+    fi
+    
+    echo "$formatted_range"
+    return 0
+}
+
+
diff --git a/bash/modules/slurm_wrapper.sh b/bash/modules/slurm_wrapper.sh
new file mode 100644
index 0000000..d203112
--- /dev/null
+++ b/bash/modules/slurm_wrapper.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+# bash/functions/slurm_wrapper.sh
+
+source "$HOME/lab_utils/bash/config/slurm_config.sh"
+source "$HOME/lab_utils/bash/functions/logging_utils.sh"
+source "$HOME/lab_utils/bash/functions/slurm_validator.sh"
+
+#' Submit SLURM Array Job
+#' @param array_range Character SLURM array specification
+#' @param script_name Character Script name
+#' @param dir_name Character Directory name
+#' @param job_type Character Job type
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful
+submit_slurm_array_job() {
+    local array_range="$1"
+    local script_name="$2"
+    local dir_name="$3"
+    local job_type="$4"
+    local timestamp_for_data="$5"
+    local log_file="$6"
+    
+    local slurm_opts
+    slurm_opts=$(get_slurm_options "$job_type")
+    
+    local job_id
+    job_id=$(sbatch --parsable \
+                    --array="$array_range" \
+                    $slurm_opts \
+                    "$script_name" \
+                    "$dir_name"
+                    "$timestamp")
+    
+    if [[ -z "$job_id" ]]; then
+        log_error "Job submission failed" "$log_file"
+        return 1
+    fi
+    
+    echo "$job_id"
+    return 0
+}
+
+
+#' Print Job Information
+#' @param job_id Character SLURM job ID
+#' @param log_file Character Log file path
+print_job_info() {
+    local job_id="$1"
+    local log_file="$2"
+    
+    cat << EOF
+
+Job Information:
+  Job ID: $job_id
+  Log File: $log_file
+
+Monitor Commands:
+  squeue -j $job_id
+  sacct -j $job_id
+  tail -f $log_file
+  
+Status Check:
+  squeue -u $USER
+  scontrol show job $job_id
+EOF
+}
+
+#' Find Script in Project
+#' @param script_name Character Script name
+#' @param log_file Character Log file path
+#' @return String Script path
+find_slurm_script() {
+    local script_name="$1"
+    local log_file="$2"
+    
+    local script_path="$HOME/lab_utils/bash/scripts/$script_name"
+    
+    if [[ ! -f "$script_path" ]]; then
+        log_error "Script not found: $script_path" "$log_file"
+        return 1
+    fi
+    
+    if [[ ! -x "$script_path" ]]; then
+        log_error "Script not executable: $script_path" "$log_file"
+        return 1
+    fi
+    
+    echo "$script_path"
+}
+
+#' Validate Experiment Directory
+#' @param dir_name Character Directory name
+#' @param log_file Character Log file path
+#' @return String Full directory path
+validate_experiment_dir() {
+    local dir_name="$1"
+    local log_file="$2"
+    
+    local exp_dir="$HOME/data/$dir_name"
+    
+    if [[ ! -d "$exp_dir" ]]; then
+        log_error "Experiment directory not found: $exp_dir" "$log_file"
+        return 1
+    fi
+    
+    echo "$exp_dir"
+}
diff --git a/bash/modules/sra_downloader.sh b/bash/modules/sra_downloader.sh
new file mode 100644
index 0000000..0ea74de
--- /dev/null
+++ b/bash/modules/sra_downloader.sh
@@ -0,0 +1,97 @@
+#!/bin/bash
+
+source "../config/sra_config.sh"
+
+function validate_input() {
+    local download_dir="$1"
+    
+    log_info "Validating input parameters"
+    
+    if [ -z "$download_dir" ]; then
+        log_error "Download directory not specified"
+        return 1
+    fi
+    
+    local full_path="${SRA_CONFIG[DATA_DIR]}/$download_dir"
+    
+    if [ ! -d "$full_path" ]; then
+        log_info "Creating directory: $full_path"
+        mkdir -p "$full_path" || {
+            log_error "Failed to create directory: $full_path"
+            return 1
+        }
+    fi
+    
+    echo "$full_path"
+}
+
+function construct_download_url() {
+    local accession="$1"
+    local base_url="${SRA_CONFIG[BASE_URL]}"
+    
+    echo "${base_url}${accession:0:6}/${accession}/${accession}.fastq.gz"
+}
+
+function verify_url() {
+    local url="$1"
+    
+    log_info "Verifying URL: $url"
+    
+    if ! curl --head --silent --fail "$url" >/dev/null; then
+        log_error "URL not accessible: $url"
+        return 1
+    fi
+    
+    return 0
+}
+
+function download_file() {
+    local url="$1"
+    local output_file="$2"
+    
+    log_info "Downloading: $url -> $output_file"
+    
+    if ! wget --quiet --show-progress --output-document="$output_file" "$url"; then
+        log_error "Download failed: $url"
+        return 1
+    fi
+    
+    log_info "Download complete: $output_file"
+    return 0
+}
+
+function concatenate_files() {
+    local output_dir="$1"
+    local output_file="$2"
+    local files=("${@:3}")
+    
+    log_info "Concatenating files to: $output_file"
+    
+    for file in "${files[@]}"; do
+        if [ ! -f "$output_dir/$file" ]; then
+            log_error "File not found: $file"
+            return 1
+        fi
+        cat "$output_dir/$file" >> "$output_file" || {
+            log_error "Failed to concatenate: $file"
+            return 1
+        }
+    done
+    
+    log_info "Concatenation complete"
+    return 0
+}
+
+function decompress_file() {
+    local file="$1"
+    
+    log_info "Decompressing: $file"
+    
+    if ! gunzip "$file"; then
+        log_error "Decompression failed: $file"
+        return 1
+    fi
+    
+    log_info "Decompression complete"
+    return 0
+}
diff --git a/bash/scripts/000_install_R_4.2.0.sh b/bash/scripts/000_install_R_4.2.0.sh
new file mode 100644
index 0000000..befe338
--- /dev/null
+++ b/bash/scripts/000_install_R_4.2.0.sh
@@ -0,0 +1,45 @@
+# Update and install necessary dependencies
+sudo apt-get update
+sudo apt-get install -y build-essential libcurl4-gnutls-dev libxml2-dev libssl-dev gfortran
+sudo apt-get install -y libx11-dev libxt-dev libpng-dev libjpeg-dev libcairo2-dev libxext-dev libxrender-dev libxmu-dev libxmuu-dev x11-apps xauth libreadline-dev libbz2-dev liblzma-dev
+sudo apt-get install -y default-jdk
+# Install related libraries for R packages.
+sudo apt-get install libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libfontconfig1-dev
+sudo apt-get install libmagick++-dev
+
+# Install LaTeX and related packages
+sudo apt-get install -y texlive texlive-fonts-extra texlive-latex-extra texinfo
+sudo apt-get install -y texlive-science texlive-extra-utils texlive-bibtex-extra
+sudo apt-get install -y texlive-fonts-recommended texlive-plain-generic
+sudo apt-get install -y texlive-fonts-extra
+
+# Set up environment variables
+export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
+export PATH=$JAVA_HOME/bin:$PATH
+
+# Download and extract R source
+R_VERSION="R-4.2.0"
+DIR_TO_INSTALL=$HOME
+curl --output "$HOME/${R_VERSION}.tar.gz" "https://cran.r-project.org/src/base/$(echo ${R_VERSION} | cut  -d. -f1)/${R_VERSION}.tar.gz"
+tar -xzvf ${R_VERSION}.tar.gz
+cd ${R_VERSION}
+
+# Configure and compile R with X11 and Cairo support
+./configure --enable-R-shlib --with-blas --with-lapack --with-cairo --with-x
+make
+sudo make install
+
+# Set up environment
+export PATH=/usr/local/bin:$PATH
+mkdir -p $HOME/R/library
+export R_LIBS_USER=$HOME/R/library
+alias R='R --no-save'
+
+# Start R and install packages
+R --vanilla << EOF
+dir.create(Sys.getenv("R_LIBS_USER"), recursive = TRUE)
+.libPaths(Sys.getenv("R_LIBS_USER"))
+options(repos = c(CRAN = "https://cloud.r-project.org"))
+install.packages(c("renv", "xml2", "lintr", "roxygen2", "languageserver"), dependencies = TRUE, INSTALL_opts = '--no-lock')
+q()
+EOF
diff --git a/bash/scripts/build_genome_indices.sh b/bash/scripts/build_genome_indices.sh
new file mode 100644
index 0000000..0a60304
--- /dev/null
+++ b/bash/scripts/build_genome_indices.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+
+#SBATCH -N ${SLURM_CONFIG[NODES]}
+#SBATCH -n ${SLURM_CONFIG[TASKS]}
+#SBATCH --mem-per-cpu=${SLURM_CONFIG[MEM_PER_CPU]}
+#SBATCH --exclude=${SLURM_CONFIG[EXCLUDE_NODES]}
+#SBATCH --mail-type=${SLURM_CONFIG[MAIL_TYPE]}
+#SBATCH --mail-user=${SLURM_CONFIG[MAIL_USER]}
+# functions moved
+
+set -euo pipefail
+
+source "../functions/slurm_handler.sh"
+source "../functions/genome_indexer.sh"
+
+function main() {
+    validate_slurm_env || exit 1
+    
+    local log_base=$(setup_logging) || exit 1
+    exec 1>"${log_base}.out" 2>"${log_base}.err"
+    
+    log_info "Starting genome indexing job"
+    log_info "Job ID: ${SLURM_JOB_ID}, Array ID: ${SLURM_ARRAY_JOB_ID}, Task ID: ${SLURM_ARRAY_TASK_ID}"
+    
+    load_required_modules || exit 1
+    
+    mapfile -t genome_paths < <(find_reference_genomes) || exit 1
+    
+    if [ ${#genome_paths[@]} -eq 0 ]; then
+        log_error "No reference genomes found"
+        exit 1
+    fi
+    
+    local task_index=$((SLURM_ARRAY_TASK_ID - 1))
+    if [ $task_index -ge ${#genome_paths[@]} ]; then
+        log_error "Task ID exceeds number of genomes"
+        exit 1
+    fi
+    
+    build_genome_index "${genome_paths[$task_index]}" || exit 1
+    
+    log_info "Job completed successfully"
+}
+
+main
diff --git a/bash/scripts/consolidate_fastq_files.sh b/bash/scripts/consolidate_fastq_files.sh
new file mode 100755
index 0000000..4f725b1
--- /dev/null
+++ b/bash/scripts/consolidate_fastq_files.sh
@@ -0,0 +1,58 @@
+#!/bin/bash
+# bash/scripts/consolidate_fastq_files.sh
+
+source "$HOME/lab_utils/bash/config/project_config.sh"
+source "$HOME/lab_utils/bash/functions/fastq_consolidator_processor.sh"
+
+#' Consolidate FASTQ Files Main Function
+#' @param experiment_id Character Experiment identifier
+#' @return Integer 0 if successful
+consolidate_fastq_files_main() {
+    local experiment_id="$1"
+    if [[ $# -ne 1 ]]; then
+        show_usage
+        return 1
+    fi
+    
+    # Initialize logging
+    local log_file
+    log_file=$(initialize_logging "consolidate_fastq")
+    
+    local experiment_dir
+    
+    # Validate input
+    if ! experiment_dir=$(validate_fastq_input "$experiment_id" "$log_file"); then
+        return 1
+    fi
+    
+    # Setup directories
+    local output_dir
+    if ! output_dir=$(setup_fastq_directories "$experiment_dir" "$log_file"); then
+        return 1
+    fi
+    
+    # Process files
+    if ! consolidate_fastq_files_by_id "$experiment_dir" "$output_dir" "$log_file"; then
+        return 1
+    fi
+    
+    log_info "FASTQ consolidation completed successfully" "$log_file"
+    return 0
+}
+
+show_usage() {
+    cat << EOF
+Usage: $(basename "$0") <experiment_id>
+
+Arguments:
+    experiment_id    Experiment identifier (format: YYMMDD'Bel')
+
+Example:
+    $(basename "$0") 241028Bel
+EOF
+}
+
+# Execute if run as script
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    consolidate_fastq_files_main "$@"
+fi
diff --git a/bash/scripts/download_bmc_fastq_to_user_bel_directory.sh b/bash/scripts/download_bmc_fastq_to_user_bel_directory.sh
new file mode 100755
index 0000000..7e63b7c
--- /dev/null
+++ b/bash/scripts/download_bmc_fastq_to_user_bel_directory.sh
@@ -0,0 +1,105 @@
+#!/bin/bash
+# bash/scripts/fastq/download_bmc_data.sh
+
+#' Download BMC Data Script
+#' @description Download and process BMC FASTQ data
+
+echo "Initializing BMC data download"
+echo "Script location: ${BASH_SOURCE[0]}"
+echo "Working directory: $(pwd)"
+
+# Find repository root using git
+repo_root=$(git rev-parse --show-toplevel 2>/dev/null) || {
+    echo "? Not in a git repository"
+    exit 1
+}
+
+# Initialize environment
+source "$repo_root/bash/core/initialize_lab_environment.sh" || {
+    echo "? Failed to initialize environment"
+    exit 1
+}
+
+
+# Load required modules
+echo "Loading required modules"
+for module in "fastq/bmc_handler" "fastq/fastq_processor"; do
+    echo "Loading: $module"
+    if ! load_lab_module "$module"; then
+        log_error "Failed to load module: $module"
+        exit 1
+    fi
+    echo "Loaded successfully"
+done
+#' Download BMC Data Main Function
+#' @param experiment_id Character Experiment identifier
+#' @return Integer 0 if successful
+download_bmc_data_main() {
+    local experiment_id="$1"
+    local log_file
+    
+    # Initialize logging
+    log_file=$(initialize_logging "download_bmc_data") || {
+        echo "? Failed to initialize logging"
+        return 1
+    }
+    
+    log_info "Starting BMC data download" "$log_file"
+    log_debug "Environment verification" "$log_file"
+    log_debug "LAB_UTILS_ROOT: $LAB_UTILS_ROOT" "$log_file"
+    log_debug "Experiment ID: $experiment_id" "$log_file"
+    log_debug "Log file: $log_file" "$log_file"
+    
+    # Verify host
+    if ! verify_host; then
+        log_error "Host verification failed" "$log_file"
+        return 1
+    fi
+    
+    # Validate paths
+    local paths
+    log_debug "Validating BMC paths" "$log_file"
+    if ! paths=$(validate_bmc_paths "$experiment_id" "$log_file"); then
+        return 1
+    fi
+    
+    local bmc_path=${paths%:*}
+    local local_path=${paths#*:}
+
+    log_debug " BMC path: $bmc_path" "$log_file"
+    log_debug " Local path: $local_path" "$log_file"
+    
+    # Download data
+    if ! download_from_bmc "$paths" "$log_file"; then
+        return 1
+    fi
+
+    # Organize files
+    if ! move_fastq_files_to_current_directory "${local_path}" "$log_file"; then
+        return 1
+    fi
+
+    # Cleanup
+    if ! clean_experiment_directory "${local_path}" "$log_file"; then
+        return 1
+    fi
+
+    log_info "Download process completed successfully" "$log_file"
+    return 0
+}
+
+# Show usage information
+show_usage() {
+    cat << EOF
+Usage: $(basename "$0") <experiment_id>
+Arguments:
+    experiment_id    Experiment identifier (format: YYMMDD'Bel')
+Example:
+    $(basename "$0") 241010Bel
+EOF
+}
+
+# Execute if run as script
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    download_bmc_data_main "$@"
+fi
diff --git a/bash/scripts/download_eaton_data.sh b/bash/scripts/download_eaton_data.sh
new file mode 100644
index 0000000..68e3176
--- /dev/null
+++ b/bash/scripts/download_eaton_data.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/sra_downloader.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") <directory>
+Downloads Eaton 2010 paper data for control and comparison
+
+Arguments:
+    directory    Target directory name (relative to ${SRA_CONFIG[DATA_DIR]})
+
+Study Information:
+    BioProject: ${EATON_2010[BIOPROJECT]}
+    Description: ${EATON_2010[DESCRIPTION]}
+EOF
+}
+
+function main() {
+    if [ $# -ne 1 ]; then
+        show_usage
+        exit 1
+    }
+    
+    local output_dir=$(validate_input "$1") || exit 1
+    local combined_output="${output_dir}/${EATON_2010[OUTPUT_FILE]}.gz"
+    local downloaded_files=()
+    
+    for file_name in "${!SAMPLES[@]}"; do
+        local accession="${SAMPLES[$file_name]}"
+        local url=$(construct_download_url "$accession")
+        local output_file="${output_dir}/$file_name"
+        
+        verify_url "$url" || continue
+        download_file "$url" "$output_file" || continue
+        
+        downloaded_files+=("$file_name")
+    done
+    
+    if [ ${#downloaded_files[@]} -gt 0 ]; then
+        concatenate_files "$output_dir" "$combined_output" "${downloaded_files[@]}" || exit 1
+        decompress_file "$combined_output" || exit 1
+    else
+        log_error "No files were downloaded successfully"
+        exit 1
+    fi
+    
+    log_info "Processing complete"
+}
+
+main "$@"
diff --git a/bash/scripts/download_feature_data.sh b/bash/scripts/download_feature_data.sh
new file mode 100644
index 0000000..0140445
--- /dev/null
+++ b/bash/scripts/download_feature_data.sh
@@ -0,0 +1,47 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/feature_data_handler.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Downloads feature data from Rossi 2021 paper
+
+Options:
+    -d, --dir DIR     Specify download directory
+                      (default: ${FEATURE_DATA[BASE_DIR]})
+    -h, --help        Show this help message
+
+Description:
+    Downloads and sets up feature data from the Rossi 2021 paper
+    for categorical analysis and plot tracking.
+EOF
+}
+
+function main() {
+    local download_dir="${FEATURE_DATA[BASE_DIR]}"
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -d|--dir) download_dir="$2"; shift 2 ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+    
+    validate_git || exit 1
+    
+    local base_dir=$(setup_feature_directory) || exit 1
+    
+    if ! download_rossi_data "$base_dir"; then
+        log_error "Failed to download Rossi data"
+        exit 1
+    fi
+    
+    log_info "Feature data download completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/generate_coverage.sh b/bash/scripts/generate_coverage.sh
new file mode 100644
index 0000000..f0df1cf
--- /dev/null
+++ b/bash/scripts/generate_coverage.sh
@@ -0,0 +1,57 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/coverage_processor.sh"
+
+function main() {
+    if [ $# -ne 2 ]; then
+        log_error "Usage: $0 <directory> <time_id>"
+        exit 1
+    }
+    
+    local exp_dir="$HOME/data/$1"
+    local time_id="$2"
+    
+    log_info "Starting coverage generation"
+    log_info "Job ID: ${SLURM_JOB_ID}"
+    log_info "Array Task: ${SLURM_ARRAY_TASK_ID}"
+    
+    setup_coverage_directories "$exp_dir" || exit 1
+    
+    # Load required modules
+    module purge
+    for module in "${REQUIRED_MODULES[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            exit 1
+        fi
+    done
+    
+    # Get BAM files
+    mapfile -t bam_files < <(find_bam_files "$exp_dir")
+    
+    if [ ${#bam_files[@]} -eq 0 ]; then
+        log_error "No BAM files found"
+        exit 1
+    }
+    
+    local task_index=$((SLURM_ARRAY_TASK_ID - 1))
+    if [ $task_index -ge ${#bam_files[@]} ]; then
+        log_error "Task ID exceeds number of files"
+        exit 1
+    }
+    
+    local output_file=$(generate_output_name "${bam_files[$task_index]}" \
+                                           "$time_id" \
+                                           "${exp_dir}/${OUTPUT_DIRS[BIGWIG]}")
+    
+    process_bam_coverage "${bam_files[$task_index]}" "$output_file" || exit 1
+    
+    log_info "Coverage generation completed successfully"
+}
+
+main "$@"
diff --git a/next_generation_sequencing/001_referenceGenomes/000_installNcbiDatasetsCli.sh b/bash/scripts/install_ncbi_datasets_cli.sh
similarity index 86%
rename from next_generation_sequencing/001_referenceGenomes/000_installNcbiDatasetsCli.sh
rename to bash/scripts/install_ncbi_datasets_cli.sh
index 35509d0..62a6f3d 100644
--- a/next_generation_sequencing/001_referenceGenomes/000_installNcbiDatasetsCli.sh
+++ b/bash/scripts/install_ncbi_datasets_cli.sh
@@ -1,4 +1,6 @@
 #!/bin/bash
+# functions moved
+# not really needed but added tag
 curl -o datasets 'https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/v2/linux-amd64/datasets'
 curl -o dataformat 'https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/v2/linux-amd64/dataformat'
 chmod +x datasets dataformat
diff --git a/bash/scripts/manage_ngs_files.sh b/bash/scripts/manage_ngs_files.sh
new file mode 100644
index 0000000..4163382
--- /dev/null
+++ b/bash/scripts/manage_ngs_files.sh
@@ -0,0 +1,54 @@
+#!/usr/bin/env bash
+# functions moved
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+source "../functions/ngs_file_manager.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Manage NGS data files
+
+Options:
+    -t, --target DIR     Target directory for file movement
+    -s, --source DIR     Source directory to search
+    -d, --depth NUM      Maximum search depth (default: ${DEFAULTS[MAX_DEPTH]})
+    -b, --batch NUM      Batch size for processing (default: ${DEFAULTS[BATCH_SIZE]})
+    -a, --analyze        Only analyze file distribution
+    -h, --help          Show this help message
+EOF
+}
+
+function main() {
+    local target_dir=""
+    local search_dir="."
+    local do_analyze=false
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -t|--target) target_dir="$2"; shift 2 ;;
+            -s|--source) search_dir="$2"; shift 2 ;;
+            -d|--depth) DEFAULTS[MAX_DEPTH]="$2"; shift 2 ;;
+            -b|--batch) DEFAULTS[BATCH_SIZE]="$2"; shift 2 ;;
+            -a|--analyze) do_analyze=true; shift ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+    
+    if $do_analyze; then
+        analyze_file_distribution "$search_dir"
+    else
+        if [[ -z "$target_dir" ]]; then
+            log_error "Target directory must be specified"
+            show_usage
+            exit 1
+        fi
+        move_ngs_files "$target_dir" "$search_dir"
+    fi
+}
+
+main "$@"
diff --git a/bash/scripts/manage_slurm_outputs.sh b/bash/scripts/manage_slurm_outputs.sh
new file mode 100644
index 0000000..c7f3bfa
--- /dev/null
+++ b/bash/scripts/manage_slurm_outputs.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+# functions moved
+
+source "../functions/slurm_output_manager.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Manage SLURM output files
+
+Options:
+    -d, --directory DIR    Search in specific directory
+    -o, --organize        Organize files into dated structure
+    -c, --cleanup        Remove old log files
+    -l, --large          Check for large files
+    -h, --help           Show this help message
+EOF
+}
+
+function main() {
+    local search_dir="."
+    local do_organize=false
+    local do_cleanup=false
+    local check_large=false
+
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -d|--directory) search_dir="$2"; shift 2 ;;
+            -o|--organize) do_organize=true; shift ;;
+            -c|--cleanup) do_cleanup=true; shift ;;
+            -l|--large) check_large=true; shift ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+
+    if $do_organize; then
+        organize_slurm_files "$search_dir"
+    fi
+
+    if $do_cleanup; then
+        cleanup_old_logs
+    fi
+
+    if $check_large; then
+        check_large_files "$search_dir"
+    fi
+
+    if ! $do_organize && ! $do_cleanup && ! $check_large; then
+        find_slurm_files "$search_dir"
+    fi
+}
+
+main "$@"
diff --git a/bash/scripts/process_reference_genomes.sh b/bash/scripts/process_reference_genomes.sh
new file mode 100644
index 0000000..56526d5
--- /dev/null
+++ b/bash/scripts/process_reference_genomes.sh
@@ -0,0 +1,57 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/ncbi_handler.sh"
+source "../functions/genome_indexer.sh"  # From previous implementation
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Downloads and processes reference genomes from NCBI
+
+Options:
+    -d, --download    Download genomes only
+    -i, --index       Build indices only
+    -a, --all         Download and index genomes
+    -h, --help        Show this help message
+EOF
+}
+
+function main() {
+    local do_download=false
+    local do_index=false
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -d|--download) do_download=true; shift ;;
+            -i|--index) do_index=true; shift ;;
+            -a|--all) do_download=true; do_index=true; shift ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+    
+    if ! $do_download && ! $do_index; then
+        log_error "No operation specified"
+        show_usage
+        exit 1
+    }
+    
+    validate_ncbi_tools || exit 1
+    
+    if $do_download; then
+        log_info "Starting genome downloads"
+        process_genome_batch "${NCBI_CONFIG[ACCESSIONS][@]}" || exit 1
+    fi
+    
+    if $do_index; then
+        log_info "Starting genome indexing"
+        build_genome_indices || exit 1  # From previous implementation
+    fi
+    
+    log_info "All operations completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/reformat_s288c_header.sh b/bash/scripts/reformat_s288c_header.sh
new file mode 100644
index 0000000..820ca82
--- /dev/null
+++ b/bash/scripts/reformat_s288c_header.sh
@@ -0,0 +1,93 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/fasta_processor.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Reformats S288C genome headers to UCSC format
+
+Options:
+    -m, --method METHOD  Use specific method (awk|while)
+    -n, --no-backup     Skip backup creation
+    -r, --restore       Restore from backup
+    -h, --help          Show this help message
+EOF
+}
+
+function find_s288c_genome() {
+    local base_dir="$1"
+    local pattern="${GENOME_FILES[S288C_PATTERN]}"
+    
+    log_info "Searching for S288C genome"
+    
+    local genome_path=$(find "$base_dir" -type f -name "$pattern")
+    
+    if [ -z "$genome_path" ]; then
+        log_error "S288C genome not found"
+        return 1
+    fi
+    
+    echo "$genome_path"
+}
+
+function restore_from_backup() {
+    local genome_path="$1"
+    local backup_file="${genome_path%_refgenome.fna}${GENOME_FILES[BACKUP_SUFFIX]}"
+    
+    if [ ! -f "$backup_file" ]; then
+        log_error "Backup file not found: $backup_file"
+        return 1
+    }
+    
+    log_info "Restoring from backup"
+    cp "$backup_file" "$genome_path"
+}
+
+function main() {
+    local method="awk"
+    local do_backup=${FILE_OPERATIONS[BACKUP]}
+    local restore=false
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -m|--method) method="$2"; shift 2 ;;
+            -n|--no-backup) do_backup=false; shift ;;
+            -r|--restore) restore=true; shift ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+    
+    local genome_path=$(find_s288c_genome "$REFGENOME_DIR") || exit 1
+    
+    if [ "$restore" = true ]; then
+        restore_from_backup "$genome_path"
+        exit 0
+    fi
+    
+    validate_fasta "$genome_path" || exit 1
+    
+    if [ "$do_backup" = true ]; then
+        create_backup "$genome_path" || exit 1
+    fi
+    
+    local backup_file="${genome_path%_refgenome.fna}${GENOME_FILES[BACKUP_SUFFIX]}"
+    
+    if [ "$method" = "awk" ]; then
+        reformat_headers_awk "$backup_file" "$genome_path"
+    else
+        reformat_headers_while "$backup_file" "$genome_path"
+    fi
+    
+    if [ "${FILE_OPERATIONS[VERIFY]}" = true ]; then
+        verify_conversion "$backup_file" "$genome_path" || exit 1
+    fi
+    
+    log_info "Header reformatting completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/reorganize_genomes.sh b/bash/scripts/reorganize_genomes.sh
new file mode 100644
index 0000000..da7fd06
--- /dev/null
+++ b/bash/scripts/reorganize_genomes.sh
@@ -0,0 +1,78 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/genome_organizer.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS]
+Reorganizes reference genome directories
+
+Options:
+    -d, --dir DIR     Process specific directory
+    -a, --all         Process all genome directories
+    -s, --standardize Standardize chromosome names
+    -h, --help        Show this help message
+EOF
+}
+
+function process_genome_directory() {
+    local dir="$1"
+    
+    log_info "Processing directory: $dir"
+    
+    local assembly_report="${dir}/${GENOME_PATHS[NCBI_DATA]}/${GENOME_PATHS[ASSEMBLY_REPORT]}"
+    
+    local organism_name=$(extract_organism_name "$assembly_report") || return 1
+    
+    if ! reorganize_genome_files "$dir" "$organism_name"; then
+        log_error "Failed to reorganize: $dir"
+        return 1
+    }
+    
+    if [ -d "$dir" ] && [ "$dir" != "$organism_name" ]; then
+        mv "$dir" "$organism_name" || {
+            log_error "Failed to rename directory to: $organism_name"
+            return 1
+        }
+    }
+    
+    log_info "Successfully processed: $organism_name"
+    return 0
+}
+
+function main() {
+    local process_all=false
+    local standardize=false
+    local target_dir=""
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -d|--dir) target_dir="$2"; shift 2 ;;
+            -a|--all) process_all=true; shift ;;
+            -s|--standardize) standardize=true; shift ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) log_error "Unknown option: $1"; show_usage; exit 1 ;;
+        esac
+    done
+    
+    if $process_all; then
+        mapfile -t dirs < <(find . -maxdepth 1 -type d -name "GC[AF]_*")
+    elif [ -n "$target_dir" ]; then
+        dirs=("$target_dir")
+    else
+        log_error "No directory specified"
+        show_usage
+        exit 1
+    fi
+    
+    for dir in "${dirs[@]}"; do
+        process_genome_directory "$dir" || continue
+    done
+    
+    log_info "Genome reorganization completed"
+}
+
+main "$@"
diff --git a/bash/scripts/run_alignment.sh b/bash/scripts/run_alignment.sh
new file mode 100644
index 0000000..a628d4b
--- /dev/null
+++ b/bash/scripts/run_alignment.sh
@@ -0,0 +1,75 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/alignment_handler.sh"
+
+function setup_directories() {
+    local base_dir="$1"
+    
+    local dirs=(
+        "logs"
+        "alignment"
+    )
+    
+    for dir in "${dirs[@]}"; do
+        local full_path="${base_dir}/${dir}"
+        mkdir -p "$full_path" || {
+            log_error "Failed to create directory: $full_path"
+            return 1
+        }
+    done
+}
+
+function main() {
+    if [ $# -ne 2 ]; then
+        log_error "Usage: $0 <experiment_dir> <time_id>"
+        exit 1
+    }
+    
+    local exp_dir="$HOME/data/$1"
+    local time_id="$2"
+    
+    setup_directories "$exp_dir" || exit 1
+    
+    log_info "Starting alignment process"
+    log_info "Job ID: ${SLURM_JOB_ID}"
+    log_info "Array Task: ${SLURM_ARRAY_TASK_ID}"
+    
+    # Load required modules
+    module purge
+    for module in "${MODULE_REQUIREMENTS[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            exit 1
+        fi
+    done
+    
+    # Get file lists
+    mapfile -t fastq_files < <(find "$exp_dir" -type f -name "${FILE_PATTERNS[FASTQ]}")
+    mapfile -t genome_files < <(find "$REFGENOME_DIR" -type f -name "${FILE_PATTERNS[GENOME]}")
+    
+    if [ ${#fastq_files[@]} -eq 0 ] || [ ${#genome_files[@]} -eq 0 ]; then
+        log_error "No input files found"
+        exit 1
+    }
+    
+    log_info "Found ${#fastq_files[@]} FASTQ files and ${#genome_files[@]} genomes"
+    
+    # Calculate indices
+    local indices=$(calculate_indices "$SLURM_ARRAY_TASK_ID" "${#fastq_files[@]}")
+    local genome_index=${indices%:*}
+    local fastq_index=${indices#*:}
+    
+    perform_alignment "${genome_files[$genome_index]}" \
+                     "${fastq_files[$fastq_index]}" \
+                     "${exp_dir}/alignment" \
+                     "$SLURM_CPUS_PER_TASK" || exit 1
+    
+    log_info "Alignment task completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/run_bam_comparison.sh b/bash/scripts/run_bam_comparison.sh
new file mode 100644
index 0000000..d28d5d8
--- /dev/null
+++ b/bash/scripts/run_bam_comparison.sh
@@ -0,0 +1,50 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/r_integration.sh"
+source "../functions/bam_comparer.sh"
+
+function main() {
+    if [ $# -ne 2 ]; then
+        log_error "Usage: $0 <directory> <time_id>"
+        exit 1
+    }
+    
+    local exp_dir="$HOME/data/$1"
+    local time_id="$2"
+    
+    log_info "Starting BAM comparison"
+    log_info "Job ID: ${SLURM_JOB_ID}"
+    log_info "Array Task: ${SLURM_ARRAY_TASK_ID}"
+    
+    # Load required modules
+    module purge
+    for module in "${REQUIRED_MODULES[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            exit 1
+        fi
+    done
+    
+    # Get BAM pairs from R
+    mapfile -t bam_pairs < <(get_bam_pairs "$(basename "$exp_dir")" "$SLURM_ARRAY_TASK_ID")
+    
+    validate_bam_pairs "${bam_pairs[@]}" || exit 1
+    
+    local output_file=$(generate_output_name "${bam_pairs[0]}" \
+                                           "${bam_pairs[1]}" \
+                                           "$time_id" \
+                                           "${exp_dir}/${OUTPUT_DIRS[BIGWIG]}")
+    
+    local threads=$((SLURM_CPUS_PER_TASK / 2))
+    
+    run_comparison "${bam_pairs[0]}" "${bam_pairs[1]}" "$output_file" "$threads" || exit 1
+    
+    log_info "Comparison completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/run_bam_qc.sh b/bash/scripts/run_bam_qc.sh
new file mode 100644
index 0000000..60aef4c
--- /dev/null
+++ b/bash/scripts/run_bam_qc.sh
@@ -0,0 +1,53 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/bam_processor.sh"
+
+function main() {
+    if [ $# -ne 2 ]; then
+        log_error "Usage: $0 <directory> <time_id>"
+        exit 1
+    }
+    
+    local exp_dir="$HOME/data/$1"
+    local time_id="$2"
+    
+    log_info "Starting BAM quality control"
+    log_info "Job ID: ${SLURM_JOB_ID}"
+    log_info "Array Task: ${SLURM_ARRAY_TASK_ID}"
+    
+    setup_qc_directories "$exp_dir" || exit 1
+    
+    # Load required modules
+    module purge
+    for module in "${REQUIRED_MODULES[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            exit 1
+        fi
+    done
+    
+    # Get BAM files
+    mapfile -t bam_files < <(find_bam_files "$exp_dir")
+    
+    if [ ${#bam_files[@]} -eq 0 ]; then
+        log_error "No BAM files found"
+        exit 1
+    }
+    
+    local task_index=$((SLURM_ARRAY_TASK_ID - 1))
+    if [ $task_index -ge ${#bam_files[@]} ]; then
+        log_error "Task ID exceeds number of files"
+        exit 1
+    }
+    
+    process_bam_file "${bam_files[$task_index]}" "${exp_dir}/${QC_DIRS[OUTPUT]}" || exit 1
+    
+    log_info "Quality control completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/run_fastq_filtering.sh b/bash/scripts/run_fastq_filtering.sh
new file mode 100644
index 0000000..a7cbe95
--- /dev/null
+++ b/bash/scripts/run_fastq_filtering.sh
@@ -0,0 +1,53 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/fastq_processor.sh"
+
+function main() {
+    if [ $# -ne 2 ]; then
+        log_error "Usage: $0 <directory> <time_id>"
+        exit 1
+    }
+    
+    local exp_dir="$HOME/data/$1"
+    local time_id="$2"
+    
+    log_info "Starting FASTQ processing"
+    log_info "Job ID: ${SLURM_JOB_ID}"
+    log_info "Array Task: ${SLURM_ARRAY_TASK_ID}"
+    
+    setup_output_directories "$exp_dir" || exit 1
+    
+    # Load required modules
+    module purge
+    for module in "${REQUIRED_MODULES[@]}"; do
+        if ! module load "$module"; then
+            log_error "Failed to load module: $module"
+            exit 1
+        fi
+    done
+    
+    # Get input files
+    mapfile -t input_files < <(find_input_files "$exp_dir")
+    
+    if [ ${#input_files[@]} -eq 0 ]; then
+        log_error "No input files found"
+        exit 1
+    }
+    
+    local task_index=$((SLURM_ARRAY_TASK_ID - 1))
+    if [ $task_index -ge ${#input_files[@]} ]; then
+        log_error "Task ID exceeds number of files"
+        exit 1
+    }
+    
+    process_fastq_file "${input_files[$task_index]}" "$exp_dir" || exit 1
+    
+    log_info "Processing completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/run_quality_control.sh b/bash/scripts/run_quality_control.sh
new file mode 100644
index 0000000..7aabca2
--- /dev/null
+++ b/bash/scripts/run_quality_control.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+# functions moved
+
+#SBATCH parameters from SLURM_CONFIG
+
+set -euo pipefail
+
+source "../functions/quality_control.sh"
+
+function validate_input() {
+    local dir_name="$1"
+    
+    if [ -z "$dir_name" ]; then
+        log_error "Directory name not provided"
+        return 1
+    }
+    
+    local full_path="$HOME/data/$dir_name"
+    if [ ! -d "$full_path" ]; then
+        log_error "Directory not found: $full_path"
+        return 1
+    }
+    
+    echo "$full_path"
+}
+
+function main() {
+    if [ $# -lt 1 ]; then
+        log_error "Usage: $0 <directory_name>"
+        exit 1
+    }
+    
+    local base_dir=$(validate_input "$1") || exit 1
+    local qc_dir=$(setup_qc_directory "$base_dir") || exit 1
+    
+    load_required_modules || exit 1
+    
+    mapfile -t raw_files < <(find_fastq_files "$base_dir")
+    mapfile -t processed_files < <(find_processed_fastq "$base_dir")
+    
+    local task_index=$((SLURM_ARRAY_TASK_ID - 1))
+    
+    if [ -n "${raw_files[$task_index]:-}" ]; then
+        run_fastqc "${raw_files[$task_index]}" "$qc_dir" || exit 1
+    fi
+    
+    if [ -n "${processed_files[$task_index]:-}" ]; then
+        run_fastqc "${processed_files[$task_index]}" "$qc_dir" || exit 1
+    fi
+    
+    log_info "Quality control completed successfully"
+}
+
+main "$@"
diff --git a/bash/scripts/submit_slurm_job.sh b/bash/scripts/submit_slurm_job.sh
new file mode 100644
index 0000000..06a45c4
--- /dev/null
+++ b/bash/scripts/submit_slurm_job.sh
@@ -0,0 +1,85 @@
+#!/bin/bash
+
+source "$HOME/lab_utils/bash/functions/slurm_wrapper.sh"
+#' Submit SLURM Job Main Function
+#' @param array_range Character SLURM array specification
+#' @param script_name Character Script name
+#' @param dir_name Character Directory name
+#' @return Integer 0 if successful
+submit_slurm_job_main() {
+    if [[ $# -ne 3 ]]; then
+        show_usage
+        return 1
+    fi
+    
+    # Initialize logging with timestamp for concurrent access
+    local timestamp_for_log=$(date +%s)
+    local log_file
+    log_file=$(initialize_logging "slurm_submit_${timestamp_for_log}")
+    
+    local array_range="$1"
+    local script_name="$2"
+    local dir_name="$3"
+    local timestamp_for_data=$(date "+%Y-%m-%d-%H-%M-%S")
+    
+    # Validate inputs
+    if ! validate_array_range "$array_range" "$log_file"; then
+        return 1
+    fi
+    
+    local script_path
+    if ! script_path=$(find_slurm_script "$script_name" "$log_file"); then
+        return 1
+    fi
+    
+    local exp_dir
+    if ! exp_dir=$(validate_experiment_dir "$dir_name" "$log_file"); then
+        return 1
+    fi
+    
+    # Submit job
+    log_info "Submitting SLURM job:" "$log_file"
+    log_info "  Directory: $exp_dir" "$log_file"
+    log_info "  Script: $script_path" "$log_file"
+    log_info "  Array: $array_range" "$log_file"
+    
+    local job_id
+    if ! job_id=$(submit_slurm_array_job "$array_range" "$script_name" "$exp_dir" "$job_type"  "$timestamp_for_data" "$log_file"); then
+        return 1
+    fi
+    
+    if [[ -z "$job_id" ]]; then
+        log_error "Job submission failed" "$log_file"
+        return 1
+    fi
+    
+    print_job_info "$job_id"  "$log_file"
+    return 0
+}
+
+
+show_usage() {
+    cat << EOF
+Usage: $(basename "$0") <array_range> <script_name> <directory>
+
+Arguments:
+    array_range    SLURM array specification:
+                   - Single task: "1"
+                   - Multiple tasks: "1,2,5"
+                   - Range: "1-10" (automatically adds %16)
+    script_name    Script to execute
+    directory      Experiment directory name
+
+Examples:
+    $(basename "$0") "1-10" "align_fastq.sh" "240304Bel"
+    $(basename "$0") "1" "align_fastq.sh" "240304Bel"
+    $(basename "$0") "1,2,5" "align_fastq.sh" "240304Bel"
+
+Note: submit_slurm_job automatically applies %16 limit to ranges
+EOF
+}
+
+# Execute if run as script
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    submit_slurm_job_main "$@"
+fi
diff --git a/bash/scripts/submit_test_job.sh b/bash/scripts/submit_test_job.sh
new file mode 100644
index 0000000..8ba02b9
--- /dev/null
+++ b/bash/scripts/submit_test_job.sh
@@ -0,0 +1,14 @@
+# bash/scripts/submit_test_job.sh
+#!/bin/bash
+
+source "$HOME/lab_utils/bash/functions/slurm_wrapper.sh"
+
+# Calculate array size based on experiment
+experiment_dir="241028Bel"
+array_range="1-4"  # For testing
+
+# Submit test job
+submit_slurm_job_main \
+    "$array_range" \
+    "test_slurm_settings.sh" \
+    "$experiment_dir"
diff --git a/bash/scripts/test_slurm_settings.sh b/bash/scripts/test_slurm_settings.sh
new file mode 100644
index 0000000..0664a1c
--- /dev/null
+++ b/bash/scripts/test_slurm_settings.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+# bash/scripts/test_slurm_settings.sh
+
+source "$HOME/lab_utils/bash/functions/logging_utils.sh"
+
+#' Test SLURM Settings
+#' @param experiment_dir Character Experiment directory
+#' @return Integer 0 if successful
+test_slurm_settings_main() {
+    if [[ $# -ne 1 ]]; then
+        echo "Usage: $0 <experiment_dir>"
+        return 1
+    }
+    
+    local experiment_dir="$1"
+    local log_file
+    log_file=$(initialize_logging "test_slurm")
+    
+    # Print SLURM Environment
+    cat << EOF
+=== SLURM Job Information ===
+Job ID: ${SLURM_JOB_ID:-Not Set}
+Array Job ID: ${SLURM_ARRAY_JOB_ID:-Not Set}
+Array Task ID: ${SLURM_ARRAY_TASK_ID:-Not Set}
+Job Name: ${SLURM_JOB_NAME:-Not Set}
+
+=== SLURM Resource Allocation ===
+Nodes: ${SLURM_JOB_NODELIST:-Not Set}
+Node Count: ${SLURM_NNODES:-Not Set}
+CPUs per Task: ${SLURM_CPUS_PER_TASK:-Not Set}
+Tasks per Node: ${SLURM_NTASKS_PER_NODE:-Not Set}
+Memory per Node: ${SLURM_MEM_PER_NODE:-Not Set}
+
+=== Directory Information ===
+Working Directory: ${SLURM_SUBMIT_DIR:-Not Set}
+Experiment Directory: $experiment_dir
+
+=== Module Status ===
+$(module list 2>&1)
+
+=== System Information ===
+Hostname: $(hostname)
+Current Directory: $(pwd)
+User: $USER
+Date: $(date)
+EOF
+
+    return 0
+}
+
+# Execute if run as script
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    test_slurm_settings_main "$@"
+fi
diff --git a/bash/scripts/transfer_bmc_experiment_directory_to_luria.sh b/bash/scripts/transfer_bmc_experiment_directory_to_luria.sh
new file mode 100755
index 0000000..ccacb10
--- /dev/null
+++ b/bash/scripts/transfer_bmc_experiment_directory_to_luria.sh
@@ -0,0 +1,128 @@
+#!/bin/bash
+# bash/scripts/transfer_bmc_experiment_to_luria.sh
+# Requires password twice, for the transfer and verification.
+# Run after setup_bmc_experiment.R
+
+# Source dependencies
+source "$HOME/lab_utils/bash/functions/logging_utils.sh"
+
+#' Check Network Connectivity
+#' @param remote_host Character Remote host address
+#' @return Integer 0 if successful, 1 otherwise
+check_connection() {
+    local remote_host="$1"
+    
+    if ! ping -c 1 "$remote_host" &> /dev/null; then
+        log_error "Cannot connect to $remote_host"
+        log_info "Please ensure:"
+        log_info "1. VPN is connected"
+        log_info "2. You can connect via: ssh -A -Y ${PROJECT_CONFIG[REMOTE_USER]}@$remote_host"
+        return 1
+    fi
+    return 0
+}
+
+#' Validate Local Directory Structure
+#' @param dir Character Directory to validate
+#' @return Integer 0 if valid, 1 otherwise
+validate_directory() {
+    local dir="$1"
+    local log_file="$2"
+    
+    if [ ! -d "$dir" ]; then
+        log_error "Directory not found: $dir" "${log_file}"
+        log_info "Provide full path." "${log_file}"
+        return 1
+    fi
+    
+    # Check required subdirectories
+    for subdir in ${PROJECT_CONFIG[REQUIRED_DIRS]}; do
+        if [ ! -d "$dir/$subdir" ]; then
+            log_error "Required subdirectory missing: $subdir"
+            return 1
+        fi
+    done
+    return 0
+}
+
+#' Transfer Data to Remote Host
+#' @param source_dir Character Source directory
+#' @param log_file Character Log file path
+#' @return Integer 0 if successful, 1 otherwise
+transfer_data() {
+    local source_dir="$1"
+    local log_file="$2"
+    local experiment_id=$(basename "$source_dir")
+    
+    log_info "Starting transfer of $experiment_id" "$log_file"
+    
+    rsync -avzP --stats \
+        "$source_dir/" \
+        "${PROJECT_CONFIG[REMOTE_USER]}@${PROJECT_CONFIG[REMOTE_HOST]}:${PROJECT_CONFIG[REMOTE_PATH]}/$experiment_id/" \
+        2>&1 | tee -a "$log_file"
+    
+    if [ ${PIPESTATUS[0]} -eq 0 ]; then
+        log_info "Transfer completed successfully" "$log_file"
+        return 0
+    else
+        log_error "Transfer failed" "$log_file"
+        return 1
+    fi
+}
+
+#' Verify Transfer Completion
+#' @param source_dir Character Source directory
+#' @param log_file Character Log file path
+#' @return Integer 0 if verified, 1 otherwise
+verify_transfer() {
+    local source_dir="$1"
+    local log_file="$2"
+    local experiment_id=$(basename "$source_dir")
+    
+    log_info "Verifying transfer" "$log_file"
+    
+    local local_count=$(find "$source_dir" -type f | wc -l)
+    local remote_count=$(ssh "${PROJECT_CONFIG[REMOTE_USER]}@${PROJECT_CONFIG[REMOTE_HOST]}" \
+        "find ${PROJECT_CONFIG[REMOTE_PATH]}/$experiment_id -type f | wc -l")
+    
+    if [ "$local_count" -eq "$remote_count" ]; then
+        log_info "Verification successful: $local_count files transferred" "$log_file"
+        return 0
+    else
+        log_error "Verification failed: Local=$local_count Remote=$remote_count" "$log_file"
+        return 1
+    fi
+}
+
+#' Main Function
+#' @param args Array Script arguments
+#' @return None
+transfer_bmc_experiment_to_luria_main() {
+    # Initialize logging
+    local log_file
+    log_file="$(initialize_logging "transfer_bmc_experiment")"
+    
+    if [ $# -ne 1 ]; then
+        log_error "Usage: $0 <experiment_directory>" "$log_file"
+        exit 1
+    fi
+    
+    local source_dir="$1"
+    
+    # Run checks
+    check_connection "${PROJECT_CONFIG[REMOTE_HOST]}" || exit 1
+    validate_directory "$source_dir" "$log_file" || exit 1
+    
+    # Transfer and verify
+    transfer_data "$source_dir" "$log_file" || exit 1
+    verify_transfer "$source_dir" "$log_file" || exit 1
+    
+    log_info "Next steps:" "$log_file"
+    log_info "1. Login to cluster: ssh -A -Y ${PROJECT_CONFIG[REMOTE_USER]}@${PROJECT_CONFIG[REMOTE_HOST]}" "$log_file"
+    log_info "2. Run: bash ~/lab_utils/bash/scripts/download_bmc_data.sh" "$log_file"
+}
+
+# Execute if run as script
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    transfer_bmc_experiment_to_luria_main "$@"
+fi
diff --git a/bash/scripts/unzip_fastqc.sh b/bash/scripts/unzip_fastqc.sh
new file mode 100644
index 0000000..1a6ee8e
--- /dev/null
+++ b/bash/scripts/unzip_fastqc.sh
@@ -0,0 +1,96 @@
+#!/bin/bash
+# functions moved
+
+set -euo pipefail
+
+source "../functions/archive_handler.sh"
+
+function show_usage() {
+    cat << EOF
+Usage: $(basename "$0") [OPTIONS] <directory>
+Unzips FASTQC result files in specified directory
+
+Options:
+    -f, --force       Skip confirmation
+    -k, --keep        Keep ZIP files after extraction
+    -b, --batch SIZE  Process in batches of SIZE files
+    -h, --help        Show this help message
+
+Example:
+    $(basename "$0") experiment_20240101
+EOF
+}
+
+function confirm_operation() {
+    local files="$1"
+    local timeout="${OPERATION_DEFAULTS[CONFIRM_TIMEOUT]}"
+    
+    echo "Files to process:"
+    echo "$files"
+    echo
+    
+    read -t "$timeout" -p "Proceed with unzipping these files? (y/n): " -r || {
+        echo
+        log_error "Confirmation timed out"
+        return 1
+    }
+    
+    [[ $REPLY =~ ^[Yy]$ ]]
+}
+
+function validate_directory() {
+    local dir_name="$1"
+    local base_dir="${QC_PATHS[BASE_DIR]}"
+    
+    log_info "Validating directory: $dir_name"
+    
+    local full_path=$(find -H "$base_dir" \
+                      -maxdepth "${QC_PATHS[MAX_DEPTH]}" \
+                      -type d \
+                      -name "$dir_name")
+    
+    if [ -z "$full_path" ]; then
+        log_error "Directory not found: $dir_name"
+        return 1
+    }
+    
+    echo "$full_path"
+}
+
+function main() {
+    local force=false
+    local preserve=${OPERATION_DEFAULTS[PRESERVE_ZIP]}
+    local batch_size=${OPERATION_DEFAULTS[UNZIP_BATCH_SIZE]}
+    
+    while [[ $# -gt 0 ]]; do
+        case $1 in
+            -f|--force) force=true; shift ;;
+            -k|--keep) preserve=true; shift ;;
+            -b|--batch) batch_size="$2"; shift 2 ;;
+            -h|--help) show_usage; exit 0 ;;
+            *) break ;;
+        esac
+    done
+    
+    if [ $# -ne 1 ]; then
+        log_error "Directory name required"
+        show_usage
+        exit 1
+    fi
+    
+    local exp_dir=$(validate_directory "$1") || exit 1
+    local qc_dir="$exp_dir/${QC_PATHS[QC_SUBDIR]}"
+    
+    mapfile -t zip_files < <(find_zip_files "$qc_dir") || {
+        log_error "No ZIP files found in: $qc_dir"
+        exit 1
+    }
+    
+    if [ "$force" = false ]; then
+        confirm_operation "${zip_files[*]}" || exit 1
+    fi
+    
+    process_zip_files "${zip_files[@]}"
+}
+
+main "$@"
diff --git a/bash/templates/initialize_lab_environment_snippet.sh b/bash/templates/initialize_lab_environment_snippet.sh
new file mode 100644
index 0000000..43d29cc
--- /dev/null
+++ b/bash/templates/initialize_lab_environment_snippet.sh
@@ -0,0 +1,33 @@
+#!/bin/bash
+# Example script usage
+
+# Initialize lab environment
+if [[ -z "$LAB_UTILS_ROOT" ]]; then
+    # Try common locations
+    for dir in \
+        "$HOME/lab_utils" \
+        "$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)" \
+        "${LAB_UTILS_ROOT:-}"; do
+        if [[ -f "$dir/bash/core/initialize_lab_environment.sh" ]]; then
+            source "$dir/bash/core/initialize_lab_environment.sh"
+            break
+        fi
+    done
+    
+    # Check initialization
+    if [[ -z "$LAB_UTILS_INITIALIZED" ]]; then
+        echo "ERROR: Failed to initialize lab environment" >&2
+        exit 1
+    fi
+fi
+
+# Script logic
+main() {
+    local log_file
+    log_file=$(initialize_logging "$(basename "$0")")
+    
+    # Your code here
+    log_info "Script running" "$log_file"
+}
+
+main "$@"
diff --git a/bash/tests/core/run_tests.sh b/bash/tests/core/run_tests.sh
new file mode 100755
index 0000000..4237d7d
--- /dev/null
+++ b/bash/tests/core/run_tests.sh
@@ -0,0 +1,60 @@
+#!/bin/bash
+# bash/tests/core/run_tests.sh
+
+run_core_tests() {
+    local start_time=$(date +%s)
+    local repo_root
+    local failed=0
+
+    if ! repo_root=$(git rev-parse --show-toplevel 2>/dev/null); then
+        echo "[ERROR] Not in a git repository"
+        return 1
+    fi
+    
+    local test_dir="$repo_root/bash/tests/core"
+    
+    echo "[START] Lab Utils Core Test Suite"
+    
+    # Source and setup test environment
+    #source "$test_dir/verify_initialize_lab_environment.sh" || {
+    #    echo "[ERROR] Failed to source test environment"
+    #    return 1
+    #}
+    
+    # Source and setup test environment
+    source "$test_dir/test_setup.sh" || {
+        echo "[ERROR] Failed to source test environment"
+        return 1
+    }
+    setup_test_environment || {
+        echo "[ERROR] Failed to setup test environment"
+        return 1
+    }
+
+    # Advanced tests
+    local tests=(
+        "test_config_export.sh"
+        "test_logging_concurrent.sh"
+        "test_lock_recovery.sh"
+        "test_error_conditions.sh"
+    )
+    
+    for test in "${tests[@]}"; do
+        echo "[TEST] Running $test"
+        if ! "$test_dir/$test"; then
+            ((failed++))
+            echo "[FAIL] | Test failed: $test"
+        else
+            echo "[PASS] | Test passed: $test"
+        fi
+    done
+    
+    local duration=$(($(date +%s) - start_time))
+    echo "[INFO] Verification completed in ${duration}s"
+    echo "[END] Test suite complete (Failed: $failed)"
+    
+    return $failed
+}
+
+# Run tests if executed directly
+[[ "${BASH_SOURCE[0]}" == "${0}" ]] && run_core_tests
diff --git a/bash/tests/core/test_config_export.sh b/bash/tests/core/test_config_export.sh
new file mode 100755
index 0000000..d75acf7
--- /dev/null
+++ b/bash/tests/core/test_config_export.sh
@@ -0,0 +1,71 @@
+#!/bin/bash
+# bash/tests/core/test_config_export.sh
+
+test_config_export() {
+    local test_dir="$repo_root/bash/tests/core"
+    
+    # First ensure environment is initialized
+    source "$test_dir/test_setup.sh" || {
+        echo "[ERROR] Failed to source test environment"
+        return 1
+    }
+    setup_test_environment || {
+        echo "[ERROR] Failed to setup test environment"
+        return 1
+    }
+
+    echo "[TEST] Testing configuration export"
+    
+    # Test 1: Verify serialized config exists
+    if [[ -z "$LAB_UTILS_CONFIG_SERIALIZED" ]]; then
+        echo "[FAIL] Missing serialized configuration"
+        # Debug output
+        echo "[DEBUG] Environment variables:"
+        env | grep LAB_UTILS
+        return 1
+    fi
+    
+    # Test 2: Verify config values are exported
+    if [[ -z "$LAB_UTILS_LOG_LEVELS" ]]; then
+        echo "[FAIL] Missing exported LOG_LEVELS"
+        return 1
+    fi
+    
+    # Test 3: Verify import function
+    (
+        # Clear existing config
+        unset CORE_CONFIG
+        declare -A CORE_CONFIG
+        
+        # Import config
+        import_core_config
+        
+        # Verify critical values
+        if [[ -z "${CORE_CONFIG[LOG_LEVELS]}" ]]; then
+            echo "[FAIL] Failed to import LOG_LEVELS"
+            return 1
+        fi
+        
+        # Verify specific values match
+        if [[ "${CORE_CONFIG[LOG_LEVELS]}" != "$LAB_UTILS_LOG_LEVELS" ]]; then
+            echo "[FAIL] Imported config doesn't match exported values"
+            echo "[DEBUG] Imported: ${CORE_CONFIG[LOG_LEVELS]}"
+            echo "[DEBUG] Exported: $LAB_UTILS_LOG_LEVELS"
+            return 1
+        fi
+    ) || return 1
+    
+    echo "[PASS] Configuration export verified"
+    return 0
+}
+
+# Only run directly if not being sourced
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    # Get repository root
+    repo_root=$(git rev-parse --show-toplevel 2>/dev/null) || {
+        echo "[ERROR] Not in a git repository"
+        exit 1
+    }
+    
+    test_config_export
+fi
diff --git a/bash/tests/core/test_error_conditions.sh b/bash/tests/core/test_error_conditions.sh
new file mode 100755
index 0000000..5abff03
--- /dev/null
+++ b/bash/tests/core/test_error_conditions.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+# bash/tests/core/test_error_conditions.sh
+#
+# Source test environment
+source "$(dirname "${BASH_SOURCE[0]}")/test_setup.sh" || exit 1
+setup_test_environment || exit 1
+
+test_error_conditions() {
+    echo " Testing error conditions"
+    local failed=0
+    
+    # Test invalid log levels
+    echo "   Testing invalid log level"
+    if log_message "INVALID" "Test" "/dev/null" 2>/dev/null; then
+        echo "    ? Should have failed"
+        ((failed++))
+    else
+        echo "     Properly rejected"
+    fi
+    
+    # Test protected paths
+    echo "   Testing protected paths"
+    if acquire_lock "/etc/test" 2>/dev/null; then
+        echo "    ? Should have failed"
+        ((failed++))
+    else
+        echo "     Properly rejected"
+    fi
+    
+    # Test invalid configurations
+    echo "   Testing invalid config"
+    local old_config="${CORE_CONFIG[LOG_LEVELS]}"
+    CORE_CONFIG[LOG_LEVELS]=""
+    if log_message "INFO" "Test" "/dev/null" 2>/dev/null; then
+        echo "    ? Should have failed"
+        ((failed++))
+    else
+        echo "     Properly rejected"
+    fi
+    CORE_CONFIG[LOG_LEVELS]="$old_config"
+    
+    return $failed
+}
+
+[[ "${BASH_SOURCE[0]}" == "${0}" ]] && test_error_conditions
diff --git a/bash/tests/core/test_lock.sh b/bash/tests/core/test_lock.sh
new file mode 100755
index 0000000..db67526
--- /dev/null
+++ b/bash/tests/core/test_lock.sh
@@ -0,0 +1,21 @@
+#!/bin/bash
+
+# Test script for lock path validation
+test_lock_validation() {
+    local test_cases=(
+        "my_process"                           # Should pass
+        "../my_process"                        # Should fail
+        "/tmp/lab_utils_locks/${USER}/test"   # Should pass
+        "/etc/test"                           # Should fail
+        "/tmp/other_user/test"                # Should fail
+    )
+    
+    for test in "${test_cases[@]}"; do
+        echo "Testing: $test"
+        if validate_lock_path "$test"; then
+            echo " Passed"
+        else
+            echo "x Failed"
+        fi
+    done
+}
diff --git a/bash/tests/core/test_lock_recovery.sh b/bash/tests/core/test_lock_recovery.sh
new file mode 100755
index 0000000..61b16b5
--- /dev/null
+++ b/bash/tests/core/test_lock_recovery.sh
@@ -0,0 +1,39 @@
+#!/bin/bash
+# bash/tests/core/test_lock_recovery.sh
+#
+# Source test environment
+source "$(dirname "${BASH_SOURCE[0]}")/test_setup.sh" || exit 1
+setup_test_environment || exit 1
+
+test_lock_recovery() {
+    echo " Testing lock recovery"
+    local test_dir="/tmp/lab_utils_test_$$"
+    local lock_file="$test_dir/stale.lock"
+    source "$HOME/lab_utils/bash/config/core_config.sh"
+    
+    # Create stale lock
+    mkdir -p "$lock_file"
+    echo "99999" > "$lock_file/pid"
+    
+    # Test scenarios
+    echo "   Testing stale lock cleanup"
+    if cleanup_stale_locks; then
+        echo "     Cleanup successful"
+    else
+        echo "    ? Cleanup failed"
+        return 1
+    fi
+    
+    echo "   Testing forced release"
+    if release_lock "stale" true; then
+        echo "     Force release successful"
+    else
+        echo "    ? Force release failed"
+        return 1
+    fi
+    
+    rm -rf "$test_dir"
+    return 0
+}
+
+[[ "${BASH_SOURCE[0]}" == "${0}" ]] && test_lock_recovery
diff --git a/bash/tests/core/test_logging_concurrent.sh b/bash/tests/core/test_logging_concurrent.sh
new file mode 100755
index 0000000..21cc5cd
--- /dev/null
+++ b/bash/tests/core/test_logging_concurrent.sh
@@ -0,0 +1,69 @@
+#!/bin/bash
+# bash/tests/core/test_logging_concurrent.sh
+
+test_concurrent_logging() {
+    local test_dir="/tmp/lab_utils_test_$$"
+    local log_file="$test_dir/concurrent.log"
+    local pids=()
+    
+    # Initialize environment if running standalone
+    if [[ -z "$LAB_UTILS_INITIALIZED" ]]; then
+        source "$(dirname "${BASH_SOURCE[0]}")/test_setup.sh" || exit 1
+        setup_test_environment || exit 1
+    fi
+
+    echo "[TEST] Testing concurrent logging"
+    
+    # Ensure test directory exists
+    mkdir -p "$test_dir" || {
+        echo "[FAIL] Failed to create test directory"
+        return 1
+    }
+    
+    # Touch log file to ensure it exists
+    touch "$log_file" || {
+        echo "[FAIL] Failed to create log file"
+        return 1
+    }
+
+    # Export necessary functions for subprocesses
+    export -f log_message
+    export -f write_log_atomic
+    
+    # Launch parallel processes
+    for i in {1..5}; do
+        (
+            # Source core config directly in subprocess
+            source "${LAB_UTILS_ROOT}/bash/config/core_config.sh"
+            
+            for j in {1..10}; do
+                log_message "INFO" "Test message $i-$j" "$log_file"
+                sleep 0.1
+            done
+        ) &
+        pids+=($!)
+    done
+    
+    # Wait for completion
+    for pid in "${pids[@]}"; do
+        wait "$pid"
+    done
+    
+    # Verify results
+    local expected=50
+    local actual=$(wc -l < "$log_file")
+    
+    if [[ "$actual" -eq "$expected" ]]; then
+        echo "[PASS] All messages logged ($actual/$expected)"
+        rm -rf "$test_dir"
+        return 0
+    else
+        echo "[FAIL] Message count mismatch ($actual/$expected)"
+        return 1
+    fi
+}
+
+# Run test if executed directly
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    test_concurrent_logging
+fi
diff --git a/bash/tests/core/test_root_directory.sh b/bash/tests/core/test_root_directory.sh
new file mode 100755
index 0000000..abd3699
--- /dev/null
+++ b/bash/tests/core/test_root_directory.sh
@@ -0,0 +1,41 @@
+# bash/tests/core/test_root_discovery.sh
+
+#!/bin/bash
+
+test_root_discovery() {
+    echo " Testing root discovery"
+    
+    # Test from different directories
+    local dirs=(
+        "bash/core"
+        "bash/tests/core"
+        "R/core"
+        "."
+    )
+    
+    for dir in "${dirs[@]}"; do
+        echo "   Testing from: $dir"
+        (
+            cd "$dir" 2>/dev/null || {
+                echo "    ? Failed to change to directory"
+                return 1
+            }
+            
+            local root
+            root="$(discover_lab_utils_root)" || {
+                echo "    ? Root discovery failed"
+                return 1
+            }
+            
+            echo "     Found root: $root"
+        )
+    done
+    
+    echo " Tests complete"
+}
+
+# Run if executed directly
+if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
+    source "../../core/initialize_lab_environment.sh"
+    test_root_discovery
+fi
diff --git a/bash/tests/core/test_setup.sh b/bash/tests/core/test_setup.sh
new file mode 100755
index 0000000..954c1b9
--- /dev/null
+++ b/bash/tests/core/test_setup.sh
@@ -0,0 +1,71 @@
+# bash/tests/core/test_setup.sh
+
+#!/bin/bash
+
+#' Setup Test Environment
+#' @description Source all required files for testing
+setup_test_environment() {
+    # Use git to find repository root
+    local repo_root
+    if ! repo_root=$(git rev-parse --show-toplevel 2>/dev/null); then
+        echo "? Not in a git repository"
+        return 1
+    fi
+    
+    echo " Setting up test environment"
+    echo "   Repository root: $repo_root"
+    
+    # Source core configuration
+    if ! source "$repo_root/bash/config/core_config.sh"; then
+        echo "  ? Failed to source core config"
+        return 1
+    fi
+    
+    # Source core modules
+    local core_modules=(
+        "logging.sh"
+        "lock.sh"
+    )
+    
+    for module in "${core_modules[@]}"; do
+        if ! source "$repo_root/bash/core/$module"; then
+            echo "  ? Failed to source $module"
+            return 1
+        fi
+    done
+    
+    # Verify environment
+    echo "   Verifying environment"
+    echo "     LOG_LEVELS: ${CORE_CONFIG[LOG_LEVELS]}"
+    echo "     MAX_MESSAGE_LENGTH: ${CORE_CONFIG[MAX_MESSAGE_LENGTH]}"
+    
+    return 0
+}
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
diff --git a/bash/tests/core/verify_initialize_lab_environment.sh b/bash/tests/core/verify_initialize_lab_environment.sh
new file mode 100755
index 0000000..0ac29f9
--- /dev/null
+++ b/bash/tests/core/verify_initialize_lab_environment.sh
@@ -0,0 +1,122 @@
+#!/bin/bash
+# verify_init.sh
+
+test_lock_management() {
+    echo " Testing lock management"
+    local test_lock="test_lock_$$"
+    
+    if acquire_lock "$test_lock" 5; then
+        echo "   Lock acquired"
+        if release_lock "$test_lock"; then
+            echo "   Lock released"
+            return 0
+        fi
+    fi
+    echo "? Lock management failed"
+    return 1
+}
+
+test_core_config() {
+    local required_keys=(
+        "VERSION"
+        "LOG_LEVELS"
+        "DEFAULT_LOG_ROOT"
+        "LOCK_BASE_DIR"
+    )
+    
+    for key in "${required_keys[@]}"; do
+        [[ -n "${CORE_CONFIG[$key]}" ]] || return 1
+    done
+}
+
+test_directory_permissions() {
+    local dirs=(
+        "${CORE_CONFIG[DEFAULT_LOG_ROOT]}"
+        "${CORE_CONFIG[LOCK_BASE_DIR]}"
+    )
+    
+    for dir in "${dirs[@]}"; do
+        [[ -w "$dir" ]] || return 1
+    done
+}
+
+test_guard_mechanism() {
+    echo " Testing guard mechanism"
+    (
+        source "${LAB_UTILS_ROOT}/bash/core/initialize_lab_environment.sh"
+        [[ -n "$LAB_UTILS_INITIALIZED" ]] || {
+            echo "? Guard failed"
+            return 1
+        }
+    )
+    echo "   Guard verified"
+    return 0
+}
+# Quick initialization verification
+verify_init() {
+    local failed=0
+    echo " Starting verification"
+    local log_file
+    
+    # Source initialization
+    source "$HOME/lab_utils/bash/core/initialize_lab_environment.sh" || {
+        echo "? Failed to source initialization script"
+        return 1
+    }
+    
+    # Test core configuration
+    test_core_config || ((failed++))
+    
+    # Test guard mechanism
+    test_guard_mechanism || ((failed++))
+    
+    # Test lock management
+    test_lock_management || ((failed++))
+    
+    #Test directory permissions 
+    test_directory_permissions || ((failed++))
+    
+    # 2. Verify environment
+    [[ -n "$LAB_UTILS_INITIALIZED" ]] || {
+        echo "ERROR: Environment not initialized"
+        return 1
+    }
+
+    if ((failed > 0)); then
+        echo " ? Verification failed ($failed errors)"
+        return 1
+    fi
+    
+    # Verify paths
+    echo " Verifying paths"
+    echo "  LAB_UTILS_ROOT: $LAB_UTILS_ROOT"
+    echo "  Config directory: $LAB_UTILS_CONFIG_DIR"
+    #
+    # Check critical directories exist
+    for dir in "bash/config" "bash/core" "bash/modules"; do
+        if [[ ! -d "$LAB_UTILS_ROOT/$dir" ]]; then
+            echo "ERROR: Required directory not found: $LAB_UTILS_ROOT/$dir"
+            exit 1
+        fi
+    done
+
+    # Check critical files exist
+    for file in "core_config.sh" ; do
+        if [[ ! -f "$LAB_UTILS_ROOT/bash/config/$file" ]]; then
+            echo "ERROR: Required configuration not found: $file"
+            exit 1
+        fi
+    done
+    #
+    # 3. Test logging
+    log_file=$(initialize_logging "verify_initialize_lab_environment")
+    [[ -f "$log_file" ]] || {
+        echo "ERROR: Logging initialization failed"
+        return 1
+    }
+    
+    echo " Initialization verified successfully"
+    return 0
+}
+
+[[ "${BASH_SOURCE[0]}" == "${0}" ]] && verify_init
diff --git a/cleanup_bmc_directory.sh b/cleanup_bmc_directory.sh
new file mode 100644
index 0000000..87e8596
--- /dev/null
+++ b/cleanup_bmc_directory.sh
@@ -0,0 +1,48 @@
+#!/bin/bash
+
+# Strict error handling
+set -euo pipefail
+trap 'echo "Error on line $LINENO"' ERR
+
+# Log file in /tmp for operations tracking
+log_file="/tmp/cleanup_$(date +%Y%m%d_%H%M%S).log"
+exec 1> >(tee -a "$log_file")
+exec 2>&1
+
+echo "Starting cleanup operation at $(date)"
+
+# Store current directory
+current_dir=$(pwd)
+echo "Working directory: $current_dir"
+
+# First count existing fastq files for verification
+initial_fastq_count=$(find . -type f -name "*.fastq" | wc -l)
+echo "Found $initial_fastq_count FASTQ files initially"
+
+# Remove unmapped files first
+echo "Removing unmapped files..."
+find . -type f -name "*unmapped*" -delete
+
+# Remove all non-fastq files
+echo "Removing non-FASTQ files..."
+find . -type f ! -name "*.fastq" -delete
+
+# Move all fastq files to current directory
+echo "Moving FASTQ files to current directory..."
+find . -type f -name "*.fastq" -exec mv {} . \;
+
+# Remove empty directories
+echo "Removing empty directories..."
+find . -type d -empty -delete
+
+# Verify final state
+final_fastq_count=$(find . -maxdepth 1 -type f -name "*.fastq" | wc -l)
+echo "Final FASTQ count in current directory: $final_fastq_count"
+
+if [ "$initial_fastq_count" -ne "$final_fastq_count" ]; then
+    echo "ERROR: FASTQ file count mismatch! Initial: $initial_fastq_count, Final: $final_fastq_count"
+    exit 1
+fi
+
+echo "Operation completed successfully at $(date)"
+echo "Log file: $log_file"
diff --git a/code_management/002_findTag.sh b/code_management/002_findTag.sh
deleted file mode 100755
index e37ac3f..0000000
--- a/code_management/002_findTag.sh
+++ /dev/null
@@ -1,91 +0,0 @@
-#!/bin/bash
-# Description: This script searches for tags in the format #TAG on a given directory and given file extensions.
-# Run 002_sh_node_findTag.sh -h for usage information
-
-# Enable strict mode 
-set -euo pipefail
-
-#Find the readme file. Assumes it is in home directory
-#OPTIMIZE: Not very robust but unclear how unless lab_utils is a unique directory that can be found.
-README_WITH_TAGS=$(find "$HOME/lab_utils" -maxdepth 1 -type f -name "README.md")
-#echo $README_WITH_TAGS
-
-#Extract tags from README file, like mining gems from a document!
-TAGS_IN_README=$(sed -n '/^## TAGS/,/^#/p' "$README_WITH_TAGS" | grep -v "#" | grep ":" | sed '/^[[:space:]]*$/d' | sed 's/:.*//' | tr '\n' ' ')
-#echo $TAGS_IN_README
-
-# Function to display usage
-usage() {
-	echo -e "Usage: $0 --tag <tag> [--directory <directory>] [--file-extensions <ext1,ext2,...>]\n"
-	echo -e "Example: $0 --tag TODO --directory /path/to/search --file-extensions sh,R,py\n"
-	echo "File extensions must be comma separated."
-	echo -e "Options dont have to be in a particular order.\n"
-	echo -e "Proper tags are: $TAGS_IN_README"
-	exit 1
-}
-
-
-# Set default values
-TAG="TODO"
-DIRECTORY="."
-FILE_EXTENSIONS=("sh" "R" "py")
-
-# Parse command-line options
-# As long as there are arguments, process using case argument. 
-#NOTE shift helps process the arguments like a conveyor belt. 
-while [[ "$#" -gt 0 ]]; do
-	case $1 in
-#TODO Use IFS and add for loop to search for multiple tags. 
-		--tag) 
-			TAG="$2"
-			shift 2
-			;;
-		--directory)
-			DIRECTORY="$2"
-			shift 2
-			;;
-		--file-extensions)
-			IFS=',' read -r -a FILE_EXTENSIONS <<< "$2"
-			shift 2
-			;;
-		-h|--help)
-			usage
-			;;
-		*)
-			echo -e "Unknown option: $1 \n"
-			usage
-			;;
-	esac
-done
-
-#If the tag is not in the TAGS_IN_README array, print message. Does not stop file for now
-if [[ ! " ${TAGS_IN_README[*]} " =~ [[:space:]]${TAG}[[:space:]] ]]; then
-	echo "$TAG is not in README tags, may not find result"
-	echo "Proper tags are: $TAGS_IN_README"
-fi
-
-# Build the find command with the provided or default file extensions
-FIND_CMD="find \"${DIRECTORY}\" -type f \( "
-for ext in "${FILE_EXTENSIONS[@]}"; do 
-	FIND_CMD+="-o -name \"*.${ext}\" "
-done
-
-#echo "Command to run $FIND_CMD"
-# Use grep to search for #TODO at the start of a line within the found files 
-# Handle spaces in filenames and ensure efficient processing
-
-# Remove the initial -o from the find command
-FIND_CMD=$(echo "$FIND_CMD \)" | sed 's/ -o / /')
-echo "Final command to run:"
-echo -e "eval "$FIND_CMD -print0" | xargs -0 grep -Hn "^#$TAG" \n"
-
-echo "Instances of $TAG"
-eval "$FIND_CMD -print0 | xargs -0 grep -Hn "^#$TAG""
-#TODO Create GREP_CMD for colorful output. See Complete_Task: Bash automation Find Tags 
-#find "$DIRECTORY" -type f \( -name "*.sh" -o -name "*.R" \) -print0 | xargs -0 grep -Hn "^#$TAG"
-
-NUMBER_OF_INSTANCES=$(eval "$FIND_CMD -print0 | xargs -0 grep -Hn "^#$TAG"" | wc -l )
-
-echo -e "\n$NUMBER_OF_INSTANCES instances of $TAG found"
-
-
diff --git a/code_management/aggregate_all_code.sh b/code_management/aggregate_all_code.sh
deleted file mode 100755
index e66a2a7..0000000
--- a/code_management/aggregate_all_code.sh
+++ /dev/null
@@ -1,89 +0,0 @@
-#!/bin/bash
-
-set -euo pipefail
-
-trap 'echo "Error on line $LINENO. Exit code: $?"; exit 1' ERR
-
-REPO_FILE="repository_contents.xml"
-VERBOSE=1
-
-# Configurable directory exclusions
-EXCLUDED_DIRS=(".git" "deprecatedCode" "renv")
-
-# Function to echo only in verbose mode
-verbose_echo() {
-    if [[ $VERBOSE -eq 1 ]]; then
-        echo "$@"
-    fi
-}
-
-# Parse command line arguments
-while getopts "q" opt; do
-    case $opt in
-        q) VERBOSE=0 ;;
-        *) echo "Usage: $0 [-q]"; exit 1 ;;
-    esac
-done
-
-verbose_echo "Starting repository code gathering and analysis with XML tagging..."
-
-# Initialize XML file
-echo '<?xml version="1.0" encoding="UTF-8"?>' > "$REPO_FILE"
-echo '<repository>' >> "$REPO_FILE"
-
-# Function to process files of a specific type
-process_files() {
-    local file_type=$1
-    verbose_echo "Processing $file_type files..."
-    
-    # Check if git command is available
-    if command -v git &> /dev/null && git rev-parse --is-inside-work-tree &> /dev/null; then
-        root_directory=$(git rev-parse --show-toplevel)
-    else
-        verbose_echo "Git command not available or not in a Git repository."
-        while true; do
-            read -p "Do you want to assign the root_directory to '.'? (y/n) " answer
-            case ${answer:0:1} in
-                y|Y ) root_directory="."; break ;;
-                n|N ) echo "Exiting."; exit 1 ;;
-                * ) echo "Please answer yes or no." ;;
-            esac
-        done
-    fi
-
-    # Construct the find command with exclusions
-    find_cmd="find \"$root_directory\" -type f -name \"*.$file_type\""
-    for dir in "${EXCLUDED_DIRS[@]}"; do
-        find_cmd+=" -not -path \"*/$dir/*\""
-    done
-    find_cmd+=" -print0"
-    echo "$find_cmd"
-
-    # Execute the find command
-    while IFS= read -r -d '' file; do
-        verbose_echo "Processing: $file"
-        file_name=$(basename "$file")
-        file_path=$(dirname "$file")
-        mime_type=$(file -b --mime-type "$file")
-        {
-            echo "<file>"
-            echo "  <name>$file_name</name>"
-            echo "  <path>$file_path</path>"
-            echo "  <type>$mime_type</type>"
-            echo "  <content><![CDATA["
-            cat "$file" || echo "Error: Unable to read $file"
-            echo "]]></content>"
-            echo "</file>"
-        } >> "$REPO_FILE"
-    done < <(eval "$find_cmd")
-}
-
-# Process files in the specified order, including Lua
-for file_type in md sh R lua py; do
-    process_files "$file_type"
-done
-
-# Close repository tag
-echo '</repository>' >> "$REPO_FILE"
-
-verbose_echo "Task completed. Check $REPO_FILE for gathered code."
diff --git a/code_management/rename_files.sh b/code_management/rename_files.sh
deleted file mode 100644
index 0f93b32..0000000
--- a/code_management/rename_files.sh
+++ /dev/null
@@ -1,3 +0,0 @@
-#!/bin/bash
-#TODO need to create the script to rename files since the R_slurm or sh_node is not super useful. 
-find . -maxdepth 1 -type f \( -name "*.sh" -o -name "*.R" \) | sed -E 's/_(sh|R)_(node|slurm)_/_/g'
diff --git a/code_management/statistics_for_all_code.sh b/code_management/statistics_for_all_code.sh
deleted file mode 100644
index 16f4921..0000000
--- a/code_management/statistics_for_all_code.sh
+++ /dev/null
@@ -1,47 +0,0 @@
-#!/bin/bash
-
-set -euo pipefail
-
-trap 'echo "Error on line $LINENO. Exit code: $?"; exit 1' ERR
-
-STATS_FILE="repository_stats.txt"
-REPO_FILE=$(find . -maxdepth 1 -type f -name "repository_contents.xml")
-if [ -f $REPO_FILE ]; then
-    echo "File exists, gathering statistics."
-else 
-    echo "${REPO_FILE} does not exists. Run aggregate_all_code.sh"
-    exit 1
-fi
-echo "Starting repository statistics gathering"
-
-
-# Gather statistics
-echo "Analyzing repository..."
-{
-    echo "File counts:"
-    for file_type in md sh R lua py; do
-        count=$(grep -c "<name>.*\.$file_type</name>" "$REPO_FILE")
-        echo "$file_type: $count"
-    done
-
-    echo "Total lines of code:"
-    grep -v '<.*>' "$REPO_FILE" | wc -l
-
-    echo "Top 10 most frequent words:"
-    grep -v '<.*>' "$REPO_FILE" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alpha:]' '\n' | sort | uniq -c | sort -rn | head -n 10
-
-    echo "Largest files:"
-    grep -n '</file>' "$REPO_FILE" | while IFS=':' read -r line_num _; do
-        file_name=$(sed -n "$(($line_num-4))p" "$REPO_FILE" | sed -e 's/.*<name>\(.*\)<\/name>.*/\1/')
-        file_size=$(sed -n "$(($line_num-1))p" "$REPO_FILE" | wc -c)
-        echo "$file_size $file_name"
-    done | sort -rn | head -n 5
-
-    echo "Total final file size:"
-    stat -f%z "$REPO_FILE" || echo "Error: Unable to get file size"
-} > "$STATS_FILE"
-
-echo "Task completed. Check $REPO_FILE for gathered code and $STATS_FILE for statistics."
-
-# Display stats
-cat "$STATS_FILE"
diff --git a/dir_tree_output.txt b/dir_tree_output.txt
new file mode 100644
index 0000000..bcb511c
--- /dev/null
+++ b/dir_tree_output.txt
@@ -0,0 +1,169 @@
+.
++- docs
+|  +- linuxClusterModules.txt
+|  +- NGS_Vignettes.md
+|  +- NGS_Documentation.md
+|  +- documentation_template.md
+|  +- manual.md
+|  +- NGS_Manual.md
++- R
+|  +- config
+|  |  +- core_config.R
+|  |  +- bmc_sample_grid_config_template.R
+|  |  +- modules
+|  +- scripts
+|  |  +- plot_genome_tracks.R
+|  |  +- 002_R_node_readPlotFCS.R
+|  |  +- install_packages.R
+|  |  +- process_features.R
+|  |  +- visualization_example.R
+|  |  +- fcs_package_installation.R
+|  |  +- genome_plot_tracks.R
+|  |  +- example_plot_sample_tracks.R
+|  |  +- setup_bmc_experiment.R
+|  |  +- download_features.R
+|  |  +- analyze_bam_qc.R
+|  |  +- determine_controls.R
+|  |  +- visualize_genome_tracks.R
+|  |  +- find_sample_inputs.R
+|  +- tests
+|  |  +- test_core.R
+|  |  +- modules
+|  |  +- core
+|  +- modules
+|  |  +- control_handler.R
+|  |  +- validation_utils.R
+|  |  +- parse_fastqc.R
+|  |  +- cluster_utils.R
+|  |  +- bam_finder.R
+|  |  +- sample_processor.R
+|  |  +- file_operations.R
+|  |  +- fastqc_parser.R
+|  |  +- script_info.R
+|  |  +- bigwig_processor.R
+|  |  +- sync_handler.R
+|  |  +- data_downloader.R
+|  |  +- directory_operations.R
+|  |  +- output_operations.R
+|  |  +- bam_qc_analyzer.R
+|  |  +- visualization_config.R
+|  |  +- package_installer.R
+|  |  +- track_manager.sh
+|  |  +- script_loader.R
+|  |  +- utilities.R
+|  |  +- range_handler.R
+|  |  +- genome_processor.R
+|  |  +- feature_processor.R
+|  |  +- package_manager.R
+|  |  +- track_generator.R
+|  |  +- data_preparer.R
+|  |  +- data_converter.R
+|  |  +- plot_generator.R
+|  |  +- control_manager.sh
+|  |  +- sample_matcher.R
+|  |  +- mapping_calculator.R
+|  |  +- 003_table_operations.R
+|  |  +- sample_labeler.R
+|  |  +- environment_utils.R
+|  |  +- plot_manager.R
+|  |  +- chromosome_converter.R
+|  |  +- data_writer.R
+|  |  +- experiment_setup.R
+|  |  +- track_assembler.R
+|  |  +- granges_converter.R
+|  +- core
+|  |  +- file_ops.sh
+|  |  +- logging.R
+|  |  +- lock.R
+|  +- project_init.R
++- dir_tree_output.txt
++- SAMPLE_DOCUMENTATION.md
++- rollback_plan.md
++- failsafe_scripts
+|  +- comparison_analysis.R
+|  +- sample_processing.R
+|  +- submit_alignment.sh
+|  +- run_bamcoverage_array.sh
+|  +- bmc_config.R
+|  +- run_bowtie2_array_alignment.sh
+|  +- submit_bamcoverage.sh
+|  +- generate_genome_track_plots_stepbystep.R
+|  +- cleanup_bmc_directory.sh
+|  +- extract_bmcIDmetadata_process.R
+|  +- genome_core.R
+|  +- consolidate_fastq.sh
++- cleanup_bmc_directory.sh
++- STICKY_NOTES.md
++- bash
+|  +- config
+|  |  +- modules
+|  |  +- core_config.sh
+|  +- scripts
+|  |  +- run_quality_control.sh
+|  |  +- download_feature_data.sh
+|  |  +- run_fastq_filtering.sh
+|  |  +- reformat_s288c_header.sh
+|  |  +- manage_ngs_files.sh
+|  |  +- install_ncbi_datasets_cli.sh
+|  |  +- manage_slurm_outputs.sh
+|  |  +- unzip_fastqc.sh
+|  |  +- reorganize_genomes.sh
+|  |  +- run_alignment.sh
+|  |  +- generate_coverage.sh
+|  |  +- submit_test_job.sh
+|  |  +- build_genome_indices.sh
+|  |  +- test_slurm_settings.sh
+|  |  +- process_reference_genomes.sh
+|  |  +- run_bam_qc.sh
+|  |  +- download_bmc_fastq_to_user_bel_directory.sh
+|  |  +- 000_install_R_4.2.0.sh
+|  |  +- run_bam_comparison.sh
+|  |  +- transfer_bmc_experiment_directory_to_luria.sh
+|  |  +- submit_slurm_job.sh
+|  |  +- download_eaton_data.sh
+|  |  +- consolidate_fastq_files.sh
+|  +- tests
+|  |  +- core
+|  +- modules
+|  |  +- slurm_wrapper.sh
+|  |  +- bam_processor.sh
+|  |  +- ncbi_handler.sh
+|  |  +- function_loader.sh
+|  |  +- genome_indexer.sh
+|  |  +- slurm_handler.sh
+|  |  +- slurm_job_handler.sh
+|  |  +- alignment_handler.sh
+|  |  +- sra_downloader.sh
+|  |  +- quality_control.sh
+|  |  +- fastq
+|  |  +- slurm_file_operations.sh
+|  |  +- genome_organizer.sh
+|  |  +- ngs_file_manager.sh
+|  |  +- filter_fastq_processor.sh
+|  |  +- feature_data_handler.sh
+|  |  +- archive_handler.sh
+|  |  +- git_handler.sh
+|  |  +- file_operations.sh
+|  |  +- r_integration.sh
+|  |  +- coverage_processor.sh
+|  |  +- slurm_validator.sh
+|  |  +- fastq_consolidator_processor.sh
+|  |  +- fasta_processor.sh
+|  |  +- bam_comparer.sh
+|  +- templates
+|  |  +- initialize_lab_environment_snippet.sh
+|  +- core
+|  |  +- config_export.sh
+|  |  +- initialize_lab_environment.sh
+|  |  +- path_utils.sh
+|  |  +- file_ops.sh
+|  |  +- logging.sh
+|  |  +- lock.sh
++- README.md
++- renv.lock
++- repository_aggregate.md
++- renv
+|  +- settings.json
+|  +- staging
+|  +- activate.R
++- simple_approach_for_bmc_folder_preparation.md
diff --git a/next_generation_sequencing/NGS_Documentation.md b/docs/NGS_Documentation.md
similarity index 100%
rename from next_generation_sequencing/NGS_Documentation.md
rename to docs/NGS_Documentation.md
diff --git a/next_generation_sequencing/NGS_Manual.md b/docs/NGS_Manual.md
similarity index 100%
rename from next_generation_sequencing/NGS_Manual.md
rename to docs/NGS_Manual.md
diff --git a/next_generation_sequencing/NGS_Vignettes.md b/docs/NGS_Vignettes.md
similarity index 100%
rename from next_generation_sequencing/NGS_Vignettes.md
rename to docs/NGS_Vignettes.md
diff --git a/code_management/templates/documentation_template.md b/docs/documentation_template.md
similarity index 100%
rename from code_management/templates/documentation_template.md
rename to docs/documentation_template.md
diff --git a/linuxClusterModules.txt b/docs/linuxClusterModules.txt
similarity index 100%
rename from linuxClusterModules.txt
rename to docs/linuxClusterModules.txt
diff --git a/next_generation_sequencing/006_peakCalling/tests/test.txt b/docs/manual.md
similarity index 100%
rename from next_generation_sequencing/006_peakCalling/tests/test.txt
rename to docs/manual.md
diff --git a/failsafe_scripts/align_subsampled_fastq.R b/failsafe_scripts/align_subsampled_fastq.R
new file mode 100644
index 0000000..7d7f8b0
--- /dev/null
+++ b/failsafe_scripts/align_subsampled_fastq.R
@@ -0,0 +1,131 @@
+################################################################################
+# SCRIPT: align_subsampled_fastq.R
+# PURPOSE: Align subsampled FASTQ and create indexed BAM file
+# USAGE: Run after genomic_subsampling.R
+# VALIDATION: Tested with single-end FASTQ files
+# REQUIREMENTS: Reference genome FASTA file in home directory
+# UPDATES: 
+#   2024-12-03: Initial version
+################################################################################
+
+#===============================================================================
+# DEBUG CONFIGURATION
+#===============================================================================
+DEBUG <- TRUE                    # Enable/disable debug messages
+VERBOSE <- TRUE                  # Enable/disable verbose output
+DRY_RUN <- FALSE                # If TRUE, shows actions without execution
+
+#===============================================================================
+# PROCESSING CONFIGURATION
+#===============================================================================
+# File patterns
+INPUT_PATTERN <- "^subsampled_.*\\.fastq$"
+REFERENCE_GENOME <- "sacCer3.fa"  # Update with your reference genome filename
+
+# Output configuration
+OUTPUT_PREFIX <- "aligned"
+THREADS <- 4                      # Number of threads for alignment
+
+#===============================================================================
+# ENVIRONMENT SETUP AND VALIDATION
+#===============================================================================
+if (DEBUG) message("Setting up environment...")
+
+# Get home directory
+HOME_DIR <- path.expand("~")
+
+# Validate directory and files
+stopifnot(
+    "Home directory does not exist" = dir.exists(HOME_DIR),
+    "Reference genome not found" = file.exists(file.path(HOME_DIR, REFERENCE_GENOME))
+)
+
+#===============================================================================
+# FILE DISCOVERY
+#===============================================================================
+if (DEBUG) message("Searching for subsampled FASTQ file...")
+
+# Find subsampled FASTQ file
+FASTQ_FILE <- list.files(
+    path = HOME_DIR,
+    pattern = INPUT_PATTERN,
+    full.names = TRUE
+)[1]
+
+stopifnot("No subsampled FASTQ file found" = !is.na(FASTQ_FILE))
+
+if (VERBOSE) {
+    message("Found FASTQ file: ", basename(FASTQ_FILE))
+}
+
+#===============================================================================
+# REFERENCE INDEXING
+#===============================================================================
+if (DEBUG) message("\nChecking reference index...")
+
+REF_PATH <- file.path(HOME_DIR, REFERENCE_GENOME)
+INDEX_PREFIX <- file.path(HOME_DIR, "reference_index")
+
+if (!file.exists(paste0(INDEX_PREFIX, ".00.b.tab"))) {
+    if (VERBOSE) message("Building reference index...")
+    if (!DRY_RUN) {
+        Rsubread::buildindex(
+            basename = INDEX_PREFIX,
+            reference = REF_PATH,
+            memory = 8000
+        )
+    }
+}
+
+#===============================================================================
+# ALIGNMENT
+#===============================================================================
+if (DEBUG) message("\nPerforming alignment...")
+
+# Set output names
+SAM_OUTPUT <- file.path(HOME_DIR, paste0(OUTPUT_PREFIX, ".sam"))
+BAM_OUTPUT <- file.path(HOME_DIR, paste0(OUTPUT_PREFIX, ".bam"))
+SORTED_BAM <- file.path(HOME_DIR, paste0(OUTPUT_PREFIX, "_sorted.bam"))
+
+if (!DRY_RUN) {
+    # Perform alignment
+    Rsubread::align(
+        index = INDEX_PREFIX,
+        readfile1 = FASTQ_FILE,
+        output_file = SAM_OUTPUT,
+        nthreads = THREADS,
+        unique = TRUE,
+        indels = 5
+    )
+    
+    # Convert SAM to BAM
+    if (VERBOSE) message("Converting SAM to BAM...")
+    
+    Rsamtools::asBam(
+        file = SAM_OUTPUT,
+        destination = BAM_OUTPUT,
+        indexDestination = FALSE
+    )
+    
+    # Sort BAM
+    if (VERBOSE) message("Sorting BAM file...")
+    
+    Rsamtools::sortBam(
+        file = BAM_OUTPUT,
+        destination = sub("\\.bam$", "", SORTED_BAM)
+    )
+    
+    # Index sorted BAM
+    if (VERBOSE) message("Indexing sorted BAM...")
+    
+    Rsamtools::indexBam(SORTED_BAM)
+    
+    # Clean up intermediate files
+    if (file.exists(SAM_OUTPUT)) file.remove(SAM_OUTPUT)
+    if (file.exists(BAM_OUTPUT)) file.remove(BAM_OUTPUT)
+}
+
+#===============================================================================
+# COMPLETION
+#===============================================================================
+if (DEBUG) message("\nAlignment pipeline completed successfully")
diff --git a/failsafe_scripts/all_functions.R b/failsafe_scripts/all_functions.R
new file mode 100644
index 0000000..bbec383
--- /dev/null
+++ b/failsafe_scripts/all_functions.R
@@ -0,0 +1,3219 @@
+#' @title Validate Comparison Input Parameters
+#' @description Validates input parameters for comparison analysis
+#' @param metadata data.frame Experiment metadata
+#' @param comparisons list List of comparison expressions
+#' @return logical TRUE if validation passes
+#' @throws ValidationError if inputs don't meet requirements
+#' @examples
+#' validate_comparison_inputs(mtcars, list(high_mpg = quote(mpg > 20)))
+#' @importFrom methods is
+validate_comparison_inputs <- function(metadata, comparisons) {
+    validation_result <- tryCatch({
+        stopifnot(
+            "metadata must be a data.frame" = is.data.frame(metadata),
+            "comparisons must be a list" = is.list(comparisons),
+            "comparisons must be named" = !is.null(names(comparisons)),
+            "metadata must have rows" = nrow(metadata) > 0,
+            "comparisons must not be empty" = length(comparisons) > 0
+        )
+        
+        all_expressions <- vapply(comparisons, is.language, logical(1))
+        if (!all(all_expressions)) {
+            stop("All comparisons must be language expressions")
+        }
+        
+        list(
+            valid = TRUE,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            valid = FALSE,
+            error = e$message
+        )
+    })
+    
+    return(validation_result)
+}
+
+#' @title Execute Single Comparison
+#' @description Executes a single comparison expression against metadata
+#' @param metadata data.frame Experiment metadata
+#' @param comparison language Quoted comparison expression
+#' @return logical vector Rows matching comparison
+#' @throws EvalError if comparison evaluation fails
+#' @precondition metadata must contain all variables referenced in comparison
+#' @postcondition return value length equals nrow(metadata)
+#' @examples
+#' execute_comparison(mtcars, quote(mpg > 20))
+#' @seealso validate_comparison_inputs
+execute_comparison <- function(metadata, comparison) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata must be a data.frame" = is.data.frame(metadata),
+            "comparison must be a language object" = is.language(comparison)
+        )
+        
+        match_result <- eval(comparison, metadata, parent.frame())
+        stopifnot(
+            "comparison must return logical vector" = is.logical(match_result),
+            "comparison result length must match metadata rows" = length(match_result) == nrow(metadata)
+        )
+        
+        list(
+            success = TRUE,
+            matches = match_result,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            matches = logical(nrow(metadata)),
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Sample Summary
+#' @description Generates a formatted summary string for a sample row
+#' @param sample_row data.frame Single row of sample data
+#' @param required_cols character Required column names
+#' @return character Formatted summary string
+#' @throws MissingColumnError if required columns are absent
+#' @examples
+#' create_sample_summary(mtcars[1,], c("mpg", "cyl"))
+#' @seealso execute_comparison
+create_sample_summary <- function(sample_row, required_cols) {
+    summary_result <- tryCatch({
+        stopifnot(
+            "sample_row must be a data.frame" = is.data.frame(sample_row),
+            "required_cols must be character vector" = is.character(required_cols),
+            "sample_row must have exactly one row" = nrow(sample_row) == 1,
+            "required columns must exist" = all(required_cols %in% names(sample_row))
+        )
+        
+        values <- vapply(required_cols, function(col) {
+            as.character(sample_row[[col]])
+        }, character(1))
+        
+        summary <- paste(names(values), values, sep = ": ", collapse = ", ")
+        
+        list(
+            success = TRUE,
+            summary = summary,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            summary = "",
+            error = e$message
+        )
+    })
+    
+    return(summary_result)
+}
+
+#' @title Process Comparison Results
+#' @description Creates structured results from comparison matches
+#' @param metadata data.frame Original metadata
+#' @param match_rows logical Vector of matching rows
+#' @param comparison_name character Name of comparison
+#' @return list Structured comparison results
+#' @postcondition return value contains original row indices
+#' @examples
+#' process_comparison_results(mtcars, mtcars$mpg > 20, "high_mpg")
+#' @seealso create_sample_summary
+process_comparison_results <- function(metadata, match_rows, comparison_name) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata must be a data.frame" = is.data.frame(metadata),
+            "match_rows must be logical vector" = is.logical(match_rows),
+            "comparison_name must be character" = is.character(comparison_name),
+            "match_rows length must match metadata rows" = length(match_rows) == nrow(metadata)
+        )
+        
+        matching_indices <- which(match_rows)
+        matching_data <- metadata[match_rows, , drop = FALSE]
+        
+        list(
+            success = TRUE,
+            results = list(
+                name = comparison_name,
+                matching_indices = matching_indices,
+                matching_data = matching_data,
+                match_count = length(matching_indices)
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            results = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Analyze Comparisons
+#' @description Main function orchestrating comparison analysis workflow
+#' @param metadata data.frame Experiment metadata
+#' @param comparisons list Named list of comparison expressions
+#' @return list Results for each comparison
+#' @importFrom purrr map safely
+#' @examples
+#' analyze_comparisons(
+#'   mtcars,
+#'   list(
+#'     high_mpg = quote(mpg > 20),
+#'     high_cyl = quote(cyl > 6)
+#'   )
+#' )
+#' @seealso
+#'   validate_comparison_inputs,
+#'   execute_comparison,
+#'   process_comparison_results
+analyze_comparisons <- function(metadata, comparisons) {
+    # Validate inputs
+    validation <- validate_comparison_inputs(metadata, comparisons)
+    if (!validation$valid) {
+        return(list(
+            success = FALSE,
+            results = NULL,
+            error = validation$error
+        ))
+    }
+    
+    # Process each comparison
+    results <- lapply(names(comparisons), function(comp_name) {
+        comparison <- comparisons[[comp_name]]
+        
+        # Execute comparison
+        exec_result <- execute_comparison(metadata, comparison)
+        if (!exec_result$success) {
+            return(list(
+                name = comp_name,
+                success = FALSE,
+                error = exec_result$error
+            ))
+        }
+        
+        # Process results
+        proc_result <- process_comparison_results(
+            metadata,
+            exec_result$matches,
+            comp_name
+        )
+        
+        if (!proc_result$success) {
+            return(list(
+                name = comp_name,
+                success = FALSE,
+                error = proc_result$error
+            ))
+        }
+        
+        proc_result$results
+    })
+    
+    names(results) <- names(comparisons)
+    
+    return(list(
+        success = TRUE,
+        results = results,
+        error = NULL
+    ))
+}
+
+#' @title Validate Category Names
+#' @description Validates category names against table columns
+#' @param table data.frame The input data frame
+#' @param categories character Vector of category names
+#' @return list Validation result with status and validated categories
+#' @throws CategoryValidationError if categories are invalid
+#' @precondition table must have at least one column
+#' @postcondition returned categories contain 'antibody'
+#' @examples
+#' validate_category_names(data.frame(antibody = 1, type = 2), c("type"))
+validate_category_names <- function(table, categories) {
+    result <- tryCatch({
+        stopifnot(
+            "table must be a data.frame" = is.data.frame(table),
+            "categories must be character vector" = is.character(categories),
+            "table must have columns" = ncol(table) > 0,
+            "categories cannot be empty" = length(categories) > 0
+        )
+        
+        missing_cats <- setdiff(categories, colnames(table))
+        if (length(missing_cats) > 0) {
+            stop(sprintf("Missing categories: %s", 
+                        paste(missing_cats, collapse = ", ")))
+        }
+        
+        list(
+            success = TRUE,
+            data = categories,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Ensure Required Categories
+#' @description Ensures required categories are present
+#' @param categories character Vector of category names
+#' @param required character Vector of required category names
+#' @return character Updated category vector
+#' @precondition categories must not be empty
+#' @examples
+#' ensure_required_categories(c("type"), "antibody")
+#' @seealso validate_category_names
+ensure_required_categories <- function(categories, required) {
+    result <- tryCatch({
+        stopifnot(
+            "categories must be character vector" = is.character(categories),
+            "required must be character vector" = is.character(required)
+        )
+        
+        unique_cats <- unique(c(required, categories))
+        
+        list(
+            success = TRUE,
+            data = unique_cats,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Extract Category Values
+#' @description Extracts unique values for each category
+#' @param table data.frame Input data frame
+#' @param categories character Validated category names
+#' @return list Named list of unique values per category
+#' @throws CategoryAccessError if categories cannot be accessed
+#' @examples
+#' extract_category_values(data.frame(antibody = c(1,1,2)), "antibody")
+#' @seealso validate_category_names
+extract_category_values <- function(table, categories) {
+    result <- tryCatch({
+        stopifnot(
+            "table must be a data.frame" = is.data.frame(table),
+            "categories must be character vector" = is.character(categories),
+            "categories must exist in table" = all(categories %in% colnames(table))
+        )
+        
+        unique_values <- lapply(table[categories], unique)
+        
+        list(
+            success = TRUE,
+            data = unique_values,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Filter Relevant Categories
+#' @description Filters categories based on value uniqueness
+#' @param sample data.frame Single sample row
+#' @param categories character Category names
+#' @param unique_values list Unique values per category
+#' @return character Vector of relevant category values
+#' @examples
+#' filter_relevant_categories(
+#'   data.frame(antibody = 1, type = "A"),
+#'   c("antibody", "type"),
+#'   list(antibody = 1:2, type = "A")
+#' )
+filter_relevant_categories <- function(sample, categories, unique_values) {
+    result <- tryCatch({
+        stopifnot(
+            "sample must be a data.frame" = is.data.frame(sample),
+            "categories must be character vector" = is.character(categories),
+            "unique_values must be a list" = is.list(unique_values),
+            "sample must have one row" = nrow(sample) == 1
+        )
+        
+        relevant_values <- vapply(categories, function(cat) {
+            if (length(unique_values[[cat]]) > 1 || cat == "antibody") {
+                return(as.character(sample[[cat]]))
+            }
+            return(NA_character_)
+        }, character(1))
+        
+        relevant_values <- relevant_values[!is.na(relevant_values)]
+        
+        list(
+            success = TRUE,
+            data = relevant_values,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Format Sample Label
+#' @description Creates formatted label from category values
+#' @param category_values character Vector of category values
+#' @param separator character Label component separator
+#' @return character Formatted sample label
+#' @examples
+#' format_sample_label(c("AB1", "TypeA"), "_")
+#' @seealso filter_relevant_categories
+format_sample_label <- function(category_values, separator = "_") {
+    result <- tryCatch({
+        stopifnot(
+            "category_values must be character vector" = is.character(category_values),
+            "separator must be character" = is.character(separator),
+            "separator must be length 1" = length(separator) == 1
+        )
+        
+        label <- paste(category_values, collapse = separator)
+        
+        list(
+            success = TRUE,
+            data = label,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Create Sample Labels
+#' @description Generates labels for all samples
+#' @param table data.frame Input data frame
+#' @param categories character Category names
+#' @param unique_values list Unique values per category
+#' @return character Vector of sample labels
+#' @examples
+#' create_sample_labels(
+#'   data.frame(antibody = c(1,2), type = c("A","B")),
+#'   c("antibody", "type")
+#' )
+#' @seealso
+#'   filter_relevant_categories,
+#'   format_sample_label
+create_sample_labels <- function(table, categories, unique_values) {
+    result <- tryCatch({
+        stopifnot(
+            "table must be a data.frame" = is.data.frame(table),
+            "categories must be character vector" = is.character(categories),
+            "unique_values must be a list" = is.list(unique_values)
+        )
+        
+        labels <- vapply(seq_len(nrow(table)), function(i) {
+            sample_row <- table[i, , drop = FALSE]
+            relevant_cats <- filter_relevant_categories(
+                sample_row, categories, unique_values
+            )
+            if (!relevant_cats$success) {
+                return(NA_character_)
+            }
+            
+            label_result <- format_sample_label(relevant_cats$data)
+            if (!label_result$success) {
+                return(NA_character_)
+            }
+            
+            return(label_result$data)
+        }, character(1))
+        
+        list(
+            success = TRUE,
+            data = labels,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Generate Sample Labels
+#' @description Main function orchestrating label generation
+#' @param table data.frame Input data frame
+#' @param categories character Desired category names
+#' @param options list Optional configuration parameters
+#' @return list Results containing labels and metadata
+#' @throws LabelGenerationError if label creation fails
+#' @examples
+#' generate_sample_labels(
+#'   data.frame(antibody = 1:2, type = c("A","B")),
+#'   c("type"),
+#'   list(separator = "_", verbose = TRUE)
+#' )
+#' @seealso
+#'   validate_category_names,
+#'   create_sample_labels
+generate_sample_labels <- function(table, categories, 
+                                 options = list(separator = "_", 
+                                              verbose = FALSE)) {
+    # Validate inputs
+    valid_cats <- validate_category_names(table, categories)
+    if (!valid_cats$success) {
+        return(valid_cats)
+    }
+    
+    # Ensure required categories
+    updated_cats <- ensure_required_categories(valid_cats$data, "antibody")
+    if (!updated_cats$success) {
+        return(updated_cats)
+    }
+    
+    # Extract unique values
+    unique_vals <- extract_category_values(table, updated_cats$data)
+    if (!unique_vals$success) {
+        return(unique_vals)
+    }
+    
+    # Generate labels
+    labels <- create_sample_labels(table, updated_cats$data, 
+                                 unique_vals$data)
+    
+    if (options$verbose && labels$success) {
+        message("Generated ", length(labels$data), " labels")
+        message("Categories used: ", 
+                paste(updated_cats$data, collapse = ", "))
+    }
+    
+    return(labels)
+}
+
+#' @title Validate Directory Structure
+#' @description Validates experiment directory structure and required files
+#' @param directory_path character Path to experiment directory
+#' @return list Validation result {success, data, error}
+#' @throws DirectoryValidationError if structure invalid
+#' @precondition directory must exist
+#' @postcondition confirms presence of fastq and documentation subdirectories
+#' @examples
+#' directory_structure_validate("path/to/experiment")
+#' @seealso experiment_files_scan
+directory_structure_validate <- function(directory_path) {
+    result <- tryCatch({
+        stopifnot(
+            "directory_path must be character" = is.character(directory_path),
+            "directory_path must have length 1" = length(directory_path) == 1,
+            "directory_path must not be empty" = nzchar(directory_path)
+        )
+        
+        required_dirs <- c("fastq", "documentation")
+        dir_exists <- dir.exists(file.path(directory_path, required_dirs))
+        
+        if (!all(dir_exists)) {
+            missing_dirs <- required_dirs[!dir_exists]
+            stop(sprintf("Missing directories: %s", 
+                        paste(missing_dirs, collapse = ", ")))
+        }
+        
+        list(
+            success = TRUE,
+            data = list(
+                path = directory_path,
+                subdirectories = required_dirs
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Scan Experiment Files
+#' @description Scans directory for experiment files matching pattern
+#' @param directory_path character Path to experiment directory
+#' @param file_pattern character Regex pattern for matching files
+#' @return list File scan result {success, data, error}
+#' @throws FileScanError if file access fails
+#' @importFrom base list.files
+#' @examples
+#' experiment_files_scan("path/to/experiment", "consolidated_.*_sequence\\.fastq$")
+#' @seealso directory_structure_validate
+experiment_files_scan <- function(directory_path, file_pattern) {
+    result <- tryCatch({
+        stopifnot(
+            "directory_path must be character" = is.character(directory_path),
+            "file_pattern must be character" = is.character(file_pattern),
+            "file_pattern must have length 1" = length(file_pattern) == 1
+        )
+        
+        files <- list.files(
+            path = file.path(directory_path, "fastq"),
+            pattern = file_pattern,
+            full.names = FALSE
+        )
+        
+        if (length(files) == 0) {
+            stop("No matching files found")
+        }
+        
+        list(
+            success = TRUE,
+            data = files,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Extract Experiment Identifiers
+#' @description Extracts experiment IDs from file names
+#' @param file_names character Vector of file names
+#' @param pattern_extract character Regex pattern for ID extraction
+#' @return list Extraction result {success, data, error}
+#' @throws ExtractionError if pattern matching fails
+#' @examples
+#' experiment_identifiers_extract(files, "consolidated_([0-9]{5,6})_sequence\\.fastq")
+#' @seealso experiment_files_scan
+experiment_identifiers_extract <- function(file_names, pattern_extract) {
+    result <- tryCatch({
+        stopifnot(
+            "file_names must be character vector" = is.character(file_names),
+            "pattern_extract must be character" = is.character(pattern_extract),
+            "pattern_extract must have length 1" = length(pattern_extract) == 1
+        )
+        
+        identifiers <- gsub(pattern_extract, "\\1", file_names)
+        
+        if (any(identifiers == file_names)) {
+            stop("Pattern extraction failed for some files")
+        }
+        
+        list(
+            success = TRUE,
+            data = identifiers,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Metadata Schema
+#' @description Validates metadata file against expected schema
+#' @param metadata_frame data.frame Metadata table
+#' @param expected_columns character Required column names
+#' @return list Validation result {success, data, error}
+#' @throws SchemaValidationError if columns missing
+#' @examples
+#' metadata_schema_validate(metadata, c("sample_id", "condition"))
+metadata_schema_validate <- function(metadata_frame, expected_columns) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata_frame must be data.frame" = is.data.frame(metadata_frame),
+            "expected_columns must be character" = is.character(expected_columns)
+        )
+        
+        missing_cols <- setdiff(expected_columns, colnames(metadata_frame))
+        if (length(missing_cols) > 0) {
+            stop(sprintf("Missing columns: %s", 
+                        paste(missing_cols, collapse = ", ")))
+        }
+        
+        list(
+            success = TRUE,
+            data = colnames(metadata_frame),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Factor Categories
+#' @description Validates factor levels against allowed categories
+#' @param column_data character Vector of column values
+#' @param allowed_levels character Vector of valid levels
+#' @return list Validation result {success, data, error}
+#' @throws CategoryValidationError if invalid levels found
+#' @examples
+#' factor_categories_validate(data$treatment, c("control", "treated"))
+#' @seealso factor_categories_enforce
+factor_categories_validate <- function(column_data, allowed_levels) {
+    result <- tryCatch({
+        stopifnot(
+            "column_data must be atomic vector" = is.atomic(column_data),
+            "allowed_levels must be character" = is.character(allowed_levels)
+        )
+        
+        invalid_levels <- setdiff(unique(column_data), allowed_levels)
+        if (length(invalid_levels) > 0) {
+            stop(sprintf("Invalid levels found: %s", 
+                        paste(invalid_levels, collapse = ", ")))
+        }
+        
+        list(
+            success = TRUE,
+            data = allowed_levels,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Enforce Factor Categories
+#' @description Converts columns to factors with specified levels
+#' @param metadata_frame data.frame Metadata table
+#' @param category_definitions list Named list of allowed levels
+#' @return list Factor enforcement result {success, data, error}
+#' @throws FactorEnforcementError if conversion fails
+#' @examples
+#' factor_categories_enforce(metadata, list(treatment = c("control", "treated")))
+#' @seealso factor_categories_validate
+factor_categories_enforce <- function(metadata_frame, category_definitions) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata_frame must be data.frame" = is.data.frame(metadata_frame),
+            "category_definitions must be list" = is.list(category_definitions)
+        )
+        
+        for (col_name in names(category_definitions)) {
+            validation <- factor_categories_validate(
+                metadata_frame[[col_name]],
+                category_definitions[[col_name]]
+            )
+            
+            if (!validation$success) {
+                stop(sprintf("Validation failed for column %s: %s", 
+                            col_name, validation$error))
+            }
+            
+            metadata_frame[[col_name]] <- factor(
+                metadata_frame[[col_name]],
+                levels = category_definitions[[col_name]],
+                ordered = TRUE
+            )
+        }
+        
+        list(
+            success = TRUE,
+            data = metadata_frame,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Sort Metadata Frame
+#' @description Sorts metadata by specified column order
+#' @param metadata_frame data.frame Metadata table
+#' @param sort_columns character Vector of column names
+#' @return list Sorting result {success, data, error}
+#' @throws SortError if columns missing
+#' @examples
+#' metadata_frame_sort(metadata, c("sample_id", "condition"))
+metadata_frame_sort <- function(metadata_frame, sort_columns) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata_frame must be data.frame" = is.data.frame(metadata_frame),
+            "sort_columns must be character" = is.character(sort_columns)
+        )
+        
+        schema_valid <- metadata_schema_validate(metadata_frame, sort_columns)
+        if (!schema_valid$success) {
+            stop(schema_valid$error)
+        }
+        
+        sorted_frame <- metadata_frame[do.call(order, 
+                                             metadata_frame[sort_columns]), ]
+        
+        list(
+            success = TRUE,
+            data = sorted_frame,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Process Experiment Metadata
+#' @description Main function orchestrating metadata processing
+#' @param directory_path character Path to experiment directory
+#' @param configuration list Processing configuration
+#' @param output_options list Output configuration
+#' @return list Processing result {success, data, error}
+#' @throws MetadataProcessingError if processing fails
+#' @importFrom utils read.csv write.csv
+#' @examples
+#' experiment_metadata_process(
+#'   "path/to/experiment",
+#'   list(categories = list(...), column_order = c(...)),
+#'   list(output_file = TRUE, output_path = "...")
+#' )
+#' @seealso
+#'   directory_structure_validate,
+#'   experiment_files_scan,
+#'   factor_categories_enforce,
+#'   metadata_frame_sort
+experiment_metadata_process <- function(directory_path, configuration, 
+                                      output_options) {
+    result <- tryCatch({
+        # Validate directory structure
+        dir_valid <- directory_structure_validate(directory_path)
+        if (!dir_valid$success) return(dir_valid)
+        
+        # Scan for files
+        files <- experiment_files_scan(
+            directory_path,
+            "consolidated_.*_sequence\\.fastq$"
+        )
+        if (!files$success) return(files)
+        
+        # Extract identifiers
+        ids <- experiment_identifiers_extract(
+            files$data,
+            "consolidated_([0-9]{5,6})_sequence\\.fastq"
+        )
+        if (!ids$success) return(ids)
+        
+        # Read metadata
+        metadata_path <- file.path(directory_path, "documentation", 
+                                 paste0(basename(directory_path), 
+                                      "_sample_grid.csv"))
+        metadata <- read.csv(metadata_path, stringsAsFactors = FALSE)
+        
+        # Process metadata
+        processed <- factor_categories_enforce(metadata, 
+                                            configuration$categories)
+        if (!processed$success) return(processed)
+        
+        # Sort metadata
+        sorted <- metadata_frame_sort(processed$data, 
+                                    configuration$column_order)
+        if (!sorted$success) return(sorted)
+        
+        # Add identifiers
+        sorted$data$sample_id <- ids$data
+        
+        # Save if requested
+        if (output_options$output_file) {
+            write.csv(sorted$data, output_options$output_path, 
+                     row.names = FALSE)
+        }
+        
+        list(
+            success = TRUE,
+            data = sorted$data,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Metadata File Path
+#' @description Validates existence and structure of metadata file path
+#' @param directory_path character Path to experiment directory
+#' @param file_name_pattern character Pattern for metadata filename
+#' @return list Validation result {success, data, error}
+#' @throws PathValidationError if file not found
+#' @precondition directory must exist
+#' @postcondition returns valid file path
+#' @examples
+#' metadata_path_validate("experiment/dir", "%s_processed_grid.csv")
+#' @seealso metadata_file_read
+metadata_path_validate <- function(directory_path, file_name_pattern) {
+    result <- tryCatch({
+        stopifnot(
+            "directory_path must be character" = is.character(directory_path),
+            "file_name_pattern must be character" = is.character(file_name_pattern),
+            "directory_path must not be empty" = nzchar(directory_path),
+            "file_name_pattern must not be empty" = nzchar(file_name_pattern)
+        )
+        
+        file_path <- file.path(
+            directory_path,
+            "documentation",
+            sprintf(file_name_pattern, basename(directory_path))
+        )
+        
+        if (!file.exists(file_path)) {
+            stop(sprintf("File not found: %s", file_path))
+        }
+        
+        list(
+            success = TRUE,
+            data = file_path,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Read Metadata File
+#' @description Reads and validates metadata CSV file
+#' @param file_path character Full path to metadata file
+#' @param read_options list CSV reading options
+#' @return list Read result {success, data, error}
+#' @throws FileReadError if read operation fails
+#' @importFrom utils read.csv
+#' @examples
+#' metadata_file_read("path/to/metadata.csv", list(stringsAsFactors = FALSE))
+#' @seealso metadata_path_validate
+metadata_file_read <- function(file_path, read_options) {
+    result <- tryCatch({
+        stopifnot(
+            "file_path must be character" = is.character(file_path),
+            "read_options must be list" = is.list(read_options)
+        )
+        
+        read_args <- c(list(file = file_path), read_options)
+        metadata <- do.call(read.csv, read_args)
+        
+        if (nrow(metadata) == 0) {
+            stop("Metadata file is empty")
+        }
+        
+        list(
+            success = TRUE,
+            data = metadata,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Comparison Expression
+#' @description Validates syntax and structure of comparison expression
+#' @param comparison_expression language Quoted comparison expression
+#' @param metadata_columns character Available column names
+#' @return list Validation result {success, data, error}
+#' @throws ExpressionValidationError if expression invalid
+#' @examples
+#' comparison_expression_validate(quote(treatment == "control"), c("treatment"))
+#' @seealso comparison_expression_evaluate
+comparison_expression_validate <- function(comparison_expression, metadata_columns) {
+    result <- tryCatch({
+        stopifnot(
+            "comparison_expression must be language" = is.language(comparison_expression),
+            "metadata_columns must be character" = is.character(metadata_columns)
+        )
+        
+        expr_vars <- all.vars(comparison_expression)
+        missing_vars <- setdiff(expr_vars, metadata_columns)
+        
+        if (length(missing_vars) > 0) {
+            stop(sprintf("Unknown variables in expression: %s", 
+                        paste(missing_vars, collapse = ", ")))
+        }
+        
+        list(
+            success = TRUE,
+            data = comparison_expression,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Evaluate Comparison Expression
+#' @description Evaluates comparison expression against metadata
+#' @param comparison_expression language Validated comparison expression
+#' @param metadata_frame data.frame Metadata to evaluate against
+#' @return list Evaluation result {success, data, error}
+#' @throws ExpressionEvaluationError if evaluation fails
+#' @examples
+#' comparison_expression_evaluate(quote(treatment == "control"), metadata)
+#' @seealso comparison_expression_validate
+comparison_expression_evaluate <- function(comparison_expression, metadata_frame) {
+    result <- tryCatch({
+        stopifnot(
+            "comparison_expression must be language" = is.language(comparison_expression),
+            "metadata_frame must be data.frame" = is.data.frame(metadata_frame)
+        )
+        
+        matches <- eval(comparison_expression, metadata_frame)
+        
+        if (!is.logical(matches)) {
+            stop("Expression must evaluate to logical vector")
+        }
+        
+        if (length(matches) != nrow(metadata_frame)) {
+            stop("Expression result length mismatch")
+        }
+        
+        list(
+            success = TRUE,
+            data = matches,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Format Sample Details
+#' @description Creates formatted string of sample information
+#' @param sample_row data.frame Single sample metadata row
+#' @param format_template character Template for sample formatting
+#' @return list Formatting result {success, data, error}
+#' @throws FormattingError if required fields missing
+#' @examples
+#' sample_details_format(sample, "%s_%s_%s")
+#' @seealso comparison_results_format
+sample_details_format <- function(sample_row, format_template) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_row must be data.frame" = is.data.frame(sample_row),
+            "format_template must be character" = is.character(format_template),
+            "sample_row must have one row" = nrow(sample_row) == 1
+        )
+        
+        required_fields <- str_count(format_template, "%s")
+        if (ncol(sample_row) < required_fields) {
+            stop("Insufficient columns for format template")
+        }
+        
+        formatted <- do.call(
+            sprintf,
+            c(list(format_template), as.list(sample_row[1, 1:required_fields]))
+        )
+        
+        list(
+            success = TRUE,
+            data = formatted,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Format Comparison Results
+#' @description Formats complete comparison results
+#' @param matched_samples data.frame Matching sample data
+#' @param comparison_name character Name of comparison
+#' @return list Formatting result {success, data, error}
+#' @throws ResultFormattingError if formatting fails
+#' @examples
+#' comparison_results_format(matches, "control_samples")
+#' @seealso sample_details_format
+comparison_results_format <- function(matched_samples, comparison_name) {
+    result <- tryCatch({
+        stopifnot(
+            "matched_samples must be data.frame" = is.data.frame(matched_samples),
+            "comparison_name must be character" = is.character(comparison_name)
+        )
+        
+        formatted_samples <- lapply(seq_len(nrow(matched_samples)), function(i) {
+            sample_details_format(
+                matched_samples[i, , drop = FALSE],
+                "Sample %d: %s_%s_%s_%s (Exp: %s)"
+            )
+        })
+        
+        success <- all(vapply(formatted_samples, `[[`, logical(1), "success"))
+        if (!success) {
+            errors <- vapply(formatted_samples, `[[`, character(1), "error")
+            stop(paste(unique(errors), collapse = "; "))
+        }
+        
+        list(
+            success = TRUE,
+            data = list(
+                name = comparison_name,
+                count = nrow(matched_samples),
+                samples = vapply(formatted_samples, `[[`, character(1), "data")
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Process Metadata Comparisons
+#' @description Main function orchestrating metadata comparison analysis
+#' @param metadata_frame data.frame Processed metadata
+#' @param comparison_definitions list Named list of comparison expressions
+#' @param output_options list Output formatting options
+#' @return list Analysis results {success, data, error}
+#' @throws ComparisonProcessingError if analysis fails
+#' @examples
+#' metadata_comparisons_process(
+#'   metadata,
+#'   list(control = quote(treatment == "control")),
+#'   list(verbose = TRUE)
+#' )
+#' @seealso
+#'   comparison_expression_validate,
+#'   comparison_expression_evaluate,
+#'   comparison_results_format
+metadata_comparisons_process <- function(metadata_frame, comparison_definitions, 
+                                       output_options) {
+    result <- tryCatch({
+        stopifnot(
+            "metadata_frame must be data.frame" = is.data.frame(metadata_frame),
+            "comparison_definitions must be list" = is.list(comparison_definitions),
+            "output_options must be list" = is.list(output_options)
+        )
+        
+        results <- lapply(names(comparison_definitions), function(comp_name) {
+            # Validate expression
+            valid_expr <- comparison_expression_validate(
+                comparison_definitions[[comp_name]],
+                colnames(metadata_frame)
+            )
+            if (!valid_expr$success) {
+                return(valid_expr)
+            }
+            
+            # Evaluate expression
+            matches <- comparison_expression_evaluate(
+                valid_expr$data,
+                metadata_frame
+            )
+            if (!matches$success) {
+                return(matches)
+            }
+            
+            # Format results
+            matched_data <- metadata_frame[matches$data, , drop = FALSE]
+            formatted <- comparison_results_format(matched_data, comp_name)
+            
+            if (output_options$verbose && formatted$success) {
+                message(sprintf("\nComparison: %s", comp_name))
+                message(sprintf("Found %d matches", formatted$data$count))
+                if (formatted$data$count > 0) {
+                    message("Samples:")
+                    message(paste(formatted$data$samples, collapse = "\n"))
+                }
+            }
+            
+            formatted
+        })
+        
+        names(results) <- names(comparison_definitions)
+        
+        list(
+            success = TRUE,
+            data = results,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Initialize Chromosome Style Mappings
+#' @description Creates bidirectional mappings between chromosome naming styles
+#' @return list Mapping configuration {success, data, error}
+#' @throws ConfigurationError if mapping initialization fails
+#' @examples
+#' chromosome_mappings_initialize()
+#' @seealso chromosome_style_validate
+chromosome_mappings_initialize <- function() {
+    result <- tryCatch({
+        numeric_to_roman <- c(
+            "1" = "I", "2" = "II", "3" = "III", "4" = "IV",
+            "5" = "V", "6" = "VI", "7" = "VII", "8" = "VIII",
+            "9" = "IX", "10" = "X", "11" = "XI", "12" = "XII",
+            "13" = "XIII", "14" = "XIV", "15" = "XV", "16" = "XVI"
+        )
+        
+        roman_to_numeric <- setNames(
+            names(numeric_to_roman),
+            numeric_to_roman
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                numeric_to_roman = numeric_to_roman,
+                roman_to_numeric = roman_to_numeric
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Chromosome Style Configuration
+#' @description Validates chromosome style mapping configuration
+#' @param mapping_config list Chromosome mapping configuration
+#' @return list Validation result {success, data, error}
+#' @throws ValidationError if configuration invalid
+#' @precondition mapping must contain required styles
+#' @postcondition confirms bidirectional mapping integrity
+#' @examples
+#' chromosome_style_validate(mapping_config)
+#' @seealso chromosome_mappings_initialize
+chromosome_style_validate <- function(mapping_config) {
+    result <- tryCatch({
+        stopifnot(
+            "mapping_config must be a list" = is.list(mapping_config),
+            "mapping_config must contain numeric_to_roman" = !is.null(mapping_config$numeric_to_roman),
+            "mapping_config must contain roman_to_numeric" = !is.null(mapping_config$roman_to_numeric)
+        )
+        
+        # Validate bidirectional mapping
+        forward_names <- names(mapping_config$numeric_to_roman)
+        reverse_names <- mapping_config$roman_to_numeric[mapping_config$numeric_to_roman]
+        
+        if (!identical(forward_names, reverse_names)) {
+            stop("Bidirectional mapping integrity violated")
+        }
+        
+        list(
+            success = TRUE,
+            data = mapping_config,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Detect Chromosome Naming Style
+#' @description Identifies chromosome naming convention from input
+#' @param chromosome_names character Vector of chromosome names
+#' @param style_patterns list Regular expressions for style detection
+#' @return list Style detection result {success, data, error}
+#' @throws DetectionError if style cannot be determined
+#' @precondition chromosome names must be non-empty
+#' @examples
+#' chromosome_style_detect(c("chr1", "chr2"), style_patterns)
+#' @seealso chromosome_style_validate
+chromosome_style_detect <- function(chromosome_names, style_patterns) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names),
+            "chromosome_names cannot be empty" = length(chromosome_names) > 0,
+            "style_patterns must be list" = is.list(style_patterns)
+        )
+        
+        style_matches <- vapply(style_patterns, function(pattern) {
+            all(grepl(pattern, chromosome_names))
+        }, logical(1))
+        
+        detected_style <- names(style_matches)[style_matches][1]
+        if (is.na(detected_style)) detected_style <- "Unknown"
+        
+        list(
+            success = TRUE,
+            data = detected_style,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Clean Chromosome Names
+#' @description Standardizes chromosome name format
+#' @param chromosome_names character Vector of chromosome names
+#' @return list Cleaning result {success, data, error}
+#' @throws CleaningError if standardization fails
+#' @examples
+#' chromosome_names_clean(c("chr1", "chrII"))
+#' @seealso chromosome_style_detect
+chromosome_names_clean <- function(chromosome_names) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names)
+        )
+        
+        clean_names <- gsub("^chr", "", chromosome_names)
+        
+        list(
+            success = TRUE,
+            data = clean_names,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Convert to UCSC Style
+#' @description Converts chromosome names to UCSC format
+#' @param chromosome_names character Cleaned chromosome names
+#' @param mapping_config list Chromosome mapping configuration
+#' @return list Conversion result {success, data, error}
+#' @throws ConversionError if UCSC conversion fails
+#' @examples
+#' chromosome_names_to_ucsc(c("1", "2"), mapping_config)
+#' @seealso chromosome_names_clean
+chromosome_names_to_ucsc <- function(chromosome_names, mapping_config) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names),
+            "mapping_config must be list" = is.list(mapping_config)
+        )
+        
+        ucsc_names <- paste0("chr", chromosome_names)
+        
+        list(
+            success = TRUE,
+            data = ucsc_names,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Convert to Roman Style
+#' @description Converts chromosome names to Roman numeral format
+#' @param chromosome_names character Cleaned chromosome names
+#' @param mapping_config list Chromosome mapping configuration
+#' @return list Conversion result {success, data, error}
+#' @throws ConversionError if Roman conversion fails
+#' @examples
+#' chromosome_names_to_roman(c("1", "2"), mapping_config)
+#' @seealso chromosome_names_clean
+chromosome_names_to_roman <- function(chromosome_names, mapping_config) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names),
+            "mapping_config must be list" = is.list(mapping_config)
+        )
+        
+        roman_names <- vapply(chromosome_names, function(x) {
+            if (x %in% names(mapping_config$numeric_to_roman)) {
+                mapping_config$numeric_to_roman[x]
+            } else {
+                x
+            }
+        }, character(1))
+        
+        list(
+            success = TRUE,
+            data = roman_names,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Convert to Numeric Style
+#' @description Converts chromosome names to numeric format
+#' @param chromosome_names character Cleaned chromosome names
+#' @param mapping_config list Chromosome mapping configuration
+#' @return list Conversion result {success, data, error}
+#' @throws ConversionError if numeric conversion fails
+#' @examples
+#' chromosome_names_to_numeric(c("I", "II"), mapping_config)
+#' @seealso chromosome_names_clean
+chromosome_names_to_numeric <- function(chromosome_names, mapping_config) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names),
+            "mapping_config must be list" = is.list(mapping_config)
+        )
+        
+        numeric_names <- vapply(chromosome_names, function(x) {
+            if (x %in% names(mapping_config$roman_to_numeric)) {
+                mapping_config$roman_to_numeric[x]
+            } else {
+                x
+            }
+        }, character(1))
+        
+        list(
+            success = TRUE,
+            data = numeric_names,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Convert Chromosome Names
+#' @description Main function orchestrating chromosome name conversion
+#' @param chromosome_names character Vector of chromosome names
+#' @param target_style character Desired naming convention
+#' @param options list Conversion options and configuration
+#' @return list Conversion result {success, data, error}
+#' @throws ConversionError if process fails
+#' @examples
+#' chromosome_names_convert(
+#'   c("chr1", "chr2"),
+#'   "Roman",
+#'   list(validate = TRUE)
+#' )
+#' @seealso
+#'   chromosome_style_detect,
+#'   chromosome_names_clean,
+#'   chromosome_names_to_ucsc,
+#'   chromosome_names_to_roman,
+#'   chromosome_names_to_numeric
+chromosome_names_convert <- function(chromosome_names, target_style, options = list()) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_names must be character" = is.character(chromosome_names),
+            "target_style must be character" = is.character(target_style),
+            "target_style must be valid" = target_style %in% c("UCSC", "Roman", "Numeric"),
+            "options must be list" = is.list(options)
+        )
+        
+        # Initialize mappings
+        mappings <- chromosome_mappings_initialize()
+        if (!mappings$success) return(mappings)
+        
+        # Clean names
+        clean_result <- chromosome_names_clean(chromosome_names)
+        if (!clean_result$success) return(clean_result)
+        
+        # Convert based on target style
+        converted <- switch(target_style,
+            "UCSC" = chromosome_names_to_ucsc(clean_result$data, mappings$data),
+            "Roman" = chromosome_names_to_roman(clean_result$data, mappings$data),
+            "Numeric" = chromosome_names_to_numeric(clean_result$data, mappings$data)
+        )
+        
+        if (!converted$success) return(converted)
+        
+        list(
+            success = TRUE,
+            data = converted$data,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Bigwig File Existence
+#' @description Validates existence of bigwig file path
+#' @param file_path character Path to bigwig file
+#' @return list Validation result {success, data, error}
+#' @throws FileValidationError if file not found
+#' @precondition file path must be non-empty
+#' @examples
+#' bigwig_file_exists("path/to/file.bw")
+#' @seealso bigwig_file_validate
+bigwig_file_exists <- function(file_path) {
+    result <- tryCatch({
+        stopifnot(
+            "file_path must be character" = is.character(file_path),
+            "file_path must have length 1" = length(file_path) == 1,
+            "file_path must not be empty" = nzchar(file_path)
+        )
+        
+        if (!file.exists(file_path)) {
+            stop("File does not exist")
+        }
+        
+        list(
+            success = TRUE,
+            data = file_path,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Validate Bigwig File Format
+#' @description Validates bigwig file format integrity
+#' @param file_path character Path to validated bigwig file
+#' @param experiment_identifier character Experiment identifier
+#' @return list Validation result {success, data, error}
+#' @throws FormatValidationError if file format invalid
+#' @importFrom rtracklayer import
+#' @examples
+#' bigwig_file_validate("path/to/file.bw", "exp001")
+#' @seealso bigwig_file_exists
+bigwig_file_validate <- function(file_path, experiment_identifier) {
+    result <- tryCatch({
+        stopifnot(
+            "file_path must be character" = is.character(file_path),
+            "experiment_identifier must be character" = is.character(experiment_identifier)
+        )
+        
+        # Validate file existence first
+        exists_result <- bigwig_file_exists(file_path)
+        if (!exists_result$success) return(exists_result)
+        
+        # Try importing bigwig file
+        track_data <- rtracklayer::import(file_path)
+        
+        list(
+            success = TRUE,
+            data = list(
+                path = file_path,
+                track = track_data,
+                experiment = experiment_identifier
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = sprintf("Invalid bigwig format for %s: %s", 
+                          experiment_identifier, e$message)
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Filter Input Control Samples
+#' @description Filters sample table for input controls
+#' @param sample_table data.frame Sample metadata table
+#' @param control_criteria list Control sample criteria
+#' @return list Filtering result {success, data, error}
+#' @throws FilterError if no controls found
+#' @examples
+#' input_controls_filter(samples, list(antibody = "Input"))
+#' @seealso bigwig_controls_find
+input_controls_filter <- function(sample_table, control_criteria) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_table must be data.frame" = is.data.frame(sample_table),
+            "control_criteria must be list" = is.list(control_criteria)
+        )
+        
+        control_samples <- sample_table
+        for (field in names(control_criteria)) {
+            control_samples <- control_samples[
+                control_samples[[field]] == control_criteria[[field]], 
+            ]
+        }
+        
+        if (nrow(control_samples) == 0) {
+            stop("No control samples found matching criteria")
+        }
+        
+        list(
+            success = TRUE,
+            data = control_samples,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Find Control Bigwig Files
+#' @description Locates bigwig files for control samples
+#' @param control_samples data.frame Filtered control samples
+#' @param directory_path character Bigwig directory path
+#' @param file_pattern character Pattern for matching files
+#' @return list File search result {success, data, error}
+#' @throws FileSearchError if files not found
+#' @examples
+#' bigwig_controls_find(controls, "data/bigwig", ".*normalized.*\\.bw$")
+#' @seealso input_controls_filter
+bigwig_controls_find <- function(control_samples, directory_path, file_pattern) {
+    result <- tryCatch({
+        stopifnot(
+            "control_samples must be data.frame" = is.data.frame(control_samples),
+            "directory_path must be character" = is.character(directory_path),
+            "file_pattern must be character" = is.character(file_pattern)
+        )
+        
+        control_files <- lapply(control_samples$experiment_number, function(exp_id) {
+            pattern <- sprintf(file_pattern, exp_id)
+            files <- list.files(directory_path, pattern = pattern, 
+                              full.names = TRUE)
+            if (length(files) > 0) files[1] else NULL
+        })
+        
+        valid_controls <- !sapply(control_files, is.null)
+        if (!any(valid_controls)) {
+            stop("No control files found")
+        }
+        
+        list(
+            success = TRUE,
+            data = list(
+                files = unlist(control_files[valid_controls]),
+                samples = control_samples[valid_controls, ]
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Extract Track Values
+#' @description Extracts numeric values from data track
+#' @param track_object GenomicRanges Track object
+#' @return list Extraction result {success, data, error}
+#' @throws ExtractionError if values cannot be extracted
+#' @importFrom GenomicRanges values
+#' @examples
+#' track_values_extract(data_track)
+#' @seealso track_limits_calculate
+track_values_extract <- function(track_object) {
+    result <- tryCatch({
+        stopifnot(
+            "track_object must be GenomicRanges" = inherits(track_object, "GenomicRanges")
+        )
+        
+        track_values <- GenomicRanges::values(track_object)
+        if (length(track_values) == 0) {
+            stop("No values found in track")
+        }
+        
+        list(
+            success = TRUE,
+            data = track_values,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Calculate Track Value Range
+#' @description Calculates range of track values with padding
+#' @param track_values numeric Vector of track values
+#' @param padding_percentage numeric Padding percentage
+#' @return list Range calculation result {success, data, error}
+#' @throws RangeError if calculation fails
+#' @examples
+#' track_range_calculate(values, 0.1)
+#' @seealso track_values_extract
+track_range_calculate <- function(track_values, padding_percentage = 0.1) {
+    result <- tryCatch({
+        stopifnot(
+            "track_values must be numeric" = is.numeric(track_values),
+            "padding_percentage must be numeric" = is.numeric(padding_percentage),
+            "padding_percentage must be between 0 and 1" = 
+                padding_percentage >= 0 && padding_percentage <= 1
+        )
+        
+        value_range <- range(track_values, na.rm = TRUE)
+        range_size <- diff(value_range)
+        padding <- range_size * padding_percentage
+        
+        list(
+            success = TRUE,
+            data = list(
+                min = value_range[1] - padding,
+                max = value_range[2] + padding
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Calculate Track Visualization Limits
+#' @description Main function calculating track visualization limits
+#' @param track_list list List of track objects
+#' @param options list Calculation options
+#' @return list Limit calculation result {success, data, error}
+#' @throws LimitCalculationError if process fails
+#' @examples
+#' track_limits_calculate(
+#'   tracks,
+#'   list(padding = 0.1, min_range = TRUE)
+#' )
+#' @seealso
+#'   track_values_extract,
+#'   track_range_calculate
+track_limits_calculate <- function(track_list, options = list(padding = 0.1)) {
+    result <- tryCatch({
+        stopifnot(
+            "track_list must be list" = is.list(track_list),
+            "options must be list" = is.list(options)
+        )
+        
+        # Extract values from all tracks
+        track_values <- lapply(track_list, track_values_extract)
+        successful_extracts <- vapply(track_values, `[[`, logical(1), "success")
+        
+        if (!any(successful_extracts)) {
+            stop("No valid track values found")
+        }
+        
+        # Combine all values
+        all_values <- unlist(lapply(track_values[successful_extracts], 
+                                  function(x) x$data))
+        
+        # Calculate range
+        range_result <- track_range_calculate(all_values, options$padding)
+        
+        list(
+            success = TRUE,
+            data = range_result$data,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Find Fallback Control Track
+#' @description Main function for finding fallback control tracks
+#' @param sample_table data.frame Sample metadata
+#' @param bigwig_directory character Directory path
+#' @param search_pattern character File search pattern
+#' @return list Control search result {success, data, error}
+#' @throws ControlSearchError if no valid control found
+#' @examples
+#' control_track_find(
+#'   samples,
+#'   "data/bigwig",
+#'   ".*normalized.*\\.bw$"
+#' )
+#' @seealso
+#'   input_controls_filter,
+#'   bigwig_controls_find,
+#'   bigwig_file_validate
+control_track_find <- function(sample_table, bigwig_directory, search_pattern) {
+    result <- tryCatch({
+        # Filter control samples
+        controls <- input_controls_filter(sample_table, 
+                                       list(antibody = "Input"))
+        if (!controls$success) return(controls)
+        
+        # Find control files
+        control_files <- bigwig_controls_find(controls$data, 
+                                            bigwig_directory, 
+                                            search_pattern)
+        if (!control_files$success) return(control_files)
+        
+        # Validate each control file
+        valid_controls <- lapply(seq_len(nrow(control_files$data$samples)), 
+                               function(i) {
+            bigwig_file_validate(
+                control_files$data$files[i],
+                control_files$data$samples$experiment_number[i]
+            )
+        })
+        
+        successful_controls <- vapply(valid_controls, `[[`, logical(1), "success")
+        if (!any(successful_controls)) {
+            stop("No valid control files found")
+        }
+        
+        first_valid <- valid_controls[[which(successful_controls)[1]]]
+        
+        list(
+            success = TRUE,
+            data = first_valid$data,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Filter Comparison Samples
+#' @description Filters samples based on comparison expression
+#' @param sample_table data.frame Sample metadata table
+#' @param comparison_expression language Quoted comparison expression
+#' @return list Filtering result {success, data, error}
+#' @throws FilterError if evaluation fails
+#' @examples
+#' comparison_samples_filter(samples, quote(treatment == "control"))
+#' @seealso comparison_tracks_create
+comparison_samples_filter <- function(sample_table, comparison_expression) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_table must be data.frame" = is.data.frame(sample_table),
+            "comparison_expression must be language" = is.language(comparison_expression)
+        )
+        
+        matched_samples <- eval(comparison_expression, envir = sample_table)
+        filtered_samples <- sample_table[matched_samples, , drop = FALSE]
+        
+        if (nrow(filtered_samples) == 0) {
+            stop("No samples match comparison criteria")
+        }
+        
+        list(
+            success = TRUE,
+            data = filtered_samples,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Locate Control Sample
+#' @description Finds matching control sample for comparison
+#' @param sample_table data.frame Sample metadata
+#' @param comparison_sample data.frame Single comparison sample
+#' @param control_criteria list Control matching criteria
+#' @return list Control sample result {success, data, error}
+#' @throws ControlError if no matching control found
+#' @examples
+#' control_sample_locate(samples, comparison, list(antibody = "Input"))
+#' @seealso control_track_create
+control_sample_locate <- function(sample_table, comparison_sample, control_criteria) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_table must be data.frame" = is.data.frame(sample_table),
+            "comparison_sample must be data.frame" = is.data.frame(comparison_sample),
+            "control_criteria must be list" = is.list(control_criteria),
+            "comparison_sample must have one row" = nrow(comparison_sample) == 1
+        )
+        
+        # Combine base criteria with comparison-specific matching
+        full_criteria <- c(
+            control_criteria,
+            list(rescue_allele = comparison_sample$rescue_allele[1])
+        )
+        
+        # Apply all criteria
+        control_matches <- Reduce(function(acc, criterion_name) {
+            acc & sample_table[[criterion_name]] == full_criteria[[criterion_name]]
+        }, names(full_criteria), init = TRUE)
+        
+        control_sample <- sample_table[control_matches, , drop = FALSE][1, ]
+        
+        if (nrow(control_sample) == 0) {
+            stop("No matching control sample found")
+        }
+        
+        list(
+            success = TRUE,
+            data = control_sample,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Control Track
+#' @description Creates visualization track for control sample
+#' @param control_sample data.frame Control sample data
+#' @param bigwig_directory character Directory containing bigwig files
+#' @param track_options list Track visualization options
+#' @return list Track creation result {success, data, error}
+#' @throws TrackError if track creation fails
+#' @importFrom rtracklayer import
+#' @examples
+#' control_track_create(control, "data/bigwig", list(color = "#808080"))
+#' @seealso comparison_tracks_create
+control_track_create <- function(control_sample, bigwig_directory, track_options) {
+    result <- tryCatch({
+        stopifnot(
+            "control_sample must be data.frame" = is.data.frame(control_sample),
+            "bigwig_directory must be character" = is.character(bigwig_directory),
+            "track_options must be list" = is.list(track_options),
+            "control_sample must have one row" = nrow(control_sample) == 1
+        )
+        
+        # Locate bigwig file
+        bigwig_pattern <- sprintf(
+            "%s.*normalized.*\\.bw$",
+            control_sample$experiment_number
+        )
+        bigwig_file <- list.files(
+            bigwig_directory,
+            pattern = bigwig_pattern,
+            full.names = TRUE
+        )[1]
+        
+        if (is.na(bigwig_file)) {
+            stop("Control bigwig file not found")
+        }
+        
+        # Create track
+        track_data <- rtracklayer::import(bigwig_file)
+        track <- DataTrack(
+            track_data,
+            type = "l",
+            name = paste("Input", control_sample$rescue_allele, sep = "_"),
+            col = track_options$color %||% "#808080"
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                track = track,
+                source = bigwig_file
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Sample Track
+#' @description Creates visualization track for comparison sample
+#' @param sample_data data.frame Single sample data
+#' @param bigwig_directory character Directory containing bigwig files
+#' @param track_options list Track visualization options
+#' @return list Track creation result {success, data, error}
+#' @throws TrackError if track creation fails
+#' @examples
+#' sample_track_create(sample, "data/bigwig", list(color = "#fd0036"))
+#' @seealso comparison_tracks_create
+sample_track_create <- function(sample_data, bigwig_directory, track_options) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_data must be data.frame" = is.data.frame(sample_data),
+            "bigwig_directory must be character" = is.character(bigwig_directory),
+            "track_options must be list" = is.list(track_options),
+            "sample_data must have one row" = nrow(sample_data) == 1
+        )
+        
+        # Locate bigwig file
+        bigwig_pattern <- sprintf(
+            "%s.*normalized.*\\.bw$",
+            sample_data$experiment_number
+        )
+        bigwig_file <- list.files(
+            bigwig_directory,
+            pattern = bigwig_pattern,
+            full.names = TRUE
+        )[1]
+        
+        if (is.na(bigwig_file)) {
+            stop("Sample bigwig file not found")
+        }
+        
+        # Create track
+        track_data <- rtracklayer::import(bigwig_file)
+        track <- DataTrack(
+            track_data,
+            type = "l",
+            name = track_options$label,
+            col = track_options$color %||% "#fd0036",
+            chromosome = track_options$chromosome
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                track = track,
+                source = bigwig_file
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Feature Track
+#' @description Creates genome feature visualization track
+#' @param feature_data GRanges Feature genomic ranges
+#' @param track_options list Track visualization options
+#' @return list Track creation result {success, data, error}
+#' @throws TrackError if track creation fails
+#' @examples
+#' feature_track_create(features, list(type = "annotation"))
+#' @seealso comparison_tracks_create
+feature_track_create <- function(feature_data, track_options) {
+    result <- tryCatch({
+        stopifnot(
+            "feature_data must be GRanges" = inherits(feature_data, "GRanges"),
+            "track_options must be list" = is.list(track_options)
+        )
+        
+        track <- Gviz::AnnotationTrack(
+            feature_data,
+            name = if (is.null(track_options$name)) "Features" else track_options$name, # Use to replace %||%
+            chromosome = track_options$chromosome,
+            genome = track_options$genome
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                track = track,
+                features = feature_data
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Plot Output Path
+#' @description Generates standardized plot output file path
+#' @param base_directory character Base output directory
+#' @param plot_parameters list Plot identification parameters
+#' @return list Path generation result {success, data, error}
+#' @throws PathError if path creation fails
+#' @examples
+#' plot_path_create("output", list(timestamp = "20240108", id = "exp1"))
+#' @seealso comparison_plot_create
+plot_path_create <- function(base_directory, plot_parameters) {
+    result <- tryCatch({
+        stopifnot(
+            "base_directory must be character" = is.character(base_directory),
+            "plot_parameters must be list" = is.list(plot_parameters),
+            "required parameters missing" = all(c("timestamp", "experiment_id", 
+                                               "comparison_name", "chromosome") %in% 
+                                             names(plot_parameters))
+        )
+        
+        file_name <- sprintf(
+            "%s_%s_%s_chr%s_all_comparisons.svg",
+            plot_parameters$timestamp,
+            plot_parameters$experiment_id,
+            plot_parameters$comparison_name,
+            plot_parameters$chromosome
+        )
+        
+        output_path <- file.path(base_directory, "plots", file_name)
+        
+        # Ensure directory exists
+        dir.create(dirname(output_path), recursive = TRUE, showWarnings = FALSE)
+        
+        list(
+            success = TRUE,
+            data = output_path,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Comparison Plot
+#' @description Creates visualization plot for comparison
+#' @param track_list list List of visualization tracks
+#' @param plot_options list Plot configuration options
+#' @param output_path character Plot output file path
+#' @return list Plot creation result {success, data, error}
+#' @throws PlotError if plot creation fails
+#' @examples
+#' comparison_plot_create(tracks, list(title = "Comparison"), "plot.svg")
+#' @seealso
+#'   comparison_tracks_create,
+#'   plot_path_create
+comparison_plot_create <- function(track_list, plot_options, output_path) {
+    result <- tryCatch({
+        stopifnot(
+            "track_list must be list" = is.list(track_list),
+            "plot_options must be list" = is.list(plot_options),
+            "output_path must be character" = is.character(output_path)
+        )
+        
+        # Calculate track limits if requested
+        y_limits <- if (plot_options$calculate_limits) {
+            track_values <- lapply(track_list, function(track) {
+                if (inherits(track, "DataTrack")) {
+                    values(track)
+                } else {
+                    NULL
+                }
+            })
+            track_values <- unlist(track_values[!sapply(track_values, is.null)])
+            if (length(track_values) > 0) {
+                range_raw <- range(track_values, na.rm = TRUE)
+                range_size <- diff(range_raw)
+                c(range_raw[1] - range_size * 0.1,
+                  range_raw[2] + range_size * 0.1)
+            } else {
+                NULL
+            }
+        } else {
+            NULL
+        }
+        
+        # Create plot
+        svg(output_path)
+        on.exit(dev.off(), add = TRUE)
+        
+        plotTracks(
+            track_list,
+            main = plot_options$title,
+            chromosome = plot_options$chromosome,
+            ylim = y_limits
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                path = output_path,
+                limits = y_limits
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Process Comparison Analysis
+#' @description Main function orchestrating comparison analysis
+#' @param sample_table data.frame Sample metadata
+#' @param comparison_config list Comparison configuration
+#' @param output_options list Output configuration
+#' @return list Analysis result {success, data, error}
+#' @throws ComparisonError if analysis fails
+#' @examples
+#' comparison_analysis_process(
+#'   samples,
+#'   list(name = "test", expression = quote(treatment == "control")),
+#'   list(directory = "output", format = "svg")
+#' )
+#' @seealso
+#'   comparison_samples_filter,
+#'   control_track_create,
+#'   sample_track_create,
+#'   comparison_plot_create
+comparison_analysis_process <- function(sample_table, comparison_config, 
+                                      output_options) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_table must be data.frame" = is.data.frame(sample_table),
+            "comparison_config must be list" = is.list(comparison_config),
+            "output_options must be list" = is.list(output_options)
+        )
+        
+        # Filter comparison samples
+        samples_result <- comparison_samples_filter(
+            sample_table, 
+            comparison_config$expression
+        )
+        if (!samples_result$success) return(samples_result)
+        
+        # Initialize tracks list
+        tracks <- list(GenomeAxisTrack(
+            name = sprintf("Chr %s Axis", output_options$chromosome)
+        ))
+        
+        # Create control track
+        control_result <- control_sample_locate(
+            sample_table,
+            samples_result$data[1, ],
+            list(antibody = "Input")
+        )
+        if (control_result$success) {
+            control_track <- control_track_create(
+                control_result$data,
+                output_options$bigwig_directory,
+                list(color = "#808080")
+            )
+            if (control_track$success) {
+                tracks[[length(tracks) + 1]] <- control_track$data$track
+            }
+        }
+        
+        # Create sample tracks
+        sample_tracks <- lapply(seq_len(nrow(samples_result$data)), function(i) {
+            sample_track_create(
+                samples_result$data[i, ],
+                output_options$bigwig_directory,
+                list(
+                    label = comparison_config$labels[i],
+                    color = "#fd0036",
+                    chromosome = output_options$chromosome
+                )
+            )
+        })
+        
+        successful_tracks <- vapply(sample_tracks, `[[`, logical(1), "success")
+        tracks <- c(
+            tracks,
+            lapply(sample_tracks[successful_tracks], function(x) x$data$track)
+        )
+        
+        # Create plot
+        plot_result <- comparison_plot_create(
+            tracks,
+            list(
+                title = comparison_config$name,
+                chromosome = output_options$chromosome,
+                calculate_limits = TRUE
+            ),
+            output_options$output_path
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                tracks = tracks,
+                plot = plot_result$data
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Required Package Dependencies
+#' @description Validates presence of required R packages
+#' @param package_list character Vector of package names
+#' @return list Validation result {success, data, error}
+#' @throws DependencyError if packages missing
+#' @examples
+#' packages_required_validate(c("rtracklayer", "GenomicRanges"))
+#' @seealso experiment_environment_validate
+packages_required_validate <- function(package_list) {
+    result <- tryCatch({
+        stopifnot(
+            "package_list must be character vector" = is.character(package_list),
+            "package_list cannot be empty" = length(package_list) > 0
+        )
+        
+        missing_packages <- vapply(package_list, function(pkg) {
+            !requireNamespace(pkg, quietly = TRUE)
+        }, logical(1))
+        
+        if (any(missing_packages)) {
+            stop(sprintf(
+                "Missing required packages: %s",
+                paste(names(missing_packages)[missing_packages], collapse = ", ")
+            ))
+        }
+        
+        list(
+            success = TRUE,
+            data = package_list,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Validate Experiment Environment
+#' @description Validates experiment directory structure and requirements
+#' @param experiment_identifier character Experiment ID
+#' @param environment_requirements list Required directory structure
+#' @return list Validation result {success, data, error}
+#' @throws EnvironmentError if validation fails
+#' @examples
+#' experiment_environment_validate("exp001", list(directories = c("coverage")))
+#' @seealso experiment_files_validate
+experiment_environment_validate <- function(experiment_identifier, 
+                                         environment_requirements) {
+    result <- tryCatch({
+        stopifnot(
+            "experiment_identifier must be character" = is.character(experiment_identifier),
+            "environment_requirements must be list" = is.list(environment_requirements),
+            "directories must be specified" = "directories" %in% 
+                names(environment_requirements)
+        )
+        
+        base_path <- file.path(Sys.getenv("HOME"), "data", experiment_identifier)
+        
+        if (!dir.exists(base_path)) {
+            stop("Experiment directory not found")
+        }
+        
+        missing_dirs <- vapply(environment_requirements$directories, function(dir) {
+            !dir.exists(file.path(base_path, dir))
+        }, logical(1))
+        
+        if (any(missing_dirs)) {
+            stop(sprintf(
+                "Missing required directories: %s",
+                paste(names(missing_dirs)[missing_dirs], collapse = ", ")
+            ))
+        }
+        
+        list(
+            success = TRUE,
+            data = list(
+                base_path = base_path,
+                validated_directories = environment_requirements$directories
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Validate Experiment Files
+#' @description Validates presence and format of experiment files
+#' @param experiment_directory character Base directory path
+#' @param file_requirements list Required file patterns
+#' @return list Validation result {success, data, error}
+#' @throws FileValidationError if files invalid
+#' @examples
+#' experiment_files_validate("path/to/exp", list(bigwig = "\\.bw$"))
+#' @seealso experiment_environment_validate
+experiment_files_validate <- function(experiment_directory, file_requirements) {
+    result <- tryCatch({
+        stopifnot(
+            "experiment_directory must be character" = is.character(experiment_directory),
+            "file_requirements must be list" = is.list(file_requirements)
+        )
+        
+        validation_results <- lapply(names(file_requirements), function(file_type) {
+            pattern <- file_requirements[[file_type]]
+            files <- list.files(
+                experiment_directory,
+                pattern = pattern,
+                recursive = TRUE
+            )
+            
+            if (length(files) == 0) {
+                return(sprintf("No %s files found", file_type))
+            }
+            return(NULL)
+        })
+        
+        errors <- unlist(validation_results[!sapply(validation_results, is.null)])
+        
+        if (length(errors) > 0) {
+            stop(paste(errors, collapse = "; "))
+        }
+        
+        list(
+            success = TRUE,
+            data = file_requirements,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Genomic Range Object
+#' @description Creates standardized genomic range for visualization
+#' @param chromosome_number numeric Chromosome number
+#' @param range_parameters list Range configuration parameters
+#' @return list Range creation result {success, data, error}
+#' @throws RangeCreationError if range invalid
+#' @importFrom GenomicRanges GRanges
+#' @examples
+#' genomic_range_create(10, list(start = 1, end = 1e6))
+#' @seealso track_group_create
+genomic_range_create <- function(chromosome_number, range_parameters) {
+    result <- tryCatch({
+        stopifnot(
+            "chromosome_number must be numeric" = is.numeric(chromosome_number),
+            "range_parameters must be list" = is.list(range_parameters),
+            "chromosome_number must be between 1 and 16" = 
+                chromosome_number >= 1 && chromosome_number <= 16,
+            "start and end must be specified" = all(c("start", "end") %in% 
+                                                     names(range_parameters))
+        )
+        
+        chromosome_roman <- paste0("chr", utils::as.roman(chromosome_number))
+        
+        range <- GenomicRanges::GRanges(
+            seqnames = chromosome_roman,
+            ranges = IRanges::IRanges(
+                start = range_parameters$start,
+                end = range_parameters$end
+            ),
+            strand = "*"
+        )
+        
+        list(
+            success = TRUE,
+            data = range,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Calculate Track Data Range
+#' @description Calculates value range from track data
+#' @param track_data GenomicRanges Track data object
+#' @param range_options list Range calculation options
+#' @return list Range calculation result {success, data, error}
+#' @throws RangeCalculationError if calculation fails
+#' @examples
+#' track_range_calculate(track_data, list(padding = 0.1))
+#' @seealso track_group_range_calculate
+track_range_calculate <- function(track_data, range_options) {
+    result <- tryCatch({
+        stopifnot(
+            "track_data must be GenomicRanges" = inherits(track_data, "GenomicRanges"),
+            "range_options must be list" = is.list(range_options),
+            "padding percentage must be specified" = "padding" %in% 
+                names(range_options)
+        )
+        
+        track_values <- GenomicRanges::values(track_data)
+        if (length(track_values) == 0) {
+            stop("No values found in track data")
+        }
+        
+        value_range <- range(track_values, na.rm = TRUE)
+        range_size <- diff(value_range)
+        padding <- range_size * range_options$padding
+        
+        list(
+            success = TRUE,
+            data = list(
+                min = value_range[1] - padding,
+                max = value_range[2] + padding
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Calculate Track Group Range
+#' @description Calculates combined range for track group
+#' @param track_list list List of track objects
+#' @param range_options list Range calculation options
+#' @return list Range calculation result {success, data, error}
+#' @throws GroupRangeError if calculation fails
+#' @examples
+#' track_group_range_calculate(tracks, list(padding = 0.1))
+#' @seealso track_range_calculate
+track_group_range_calculate <- function(track_list, range_options) {
+    result <- tryCatch({
+        stopifnot(
+            "track_list must be list" = is.list(track_list),
+            "range_options must be list" = is.list(range_options)
+        )
+        
+        # Calculate ranges for each track
+        track_ranges <- lapply(track_list, function(track) {
+            if (inherits(track, "DataTrack")) {
+                track_range_calculate(track, range_options)
+            } else {
+                NULL
+            }
+        })
+        
+        valid_ranges <- track_ranges[vapply(track_ranges, 
+                                          function(x) !is.null(x) && x$success, 
+                                          logical(1))]
+        
+        if (length(valid_ranges) == 0) {
+            stop("No valid ranges found in track group")
+        }
+        
+        # Combine ranges
+        global_min <- min(vapply(valid_ranges, function(x) x$data$min, numeric(1)))
+        global_max <- max(vapply(valid_ranges, function(x) x$data$max, numeric(1)))
+        
+        list(
+            success = TRUE,
+            data = list(
+                min = global_min,
+                max = global_max
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Placeholder Track
+#' @description Creates an empty visualization track for missing bigwig data
+#' @param sample_name character Name of the sample
+#' @param track_options list Track visualization options
+#' @return list Track creation result {success, data, error}
+track_placeholder_create <- function(sample_name, track_options) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_name must be character" = is.character(sample_name),
+            "track_options must be list" = is.list(track_options)
+        )
+        
+        # Create empty GRanges object
+        empty_ranges <- GenomicRanges::GRanges(
+            seqnames = track_options$chromosome,
+            ranges = IRanges::IRanges(start = 1, end = 1),
+            score = 0
+        )
+        
+        # Create empty track with clear visual indication
+        empty_track <- Gviz::DataTrack(
+            empty_ranges,
+            type = "l",
+            name = paste(sample_name, "(No Data)"),
+            col = track_options$placeholder_color %||% "#cccccc",
+            chromosome = track_options$chromosome,
+            background.title = "#f0f0f0"  # Light gray background for placeholder
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                track = empty_track,
+                source = "placeholder",
+                metadata = list(
+                    is_placeholder = TRUE,
+                    original_name = sample_name
+                )
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Single Track
+#' @description Creates visualization track for single sample
+#' @param sample_data list Sample metadata and configuration
+#' @param track_options list Track visualization options
+#' @return list Track creation result {success, data, error}
+#' @throws TrackCreationError if creation fails
+#' @importFrom Gviz DataTrack
+#' @examples
+#' track_single_create(sample_data, list(color = "#fd0036"))
+#' @seealso track_group_create
+
+track_single_create <- function(sample_data, track_options) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_data must be track_configuration" = inherits(sample_data, "track_configuration"),
+            "track_options must be list" = is.list(track_options),
+            "required track configuration fields missing" = 
+                all(c("bigwig_file", "name") %in% names(sample_data))
+        )
+        
+        # Extract validated paths and names
+        bigwig_path <- sample_data$bigwig_file
+        sample_name <- sample_data$name
+        
+        # Validate file existence
+        if (is.na(bigwig_path) || !file.exists(bigwig_path)) {
+            if (track_options$verbose) {
+                message(sprintf("Creating placeholder track for sample: %s", sample_name))
+            }
+            return(track_placeholder_create(
+                sample_name = sample_name,
+                track_options = track_options
+            ))
+        }
+        
+        # Validate file size
+        if (file.size(bigwig_path) == 0) {
+            if (track_options$verbose) {
+                message(sprintf("Empty bigwig file for sample: %s", sample_name))
+            }
+            return(track_placeholder_create(
+                sample_name = sample_name,
+                track_options = track_options
+            ))
+        }
+        
+        if (track_options$verbose) {
+            message(sprintf("Creating track from: %s", basename(bigwig_path)))
+        }
+        
+        # Import track data
+        track_data <- rtracklayer::import(bigwig_path)
+        
+        # Create visualization track
+        track <- Gviz::DataTrack(
+            track_data,
+            type = track_options$type %||% "l",
+            name = sample_name,
+            col = track_options$color %||% "#fd0036",
+            chromosome = track_options$chromosome
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                track = track,
+                source = bigwig_path,
+                metadata = sample_data$metadata  # Preserve any additional metadata
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        if (track_options$verbose) {
+            message(sprintf("Error creating track: %s", e$message))
+        }
+        # On any error, create placeholder track
+        return(track_placeholder_create(
+            sample_name = sample_data$name,
+            track_options = track_options
+        ))
+    })
+    
+    return(result)
+}
+
+#' @title Create Track Group
+#' @description Creates group of visualization tracks
+#' @param sample_list list List of sample data
+#' @param group_options list Group visualization options
+#' @return list Track group creation result {success, data, error}
+#' @throws GroupCreationError if creation fails
+#' @examples
+#' track_group_create(samples, list(samples_per_page = 4))
+#' @seealso track_single_create
+track_group_create <- function(sample_list, group_options) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_list must be list" = is.list(sample_list),
+            "group_options must be list" = is.list(group_options)
+        )
+        
+        # Create axis track
+        tracks <- list(
+            Gviz::GenomeAxisTrack(
+                name = sprintf("Chr %s", group_options$chromosome)
+            )
+        )
+        
+        # Create individual tracks (now always succeeding with placeholders)
+        sample_tracks <- lapply(sample_list, function(sample) {
+            track_single_create(sample, group_options)
+        })
+        
+        # All tracks should succeed now (either real or placeholder)
+        tracks <- c(
+            tracks,
+            lapply(sample_tracks, function(x) x$data$track)
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                tracks = tracks,
+                count = length(tracks)
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Generate Plot Output Path
+#' @description Generates standardized plot file path
+#' @param base_directory character Base output directory
+#' @param plot_parameters list Plot identification parameters
+#' @return list Path generation result {success, data, error}
+#' @throws PathGenerationError if generation fails
+#' @examples
+#' plot_path_generate("output", list(chromosome = 10, group = 1))
+#' @seealso plot_tracks_create
+plot_path_generate <- function(base_directory, plot_parameters) {
+    result <- tryCatch({
+        stopifnot(
+            "base_directory must be character" = is.character(base_directory),
+            "plot_parameters must be list" = is.list(plot_parameters),
+            "required parameters missing" = all(c("chromosome", "group") %in% 
+                                               names(plot_parameters))
+        )
+        
+        timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
+        filename <- sprintf(
+            "%s_overview_chr%s_group%d.svg",
+            timestamp,
+            plot_parameters$chromosome,
+            plot_parameters$group
+        )
+        
+        output_path <- file.path(base_directory, "plots", filename)
+        dir.create(dirname(output_path), recursive = TRUE, showWarnings = FALSE)
+        
+        list(
+            success = TRUE,
+            data = output_path,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+#' @title Create Track Visualization
+#' @description Creates track visualization plot
+#' @param track_group list Group of visualization tracks
+#' @param plot_options list Plot configuration options
+#' @return list Plot creation result {success, data, error}
+#' @throws PlotCreationError if creation fails
+#' @examples
+#' plot_tracks_create(tracks, list(width = 10, height = 8))
+#' @seealso track_group_create
+plot_tracks_create <- function(track_group, plot_options) {
+    result <- tryCatch({
+        stopifnot(
+            "track_group must be list" = is.list(track_group),
+            "plot_options must be list" = is.list(plot_options),
+            "required options missing" = all(c("width", "height", "output_path") %in% 
+                                            names(plot_options))
+        )
+        
+        # Calculate track limits if requested
+        y_limits <- if (plot_options$calculate_limits) {
+            range_result <- track_group_range_calculate(
+                track_group$tracks,
+                list(padding = 0.1)
+            )
+            if (range_result$success) {
+                c(range_result$data$min, range_result$data$max)
+            } else {
+                NULL
+            }
+        } else {
+            NULL
+        }
+        
+        # Create plot
+        svg(plot_options$output_path, 
+            width = plot_options$width, 
+            height = plot_options$height)
+        on.exit(dev.off(), add = TRUE)
+        
+        Gviz::plotTracks(
+            track_group$tracks,
+            chromosome = plot_options$chromosome,
+            ylim = y_limits
+        )
+        
+        list(
+            success = TRUE,
+            data = list(
+                path = plot_options$output_path,
+                limits = y_limits
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Calculate Global Range Across Bigwig Files
+#' @description Calculates combined value range across multiple bigwig files
+#' @param bigwig_files character Vector of bigwig file paths
+#' @param genome_range GRanges Genomic range to consider
+#' @return list Range calculation result {success, data, error}
+#' @throws RangeCalculationError if calculation fails
+#' @examples
+#' get_global_range(c("sample1.bw", "sample2.bw"), genome_range)
+#' @seealso track_range_calculate
+get_global_range <- function(bigwig_files, genome_range) {
+    result <- tryCatch({
+        stopifnot(
+            "bigwig_files must be character vector" = is.character(bigwig_files),
+            "bigwig_files cannot be empty" = length(bigwig_files) > 0,
+            "genome_range must be GRanges" = inherits(genome_range, "GRanges")
+        )
+        
+        # Calculate range for each file
+        ranges <- lapply(bigwig_files, function(bw_file) {
+            tryCatch({
+                # Import bigwig data for specified range
+                track_data <- rtracklayer::import(bw_file, which = genome_range)
+                if (length(track_data) == 0) {
+                    return(NULL)
+                }
+                
+                # Extract values and calculate range
+                values <- GenomicRanges::values(track_data)$score
+                if (length(values) == 0) {
+                    return(NULL)
+                }
+                
+                range(values, na.rm = TRUE)
+            }, error = function(e) {
+                warning(sprintf("Failed to process %s: %s", 
+                              basename(bw_file), e$message))
+                return(NULL)
+            })
+        })
+        
+        # Remove NULL results and combine ranges
+        valid_ranges <- do.call(rbind, ranges[!sapply(ranges, is.null)])
+        
+        if (nrow(valid_ranges) == 0) {
+            stop("No valid ranges found in any bigwig file")
+        }
+        
+        # Calculate global min and max
+        global_range <- c(
+            min(valid_ranges[, 1], na.rm = TRUE),
+            max(valid_ranges[, 2], na.rm = TRUE)
+        )
+        
+        list(
+            success = TRUE,
+            data = global_range,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+
+create_bigwig_sample_mapping <- function(sample_table, bigwig_files) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_table must be data.frame" = is.data.frame(sample_table),
+            "bigwig_files must be character" = is.character(bigwig_files),
+            "sample_id column must exist" = "sample_id" %in% colnames(sample_table)
+        )
+        
+        # Create mapping between sample IDs and bigwig files
+        bigwig_mapping <- sapply(
+            X = sample_table$sample_id,
+            FUN = function(sample_id) {
+                # Remove file extension for matching
+                clean_sample_id <- sub("\\.bw$", "", sample_id)
+                matching_file <- bigwig_files[grepl(clean_sample_id, bigwig_files)]
+                if (length(matching_file) > 0) matching_file[1] else NA_character_
+            },
+            USE.NAMES = TRUE
+        )
+        
+        list(
+            success = TRUE,
+            data = bigwig_mapping,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+
+# Define a formal data structure contract
+#' @title Create Standard Track Configuration
+#' @description Creates standardized track configuration structure
+track_configuration_create <- function(sample_data) {
+    result <- tryCatch({
+        stopifnot(
+            "sample_data must be list" = is.list(sample_data),
+            "required fields missing" = all(c("bigwig_file", "name") %in% names(sample_data))
+        )
+        
+        # Create standardized structure
+        track_config <- list(
+            bigwig_file = sample_data$bigwig_file,
+            name = sample_data$name,
+            metadata = list()  # Optional additional metadata
+        )
+        
+        class(track_config) <- "track_configuration"
+        
+        list(
+            success = TRUE,
+            data = track_config,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+
+create_sample_track_configs <- function(group_samples, bigwig_mapping) {
+    result <- tryCatch({
+        configs <- lapply(seq_len(nrow(group_samples)), function(i) {
+            current_sample_id <- group_samples$sample_id[i]
+            config_result <- track_configuration_create(list(
+                bigwig_file = bigwig_mapping[current_sample_id],
+                name = current_sample_id
+            ))
+            if (!config_result$success) stop(config_result$error)
+            config_result$data
+        })
+        
+        list(
+            success = TRUE,
+            data = configs,
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+
+validate_bigwig <- function(bigwig_path, experiment_number) {
+    if (is.na(bigwig_path) || !file.exists(bigwig_path)) {
+        warning(sprintf("Bigwig file not found for sample %s", experiment_number))
+        return(NULL)
+    }
+    tryCatch({
+        rtracklayer::import(bigwig_path)
+        return(bigwig_path)
+    }, error = function(e) {
+        warning(sprintf("Invalid bigwig file for sample %s: %s", experiment_number, e$message))
+        return(NULL)
+    })
+}
+
+find_fallback_control <- function(sample_table, bigwig_dir, pattern) {
+    # Get all Input samples
+    input_samples <- sample_table[sample_table$antibody == "Input", ]
+    
+    for (i in seq_len(nrow(input_samples))) {
+        control_bigwig <- list.files(
+            bigwig_dir,
+            pattern = paste0(input_samples$experiment_number[i], ".*normalized.*\\.bw$"),
+            full.names = TRUE
+        )[1]
+        
+        valid_control <- validate_bigwig(control_bigwig, input_samples$experiment_number[i])
+        if (!is.null(valid_control)) {
+            message("Using fallback control: ", basename(valid_control))
+            return(
+                list(
+                    index = i,
+                    path = valid_control
+                ))
+        }
+    }
+    
+    warning("No valid Input controls found in dataset")
+    return(NULL)
+}
diff --git a/failsafe_scripts/bmc_config.R b/failsafe_scripts/bmc_config.R
new file mode 100644
index 0000000..d3eb2d1
--- /dev/null
+++ b/failsafe_scripts/bmc_config.R
@@ -0,0 +1,181 @@
+################################################################################
+# BMC Experiment Configuration
+################################################################################
+#
+# PURPOSE:
+#   Defines and validates experimental design for BMC ChIP-seq experiments,
+#   including sample categories, valid combinations, and comparison groups.
+#
+# USAGE:
+#   1. Use '/!!' in vim/neovim to jump to required updates
+#   2. Modify METADATA section with experiment details
+#   3. Update CATEGORIES if experimental design changes
+#   4. Review INVALID_COMBINATIONS and EXPERIMENTAL_CONDITIONS
+#
+# !! ----> REQUIRED UPDATES:
+# !! EXPERIMENT_CONFIG$METADATA <- list(
+# !!     EXPERIMENT_ID = "241010Bel",
+# !!     EXPECTED_SAMPLES = 65,
+# !!     VERSION = "1.0.0"
+# !! )
+#
+# STRUCTURE:
+#   EXPERIMENT_CONFIG/
+#   +-- METADATA/
+#   |   +-- EXPERIMENT_ID    # Format: YYMMDD'Bel'
+#   |   +-- EXPECTED_SAMPLES # Total valid combinations
+#   |   +-- VERSION         # Configuration version
+#   +-- CATEGORIES/         # Valid values for each factor
+#   +-- INVALID_COMBINATIONS/# Excluded experimental combinations
+#   +-- EXPERIMENTAL_CONDITIONS/# Valid sample definitions
+#   +-- COMPARISONS/        # Analysis groupings
+#   +-- CONTROL_FACTORS/    # Control sample definitions
+#   +-- COLUMN_ORDER/       # Standard column arrangement
+#
+# VALIDATION:
+#   1. Category Values: Must be character vectors, unique
+#   2. Column References: All referenced columns must exist
+#   3. Column Order: Must include all category columns
+#   4. Sample Count: Must match EXPECTED_SAMPLES
+#
+# DEPENDENCIES:
+#   - R base packages only
+#   - functions_for_bmc_config_validation.R for validation functions
+#
+# COMMON ISSUES:
+#   1. Mismatched EXPERIMENT_ID -> Check format YYMMDD'Bel'
+#   2. Wrong sample count -> Review INVALID_COMBINATIONS
+#   3. Missing categories -> Check CATEGORIES vs COLUMN_ORDER
+#
+# AUTHOR: Luis
+# DATE: 2024-11-27
+# VERSION: 2.0.0
+#
+################################################################################
+# !! Update EXPERIMENT_CONFIG if starting a new experiment.
+EXPERIMENT_CONFIG <- list(
+    METADATA = list(
+        EXPERIMENT_ID = "241122Bel",
+        EXPECTED_SAMPLES = 65,
+        VERSION = "1.0.0"
+    ),
+
+    CATEGORIES = list(
+        rescue_allele = c("NONE", "WT", "4R", "PS"),
+        auxin_treatment = c("NO", "YES"),
+        time_after_release = c("0", "1", "2"),
+        antibody = c("Input", "ProtG", "HM1108", "V5", "ALFA", "UM174")
+    ),
+
+    INVALID_COMBINATIONS = list(
+        rescue_allele_auxin_treatment = quote(rescue_allele %in% c("4R", "PS") & auxin_treatment == "NO"),
+        protg_time_after_release = quote(antibody == "ProtG" & time_after_release %in% c("1", "2")),
+        input_time_after_release = quote(antibody == "Input" & time_after_release %in% c("1", "2")),
+        input_rescue_allele_auxin_treatment = quote(antibody == "Input" & rescue_allele %in% c("NONE", "WT") & auxin_treatment == "YES")
+    ),
+
+    EXPERIMENTAL_CONDITIONS = list(
+        is_input = quote(time_after_release == "0" & antibody == "Input"),
+        is_protg = quote(rescue_allele == "WT" & time_after_release == "0" & antibody == "ProtG" & auxin_treatment == "NO"),
+        is_v5 = quote(antibody == "V5"),
+        is_alfa = quote(antibody == "ALFA"),
+        is_1108 = quote(antibody == "HM1108" & time_after_release == "0"),
+        is_174 = quote(antibody == "UM174")
+    ),
+
+    SAMPLE_CLASSIFICATIONS = list(
+        is_input = quote(antibody == "Input"),
+
+        is_negative = quote(
+            antibody == "ProtG" |  # Protein G negative control
+            (antibody == "V5" & rescue_allele == "NONE") | # No-tag control
+            (time_after_release == "0" & antibody == "UM174") | # MCM at G2
+            (auxin_treatment == "YES" & antibody == "ALFA") # Degradation of Orc4-ALFA.
+        ),
+
+        is_positive = quote(
+            (antibody == "HM1108") |  # Known working condition
+            (antibody == "V5" & rescue_allele == "WT") | # Test by removing rescue_allele condition.
+            (antibody == "UM174" & time_after_release %in% c("1", "2") & rescue_allele == "WT")
+        )
+    ),
+
+    COMPARISONS = list(
+        comp_1108forNoneAndWT = quote(antibody == "HM1108" & rescue_allele %in% c("NONE", "WT")),
+        comp_1108forNoneAndWT_auxin = quote(antibody == "HM1108" & auxin_treatment == "YES"),
+        comp_timeAfterReleaseV5WT = quote(antibody == "V5" & rescue_allele == "WT" & auxin_treatment == "YES"),
+        comp_timeAfterReleaseV5NoTag = quote(antibody == "V5" & rescue_allele == "NONE" & auxin_treatment == "YES"),
+        comp_V5atTwoHours = quote(antibody == "V5" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_UM174atTwoHours = quote(antibody == "UM174" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_ALFAforNoRescueNoTreat = quote(antibody == "ALFA" & rescue_allele == "NONE" & auxin_treatment == "NO"),
+        comp_ALFAforNoRescueWithTreat = quote(antibody == "ALFA" & rescue_allele == "NONE" & auxin_treatment == "YES"),
+        comp_ALFAatTwoHoursForAllAlleles = quote(antibody == "ALFA" & time_after_release == "2" & auxin_treatment == "YES"),
+        comp_UM174atZeroHoursForAllAlleles = quote(antibody == "UM174" & time_after_release == "0" & auxin_treatment == "YES"),
+        comp_AuxinEffectOnUM174 = quote(antibody == "UM174" & time_after_release == "2" & rescue_allele %in% c("NONE", "WT"))
+    ),
+
+    CONTROL_FACTORS = list(
+        genotype = c("rescue_allele")
+    ),
+
+    COLUMN_ORDER = c("antibody", "rescue_allele", "auxin_treatment", "time_after_release"),
+
+    NORMALIZATION = list(
+        methods = c("CPM", "BPM", "RPGC", "RPKM"),
+        active = "CPM"  # Set via config
+    )
+)
+
+
+################################################################################
+# Configuration Validation
+################################################################################
+source("~/lab_utils/failsafe_scripts/functions_for_bmc_config_validation.R")
+
+# !! Update if you want thourough messages during validation.
+validation_verbose <- FALSE  # Set to TRUE for detailed validation output
+
+# Validate configuration structure
+if (validation_verbose) cat("\nValidating EXPERIMENT_CONFIG structure...\n")
+
+required_sections <- c("METADATA", "CATEGORIES", "INVALID_COMBINATIONS",
+                      "EXPERIMENTAL_CONDITIONS", "COMPARISONS",
+                      "CONTROL_FACTORS", "COLUMN_ORDER", "NORMALIZATION",
+                      "SAMPLE_CLASSIFICATIONS")
+
+
+missing_sections <- setdiff(required_sections, names(EXPERIMENT_CONFIG))
+if (length(missing_sections) > 0) {
+    stop(sprintf("Missing required config sections: %s",
+                paste(missing_sections, collapse = ", ")))
+}
+
+if (validation_verbose) cat("[PASS] All required sections present\n\n")
+
+# Validate configuration structure
+stopifnot(
+    "Missing required config sections" =
+        all(required_sections %in% names(EXPERIMENT_CONFIG))
+)
+
+# Validate each section
+validate_category_values(
+    EXPERIMENT_CONFIG$CATEGORIES,
+    verbose = validation_verbose
+)
+
+validate_column_references(
+    categories = EXPERIMENT_CONFIG$CATEGORIES,
+    comparisons = EXPERIMENT_CONFIG$COMPARISONS,
+    control_factors = EXPERIMENT_CONFIG$CONTROL_FACTORS,
+    conditions = EXPERIMENT_CONFIG$EXPERIMENTAL_CONDITIONS,
+    verbose = validation_verbose
+)
+
+validate_column_order(
+    categories = EXPERIMENT_CONFIG$CATEGORIES,
+    column_order = EXPERIMENT_CONFIG$COLUMN_ORDER,
+    verbose = validation_verbose
+)
+
+cat("\n[VALIDATED] Experiment configuration loaded successfully\n")
diff --git a/failsafe_scripts/cleanup_bmc_directory.sh b/failsafe_scripts/cleanup_bmc_directory.sh
new file mode 100755
index 0000000..87e8596
--- /dev/null
+++ b/failsafe_scripts/cleanup_bmc_directory.sh
@@ -0,0 +1,48 @@
+#!/bin/bash
+
+# Strict error handling
+set -euo pipefail
+trap 'echo "Error on line $LINENO"' ERR
+
+# Log file in /tmp for operations tracking
+log_file="/tmp/cleanup_$(date +%Y%m%d_%H%M%S).log"
+exec 1> >(tee -a "$log_file")
+exec 2>&1
+
+echo "Starting cleanup operation at $(date)"
+
+# Store current directory
+current_dir=$(pwd)
+echo "Working directory: $current_dir"
+
+# First count existing fastq files for verification
+initial_fastq_count=$(find . -type f -name "*.fastq" | wc -l)
+echo "Found $initial_fastq_count FASTQ files initially"
+
+# Remove unmapped files first
+echo "Removing unmapped files..."
+find . -type f -name "*unmapped*" -delete
+
+# Remove all non-fastq files
+echo "Removing non-FASTQ files..."
+find . -type f ! -name "*.fastq" -delete
+
+# Move all fastq files to current directory
+echo "Moving FASTQ files to current directory..."
+find . -type f -name "*.fastq" -exec mv {} . \;
+
+# Remove empty directories
+echo "Removing empty directories..."
+find . -type d -empty -delete
+
+# Verify final state
+final_fastq_count=$(find . -maxdepth 1 -type f -name "*.fastq" | wc -l)
+echo "Final FASTQ count in current directory: $final_fastq_count"
+
+if [ "$initial_fastq_count" -ne "$final_fastq_count" ]; then
+    echo "ERROR: FASTQ file count mismatch! Initial: $initial_fastq_count, Final: $final_fastq_count"
+    exit 1
+fi
+
+echo "Operation completed successfully at $(date)"
+echo "Log file: $log_file"
diff --git a/failsafe_scripts/comparison_analysis.R b/failsafe_scripts/comparison_analysis.R
new file mode 100644
index 0000000..8b3751f
--- /dev/null
+++ b/failsafe_scripts/comparison_analysis.R
@@ -0,0 +1,99 @@
+# Function to load and validate metadata
+load_metadata <- function(directory_path) {
+    print("Loading processed metadata file")
+    
+    csv_path <- file.path(
+        directory_path,
+        "documentation",
+        sprintf("%s_processed_grid.csv", basename(directory_path))
+    )
+    
+    if (!file.exists(csv_path)) {
+        stop(sprintf("Processed metadata file not found: %s", csv_path))
+    }
+    
+    metadata <- read.csv(
+        file = csv_path,
+        stringsAsFactors = FALSE,
+        check.names = TRUE
+    )
+    
+    print(sprintf("Loaded metadata with %d rows", nrow(metadata)))
+    return(metadata)
+}
+
+# Function to perform comparisons
+analyze_comparisons <- function(metadata, comparisons) {
+    print("Starting comparison analysis")
+    
+    results <- list()
+    
+    for (comp_name in names(comparisons)) {
+        print(sprintf("\nAnalyzing comparison: %s", comp_name))
+        
+        # Evaluate the comparison expression
+        subset_rows <- eval(comparisons[[comp_name]], metadata)
+        subset_data <- metadata[subset_rows, ]
+        
+        # Store results
+        results[[comp_name]] <- subset_data
+        
+        # Log results
+        print(sprintf("Found %d matching samples:", nrow(subset_data)))
+        if (nrow(subset_data) > 0) {
+            print("Sample details:")
+            for (i in 1:nrow(subset_data)) {
+                print(sprintf(
+                    "  Sample %d: %s_%s_%s_%s (Exp: %s)",
+                    i,
+                    subset_data$antibody[i],
+                    subset_data$rescue_allele[i],
+                    subset_data$auxin_treatment[i],
+                    subset_data$time_after_release[i],
+                    subset_data$experiment_number[i]
+                ))
+            }
+        } else {
+            print("  No matching samples found")
+        }
+    }
+    
+    return(invisible(results))
+}
+
+# Main execution
+main <- function() {
+    args <- commandArgs(trailingOnly = TRUE)
+    
+    if (length(args) != 1) {
+        stop("Usage: Rscript comparison_analysis.R <experiment_directory>")
+    }
+    
+    directory_path <- args[1]
+    
+    if (!dir.exists(directory_path)) {
+        stop(sprintf("Directory not found: %s", directory_path))
+    }
+    
+    print("Loading configuration")
+    source("~/lab_utils/scripts/bmc_config.R")
+    
+    if (!exists("EXPERIMENT_CONFIG")) {
+        stop("Configuration loading failed: EXPERIMENT_CONFIG not found")
+    }
+    
+    print(sprintf("Analyzing experiment: %s", EXPERIMENT_CONFIG$METADATA$EXPERIMENT_ID))
+    
+    # Load metadata
+    metadata <- load_metadata(directory_path)
+    
+    # Perform comparisons
+    results <- analyze_comparisons(metadata, EXPERIMENT_CONFIG$COMPARISONS)
+    
+    print("\nAnalysis completed successfully")
+}
+
+# Execute main if script is run directly
+if (identical(environment(), globalenv())) {
+    main()
+}
diff --git a/failsafe_scripts/consolidate_fastq.sh b/failsafe_scripts/consolidate_fastq.sh
new file mode 100755
index 0000000..db2e262
--- /dev/null
+++ b/failsafe_scripts/consolidate_fastq.sh
@@ -0,0 +1,90 @@
+#!/bin/bash
+
+# Strict error handling
+set -euo pipefail
+
+# Function to validate FASTQ files
+validate_fastq() {
+    local file="$1"
+    if ! [ -f "$file" ]; then
+        echo "Error: $file not found"
+        exit 1
+    fi
+    if ! [[ "$file" =~ \.fastq$ ]]; then
+        echo "Error: $file is not a FASTQ file"
+        exit 1
+    fi
+}
+
+# Extract unique IDs using delimiter-based approach
+# This specifically extracts the ID between the first and second dash after 'D24'
+unique_ids=$(ls *_sequence.fastq | awk -F'D24-' '{print $2}' | cut -d'-' -f1 | sort -u)
+
+# Verify we found some IDs
+if [ -z "$unique_ids" ]; then
+    echo "Error: No valid IDs found in fastq files"
+    exit 1
+fi
+
+echo "Found the following unique IDs:"
+echo "$unique_ids" | xargs -n6 | sed 's/^/    /'
+
+# Process each unique ID
+for id in $unique_ids; do
+    echo "Processing ID: $id"
+
+    # Find all files matching the specific pattern
+    # Using more strict pattern matching to avoid false matches
+    files=$(ls *"D24-${id}-"*_sequence.fastq 2>/dev/null || true)
+
+    # Check if we found any files
+    if [ -z "$files" ]; then
+        echo "No files found for ID: $id"
+        continue
+    fi
+
+    # Validate all files before processing
+    for file in $files; do
+        validate_fastq "$file"
+        echo "Validated: $file"
+    done
+
+    # Create output filename
+    output_file="consolidated_${id}_sequence.fastq"
+
+    echo "Successfully created $output_file"
+    # Consolidate files
+    if cat $files > "$output_file"; then
+        echo "Successfully created $output_file"
+
+        # Verify the new file exists and has content
+        if [ -s "$output_file" ]; then
+            # Get size before and after
+            original_size=$(du -b $files | awk '{sum += $1} END {print sum}')
+            new_size=$(du -b "$output_file" | awk '{print $1}')
+
+            echo "Original files total size: $original_size bytes"
+            echo "New file size: $new_size bytes"
+
+            if [ "$new_size" -gt 0 ]; then
+                echo "Removing original files..."
+                rm -f $files
+                echo "Original files removed"
+            else
+                echo "Error: Consolidated file is empty"
+                rm -f "$output_file"
+                exit 1
+            fi
+        else
+            echo "Error: Consolidated file is empty"
+            rm -f "$output_file"
+            exit 1
+        fi
+    else
+        echo "Error during consolidation"
+        rm -f "$output_file"
+        exit 1
+    fi
+done
+
+echo "All consolidation operations completed successfully"
diff --git a/failsafe_scripts/context_registry.R b/failsafe_scripts/context_registry.R
new file mode 100644
index 0000000..bfec218
--- /dev/null
+++ b/failsafe_scripts/context_registry.R
@@ -0,0 +1,117 @@
+# context_registry.R
+
+FUNCTION_REGISTRY <- list(
+    data_processing = c(
+        "metadata_path_validate",
+        "metadata_file_read",
+        "metadata_schema_validate",
+        "metadata_comparisons_process",
+        "comparison_samples_filter",
+        "comparison_expression_validate",
+        "comparison_expression_evaluate",
+        "comparison_results_format"
+    ),
+    genomic_data = c(
+        "chromosome_mappings_initialize",
+        "chromosome_style_validate",
+        "chromosome_names_clean",
+        "chromosome_names_to_ucsc",
+        "chromosome_names_to_roman",
+        "chromosome_names_to_numeric",
+        "chromosome_names_convert"
+    ),
+    visualization = c(
+        "bigwig_file_exists",
+        "bigwig_file_validate",
+        "track_values_extract",
+        "track_range_calculate",
+        "track_group_range_calculate",
+        "track_single_create",
+        "track_group_create",
+        "plot_path_generate",
+        "plot_tracks_create",
+        "get_global_range"
+    )
+)
+
+DATA_CONTRACTS <- list(
+    result_structure = list(
+        success = "logical(1)",
+        data = "list()",
+        error = "character(1)"
+    ),
+    track_data = list(
+        track = "GenomicRanges",
+        source = "character(1)",
+        metadata = "list()"
+    ),
+    plot_options = list(
+        width = "numeric(1)",
+        height = "numeric(1)",
+        color = "character(1)",
+        chromosome = "character(1)"
+    )
+)
+
+SHARED_CONSTANTS <- list(
+    PLOT_CONFIG = list(
+        SAMPLES_PER_PAGE = 4,
+        DEFAULT_CHROMOSOME = 10,
+        TRACK_COLOR = "#fd0036",
+        WIDTH = 10,
+        HEIGHT = 8
+    ),
+    REQUIRED_PACKAGES = c(
+        "rtracklayer",
+        "GenomicRanges",
+        "Gviz",
+        "tidyverse"
+    )
+)
+
+FUNCTION_DEPENDENCIES <- list(
+    metadata_comparisons_process = c(
+        "comparison_samples_filter",
+        "comparison_expression_validate",
+        "comparison_expression_evaluate"
+    ),
+    track_group_create = c(
+        "track_single_create",
+        "bigwig_file_validate",
+        "track_values_extract"
+    ),
+    plot_tracks_create = c(
+        "track_group_range_calculate",
+        "track_range_calculate"
+    ),
+    get_global_range = c(
+        "track_range_calculate"
+    )
+)
+
+# Function to validate function existence
+validate_function_existence <- function() {
+    all_functions <- unlist(FUNCTION_REGISTRY)
+    missing_functions <- all_functions[!sapply(all_functions, exists)]
+    if (length(missing_functions) > 0) {
+        stop("Missing functions: ", paste(missing_functions, collapse = ", "))
+    }
+    return(TRUE)
+}
+
+# Function to check dependencies
+validate_dependencies <- function() {
+    missing_deps <- lapply(names(FUNCTION_DEPENDENCIES), function(func) {
+        deps <- FUNCTION_DEPENDENCIES[[func]]
+        missing <- deps[!sapply(deps, exists)]
+        if (length(missing) > 0) {
+            return(list(function = func, missing = missing))
+        }
+        return(NULL)
+    })
+    missing_deps <- missing_deps[!sapply(missing_deps, is.null)]
+    if (length(missing_deps) > 0) {
+        stop("Unmet dependencies found")
+    }
+    return(TRUE)
+}
diff --git a/failsafe_scripts/extract_bmcIDmetadata_process.R b/failsafe_scripts/extract_bmcIDmetadata_process.R
new file mode 100644
index 0000000..66defe6
--- /dev/null
+++ b/failsafe_scripts/extract_bmcIDmetadata_process.R
@@ -0,0 +1,186 @@
+# Function to extract experiment numbers from fastq files
+extract_experiment_numbers <- function(directory_path) {
+    print(sprintf("Scanning directory: %s", directory_path))
+    
+    fastq_files <- list.files(
+        path = file.path(directory_path, "fastq"),
+        pattern = "consolidated_.*_sequence\\.fastq$",
+        full.names = FALSE
+    )
+    
+    if (length(fastq_files) == 0) {
+        stop("No fastq files found in specified directory")
+    }
+    
+    print(sprintf("Found %d fastq files", length(fastq_files)))
+    
+    # Extract numbers using base R
+    exp_numbers <- gsub(
+        pattern = "consolidated_([0-9]{5,6})_sequence\\.fastq",
+        replacement = "\\1",
+        x = fastq_files
+    )
+    
+    return(exp_numbers)
+}
+
+
+# Function to validate and enforce factor levels
+enforce_factor_levels <- function(data_frame, categories) {
+    print("Validating and enforcing factor levels")
+    
+    if (!all(names(categories) %in% colnames(data_frame))) {
+        missing_cols <- setdiff(names(categories), colnames(data_frame))
+        stop(sprintf("Missing required columns: %s", paste(missing_cols, collapse = ", ")))
+    }
+    
+    for (col_name in names(categories)) {
+        print(sprintf("Processing column: %s", col_name))
+        
+        # Check for invalid levels
+        invalid_levels <- setdiff(data_frame[[col_name]], categories[[col_name]])
+        if (length(invalid_levels) > 0) {
+            stop(sprintf(
+                "Invalid levels in %s: %s\nAllowed levels: %s",
+                col_name,
+                paste(invalid_levels, collapse = ", "),
+                paste(categories[[col_name]], collapse = ", ")
+            ))
+        }
+        
+        # Convert to factor with predefined levels
+        data_frame[[col_name]] <- factor(
+            x = data_frame[[col_name]],
+            levels = categories[[col_name]],
+            ordered = TRUE
+        )
+        
+        print(sprintf(
+            "Enforced %d levels for %s",
+            length(categories[[col_name]]),
+            col_name
+        ))
+    }
+    
+    return(data_frame)
+}
+
+# Function to sort dataframe by multiple columns
+sort_metadata_frame <- function(data_frame, column_order) {
+    print("Sorting metadata frame")
+    
+    if (!all(column_order %in% colnames(data_frame))) {
+        missing_cols <- setdiff(column_order, colnames(data_frame))
+        stop(sprintf("Missing sort columns: %s", paste(missing_cols, collapse = ", ")))
+    }
+    
+    sorted_frame <- data_frame[do.call(
+        order,
+        data_frame[column_order]
+    ), ]
+    
+    print(sprintf("Sorted by columns: %s", paste(column_order, collapse = ", ")))
+    
+    return(sorted_frame)
+}
+
+# Modified process_metadata function
+process_metadata <- function(directory_path, output_to_file = TRUE) {
+    print("Starting metadata processing")
+    
+    # Source configuration
+    source("~/lab_utils/scripts/bmc_config.R")
+    if (!exists("EXPERIMENT_CONFIG")) {
+        stop("Configuration loading failed: EXPERIMENT_CONFIG not found")
+    }
+    
+    # Construct file path
+    csv_path <- file.path(
+        directory_path,
+        "documentation",
+        sprintf("%s_sample_grid.csv", basename(directory_path))
+    )
+    
+    # Validate file existence
+    if (!file.exists(csv_path)) {
+        stop(sprintf("Metadata file not found: %s", csv_path))
+    }
+    
+    # Read CSV
+    metadata <- read.csv(
+        file = csv_path,
+        stringsAsFactors = FALSE,
+        check.names = TRUE
+    )
+    
+    print(sprintf("Read metadata file with %d rows", nrow(metadata)))
+    
+    # Enforce factor levels
+    metadata <- enforce_factor_levels(
+        data_frame = metadata,
+        categories = EXPERIMENT_CONFIG$CATEGORIES
+    )
+    
+    # Sort dataframe
+    metadata <- sort_metadata_frame(
+        data_frame = metadata,
+        column_order = EXPERIMENT_CONFIG$COLUMN_ORDER
+    )
+    
+    # Get experiment numbers
+    exp_numbers <- extract_experiment_numbers(directory_path)
+    
+    # Validate length match
+    if (length(exp_numbers) != nrow(metadata)) {
+        stop(sprintf(
+            "Mismatch between number of fastq files (%d) and metadata rows (%d)",
+            length(exp_numbers),
+            nrow(metadata)
+        ))
+    }
+    
+    # Add experiment numbers
+    metadata$sample_id <- exp_numbers
+    
+    # Save processed file
+    if(output_to_file) {
+        output_path <- file.path(
+            directory_path,
+            "documentation",
+            sprintf("%s_processed_grid.csv", basename(directory_path))
+        )
+        
+        write.csv(
+            x = metadata,
+            file = output_path,
+            row.names = FALSE,
+            quote = TRUE
+        )
+    }
+    print(sprintf("Saved processed metadata to: %s", output_path))
+    
+    return(metadata)
+}
+
+# Main execution
+main <- function() {
+    args <- commandArgs(trailingOnly = TRUE)
+    
+    if (length(args) != 1) {
+        stop("Usage: Rscript experiment_metadata_process.R <experiment_directory>")
+    }
+    
+    directory_path <- args[1]
+    
+    if (!dir.exists(directory_path)) {
+        stop(sprintf("Directory not found: %s", directory_path))
+    }
+    
+    processed_data <- process_metadata(directory_path)
+    print("Processing completed successfully")
+}
+
+## Execute main if script is run directly
+#if (identical(environment(), globalenv())) {
+#    main()
+#}
diff --git a/failsafe_scripts/functions_for_bmc_config_validation.R b/failsafe_scripts/functions_for_bmc_config_validation.R
new file mode 100644
index 0000000..b137c2c
--- /dev/null
+++ b/failsafe_scripts/functions_for_bmc_config_validation.R
@@ -0,0 +1,190 @@
+################################################################################
+# Experiment Configuration Validation Functions
+################################################################################
+#' @title Validate Category Values in Experiment Configuration
+#' @description Ensures all category values are character vectors and unique
+#' @param categories list Category name to character vector mapping
+#' @param stop_on_error logical Whether to stop on error or return status (default: TRUE)
+#' @param verbose logical Print detailed validation steps (default: FALSE)
+#' @return logical TRUE if validation passes, FALSE if fails and stop_on_error is FALSE
+#' @examples
+#' categories <- list(
+#'     condition = c("control", "treatment"),
+#'     replicate = c("1", "2", "3")
+#' )
+#' validate_category_values(categories, verbose = TRUE)
+validate_category_values <- function(
+    categories,     # list: category name -> character vector mapping
+    stop_on_error = TRUE,  # logical: whether to stop or return validation status
+    verbose = FALSE    # logical: whether to print validation steps
+) {
+    tryCatch({
+        if (verbose) cat("[VALIDATING] Checking category values...\n")
+        
+        sapply(names(categories), function(category_name) {
+            values <- categories[[category_name]]
+            
+            if (verbose) {
+                cat(sprintf("  Checking category '%s':\n", category_name))
+                cat(sprintf("    Values: %s\n", paste(values, collapse = ", ")))
+            }
+            
+            # Check if values are character
+            if (!is.character(values)) {
+                if (verbose) cat(sprintf("    [FAIL] Not character vector\n"))
+                stop(sprintf(
+                    "Category '%s' values must be character vectors",
+                    category_name
+                ))
+            }
+            if (verbose) cat("    [PASS] Character vector check\n")
+            
+            # Check for duplicates
+            if (any(duplicated(values))) {
+                if (verbose) cat("    [FAIL] Contains duplicates\n")
+                stop(sprintf(
+                    "Category '%s' values must be unique",
+                    category_name
+                ))
+            }
+            if (verbose) cat("    [PASS] Uniqueness check\n")
+        })
+        
+        if (verbose) cat("[PASS] All category values valid\n")
+        return(TRUE)
+    }, error = function(e) {
+        if (verbose) cat(sprintf("[FAIL] Category validation: %s\n", e$message))
+        if (stop_on_error) stop(e) else return(FALSE)
+    })
+}
+
+#' @title Validate Column References in Experimental Design
+#' @description Ensures all column references in comparisons, control factors,
+#'   and conditions match the defined categories
+#' @param categories list Valid column names from configuration
+#' @param comparisons list Named expressions for experimental comparisons
+#' @param control_factors list Factor definitions for controls
+#' @param conditions list Experimental condition definitions
+#' @param stop_on_error logical Whether to stop on error or return status (default: TRUE)
+#' @param verbose logical Print detailed validation steps (default: FALSE)
+#' @return logical TRUE if validation passes, FALSE if fails and stop_on_error is FALSE
+#' @examples
+#' validate_column_references(
+#'     categories = list(treatment = c("A", "B")),
+#'     comparisons = list(comp1 = quote(treatment == "A")),
+#'     control_factors = list(ctrl = c("treatment")),
+#'     conditions = list(cond1 = quote(treatment == "B"))
+#' )
+validate_column_references <- function(
+    categories,        # list: valid column names
+    comparisons,       # list: named expressions
+    control_factors,   # list: factor definitions
+    conditions,        # list: experimental conditions
+    stop_on_error = TRUE,
+    verbose = FALSE
+) {
+    valid_columns <- names(categories)
+    
+    if (verbose) {
+        cat("[VALIDATING] Checking column references...\n")
+        cat("  Valid columns:", paste(valid_columns, collapse = ", "), "\n")
+    }
+    
+    # Helper function for expression validation
+    check_expr_vars <- function(expr, context_name) {
+        if (verbose) cat(sprintf("  Checking %s\n", context_name))
+        
+        invalid_cols <- setdiff(all.vars(expr), valid_columns)
+        if (length(invalid_cols) > 0) {
+            if (verbose) {
+                cat(sprintf("    [FAIL] Invalid columns found: %s\n", 
+                    paste(invalid_cols, collapse = ", ")))
+            }
+            stop(sprintf(
+                "Invalid columns in %s: %s",
+                context_name, paste(invalid_cols, collapse = ", ")
+            ))
+        }
+        if (verbose) cat("    [PASS] All columns valid\n")
+    }
+    
+    tryCatch({
+        # Check comparisons
+        if (verbose) cat("Validating comparisons:\n")
+        lapply(names(comparisons), function(comp_name) {
+            check_expr_vars(comparisons[[comp_name]], 
+                          sprintf("comparison '%s'", comp_name))
+        })
+        
+        # Check control factors
+        if (verbose) cat("Validating control factors:\n")
+        lapply(names(control_factors), function(factor_name) {
+            if (verbose) {
+                cat(sprintf("  Checking control factor '%s'\n", factor_name))
+            }
+            invalid_cols <- setdiff(control_factors[[factor_name]], valid_columns)
+            if (length(invalid_cols) > 0) {
+                if (verbose) {
+                    cat(sprintf("    [FAIL] Invalid columns: %s\n",
+                        paste(invalid_cols, collapse = ", ")))
+                }
+                stop(sprintf(
+                    "Invalid columns in control factor '%s': %s",
+                    factor_name, paste(invalid_cols, collapse = ", ")
+                ))
+            }
+            if (verbose) cat("    [PASS] All columns valid\n")
+        })
+        
+        # Check conditions
+        if (verbose) cat("Validating experimental conditions:\n")
+        lapply(names(conditions), function(cond_name) {
+            check_expr_vars(conditions[[cond_name]], 
+                          sprintf("condition '%s'", cond_name))
+        })
+        
+        if (verbose) cat("[PASS] All column references valid\n")
+        return(TRUE)
+    }, error = function(e) {
+        if (verbose) cat(sprintf("[FAIL] Column reference validation: %s\n", e$message))
+        if (stop_on_error) stop(e) else return(FALSE)
+    })
+}
+
+#' @title Validate Column Order in Experiment Configuration
+#' @description Ensures column order includes all category columns
+#' @param categories list Category definitions from configuration
+#' @param column_order character vector Ordered column names
+#' @param stop_on_error logical Whether to stop on error or return status (default: TRUE)
+#' @param verbose logical Print detailed validation steps (default: FALSE)
+#' @return logical TRUE if validation passes, FALSE if fails and stop_on_error is FALSE
+#' @examples
+#' validate_column_order(
+#'     categories = list(treatment = c("A", "B"), time = c("0h", "2h")),
+#'     column_order = c("treatment", "time")
+#' )
+validate_column_order <- function(
+    categories,    # list: category definitions
+    column_order,  # character: ordered column names
+    stop_on_error = TRUE,
+    verbose = FALSE
+) {
+    tryCatch({
+        if (verbose) {
+            cat("[VALIDATING] Checking column order...\n")
+            cat("  Category columns:", paste(sort(names(categories)), collapse = ", "), "\n")
+            cat("  Column order:", paste(sort(column_order), collapse = ", "), "\n")
+        }
+        
+        if (!identical(sort(names(categories)), sort(column_order))) {
+            if (verbose) cat("  [FAIL] Column order mismatch\n")
+            stop("Column order must include all category columns")
+        }
+        
+        if (verbose) cat("[PASS] Column order valid\n")
+        return(TRUE)
+    }, error = function(e) {
+        if (verbose) cat(sprintf("[FAIL] Column order validation: %s\n", e$message))
+        if (stop_on_error) stop(e) else return(FALSE)
+    })
+}
diff --git a/failsafe_scripts/functions_for_file_operations.R b/failsafe_scripts/functions_for_file_operations.R
new file mode 100644
index 0000000..c34f2c5
--- /dev/null
+++ b/failsafe_scripts/functions_for_file_operations.R
@@ -0,0 +1,179 @@
+#' Safe File Writing Function
+#' @param data The data to write (or source path for file.copy)
+#' @param path Destination file path
+#' @param write_fn The writing function to use. Supported functions:
+#'                 - write.csv(x, file, ...)
+#'                 - write.table(x, file, ...)
+#'                 - file.copy(from, to, ...)
+#'                 - saveRDS(object, file, ...)
+#' @param verbose Boolean to control logging output
+#' @param interactive Boolean to control user prompts for overwriting
+#' @param ... Additional arguments passed to write_fn
+#' @return Boolean indicating success
+safe_write_file <- function(data, path, write_fn, verbose = FALSE, interactive = TRUE, ...) {
+    # Convert to absolute path
+    tryCatch({
+        abs_path <- normalizePath(path, mustWork = FALSE)
+    }, error = function(e) {
+        cat("[ERROR] Failed to resolve path. Check path validity.\n")
+        return(FALSE)
+    })
+
+    # Input validation 
+    stopifnot(
+        "write_fn must be a function" = is.function(write_fn),
+        "verbose must be logical" = is.logical(verbose) && length(verbose) == 1,
+        "interactive must be logical" = is.logical(interactive) && length(interactive) == 1,
+        "parent directory must be writable" = file.access(dirname(abs_path), mode = 2) == 0
+    )
+
+    # Create wrapper for supported functions
+    # Handles differences in parameter ordering for different file function.
+    # Only allows explicitly handled functions.
+    wrapped_write <- function(data, path, ...) {
+        if (identical(write_fn, write.csv) || identical(write_fn, write.table)) {
+            write_fn(x = data, file = path, ...)
+        } else if (identical(write_fn, file.copy)) {
+            write_fn(from = data, to = path, ...)
+        } else if (identical(write_fn, saveRDS)) {
+            write_fn(object = data, file = path, ...)
+        } else {
+            stop(sprintf(
+                "Unsupported writing function: %s. Supported functions are: write.csv, write.table, file.copy, saveRDS",
+                deparse(substitute(write_fn))
+            ))
+        }
+    }
+    if (verbose) {
+        cat(sprintf("[INFO] Processing file: %s\n", abs_path))
+    }
+
+    do_write <- function() {
+        tryCatch({
+            wrapped_write(data, abs_path, ...)
+            if (verbose) {
+                cat(sprintf("[WROTE] File to: %s\n", abs_path))
+            }
+            return(TRUE)
+        }, error = function(e) {
+            cat(sprintf("[ERROR] Failed to write file: %s\n", e$message))
+            return(FALSE)
+        })
+    }
+
+    # If file doesn't exist or non-interactive, write immediately
+    if (!file.exists(abs_path) || !interactive) {
+        return(do_write())
+    }
+
+    # Handle existing file in interactive mode
+    if (verbose) {
+        cat(sprintf("[WARNING] File already exists: %s\n", abs_path))
+    }
+
+    user_input <- readline(prompt="File exists. Overwrite? (y/n): ")
+    if (tolower(user_input) == "y") {
+        return(do_write())
+    }
+
+    # Handle non-overwrite cases
+    message <- if(tolower(user_input) == "n") {
+        "[WARNING SKIP] No overwrite. Did not write: %s\n"
+    } else {
+        "[WARNING SKIP] Option not recognized. Did not write: %s\n"
+    }
+    cat(sprintf(message, abs_path))
+    return(FALSE)
+}
+
+#' Safe Source Function for R Scripts
+#' @param file_path Path to the R script to source
+#' @param verbose Boolean to control logging output
+#' @param local Logical; evaluate sourced code in local or global environment
+#' @param chdir Logical; change directory when sourcing
+#' @param ... Additional arguments passed to source()
+#' @return Boolean indicating success of sourcing operation
+safe_source <- function(file_path, verbose = FALSE, local = FALSE, chdir = FALSE, ...) {
+    # Input validation
+    stopifnot(
+        "file_path must be a character string" = is.character(file_path) && length(file_path) == 1,
+        "verbose must be logical" = is.logical(verbose) && length(verbose) == 1,
+        "local must be logical" = is.logical(local) && length(local) == 1,
+        "chdir must be logical" = is.logical(chdir) && length(chdir) == 1,
+        "file must exist" = file.exists(file_path),
+        "file must be readable" = file.access(file_path, mode = 4) == 0
+    )
+
+    # Convert to absolute path
+    tryCatch({
+        abs_path <- normalizePath(file_path, mustWork = FALSE)
+    }, error = function(e) {
+        cat("[ERROR] Failed to resolve path. Check path validity.\n")
+        return(FALSE)
+    })
+
+
+    # Attempt to source the file
+    if (verbose) {
+        cat(sprintf("[INFO] Attempting to source: %s\n", abs_path))
+    }
+
+    tryCatch({
+        source(abs_path, local = local, chdir = chdir, ...)
+        if (verbose) {
+            cat(sprintf("[SUCCESS] Successfully sourced: %s\n", abs_path))
+        }
+        return(TRUE)
+    }, error = function(e) {
+        # Enhanced error messages for common source() errors
+        error_msg <- switch(class(e)[1],
+            "parseError" = "Syntax error in script",
+            "evaluationError" = "Error evaluating script",
+            e$message
+        )
+        cat(sprintf("[ERROR] Failed to source %s: %s\n", abs_path, error_msg))
+        return(FALSE)
+    }, warning = function(w) {
+        if (verbose) {
+            cat(sprintf("[WARNING] Warning while sourcing %s: %s\n", abs_path, w$message))
+        }
+        return(TRUE)
+    })
+}
+
+#' Find Files with Timestamp Pattern
+#' @param base_path Path to check for timestamped versions
+#' @param full_paths Boolean to return full paths or just filenames
+#' @return Character vector of matching files
+find_timestamped_files <- function(base_path, full_paths = TRUE) {
+    # Input validation
+    stopifnot(
+        "base_path must be a character string" = is.character(base_path) && length(base_path) == 1,
+        "full_paths must be logical" = is.logical(full_paths) && length(full_paths) == 1
+    )
+
+    # Path normalization
+    base_path <- normalizePath(base_path, mustWork = FALSE)
+    
+    # Extract path components
+    dir_path <- dirname(base_path)
+    base_name <- basename(base_path)
+    
+    # Timestamp regex pattern
+    timestamp_regex <- "\\d{4}\\d{2}\\d{2}_\\d{2}\\d{2}\\d{2}"
+    
+    # Remove timestamp from base name
+    base_pattern <- gsub(timestamp_regex, "", base_name, perl = TRUE)
+    
+    # Create search pattern
+    search_pattern <- sprintf("^.*%s$", base_pattern)
+    
+    # Find matching files
+    matches <- list.files(
+        dir_path,
+        pattern = search_pattern,
+        full.names = full_paths
+    )
+    
+    return(matches)
+}
diff --git a/failsafe_scripts/functions_for_genome_tracks.R b/failsafe_scripts/functions_for_genome_tracks.R
new file mode 100644
index 0000000..1d01b1e
--- /dev/null
+++ b/failsafe_scripts/functions_for_genome_tracks.R
@@ -0,0 +1,543 @@
+create_minimal_identifiers <- function(sample_ids, verbose = FALSE) {
+
+    # Validation
+    stopifnot(
+        "sample_ids must be character vector" = is.character(sample_ids),
+        "sample_ids cannot be empty" = length(sample_ids) > 0,
+        "sample_ids must be unique" = !any(duplicated(sample_ids)),
+        "sample_ids must have equal length" = length(unique(nchar(sample_ids))) == 1,
+        "verbose must be logical" = is.logical(verbose)
+    )
+
+    if (verbose) {
+        message(sprintf("Processing %d sample IDs of length %d", 
+                       length(sample_ids), nchar(sample_ids[1])))
+    }
+    # Find positions where values differ
+    id_matrix <- do.call(rbind, strsplit(sample_ids, ""))
+    diff_positions <- which(apply(id_matrix, 2, function(x) length(unique(x)) > 1))
+
+    if (verbose) {
+        message(sprintf("Found differences at positions: %s", 
+                       paste(diff_positions, collapse = ", ")))
+    }
+    
+    # Get minimal required positions
+    min_pos <- min(diff_positions)
+    max_pos <- max(diff_positions)
+    
+    # Extract minimal substring that ensures uniqueness
+    short_ids <- substr(sample_ids, min_pos, max_pos + 1)
+
+    if (verbose) {
+        message(sprintf("Reduced sample IDs from %d to %d digits", 
+                       nchar(sample_ids[1]), nchar(short_ids[1])))
+        message(sprintf("Using positions %d to %d", min_pos, max_pos + 1))
+    }
+    
+    # Verify uniqueness of result
+    if (any(duplicated(short_ids))) {
+        stop("Failed to create unique short identifiers")
+    }
+    
+    return(short_ids)
+}
+
+generate_distinct_colors <- function(n) {
+    # RColorBrewer provides good distinct colors
+    if (n <= 8) {
+        RColorBrewer::brewer.pal(max(3, n), "Set2")[1:n]
+    } else {
+        # For more categories, use rainbow with better spacing
+        rainbow(n, s = 0.7, v = 0.9)
+    }
+}
+
+# Function to find matching control sample
+find_control_sample <- function(experimental_sample, metadata, control_factors) {
+    # Create matching conditions for control
+    control_conditions <- lapply(control_factors$genotype, function(factor) {
+        metadata[[factor]] == experimental_sample[[factor]]
+    })
+    
+    # Combine conditions with Input antibody requirement
+    control_conditions$is_input <- metadata$antibody == "Input"
+    
+    # Find matching control samples
+    control_matches <- Reduce(`&`, control_conditions)
+    
+    if (sum(control_matches) == 0) {
+        if (DEBUG_CONFIG$verbose) {
+            message("No matching control found for sample: ", 
+                   experimental_sample$sample_id)
+        }
+        return(NULL)
+    }
+    
+    # Return first matching control
+    metadata[control_matches, ][1, ]
+}
+
+#' @title Calculate Global Y-limits for Bigwig Tracks
+#' @description Calculates global y-axis limits from multiple bigwig files with padding
+#' @param bigwig_files character vector of bigwig file paths
+#' @param genome_range GRanges object specifying genomic region
+#' @param padding_fraction numeric Fraction for range padding (default: 0.1)
+#' @param verbose logical Print processing information
+#' @return list containing {success, data, error} where data has y_limits
+#' @importFrom rtracklayer import
+#' @importFrom GenomicRanges values
+calculate_track_limits <- function(bigwig_files, genome_range, 
+                                 padding_fraction = 0.1, verbose = FALSE) {
+    result <- tryCatch({
+        # Input validation
+        stopifnot(
+            "bigwig_files must be character vector" = is.character(bigwig_files),
+            "bigwig_files cannot be empty" = length(bigwig_files) > 0,
+            "genome_range must be GRanges" = inherits(genome_range, "GRanges"),
+            "padding_fraction must be numeric" = is.numeric(padding_fraction),
+            "padding_fraction must be between 0 and 1" = 
+                padding_fraction >= 0 && padding_fraction <= 1,
+            "verbose must be logical" = is.logical(verbose)
+        )
+        
+        if (verbose) {
+            message(sprintf("\nProcessing %d bigwig files for y-limits...", 
+                          length(bigwig_files)))
+        }
+        
+        # Initialize storage for values
+        all_track_values <- c()
+        processed_files <- 0
+        skipped_files <- 0
+        
+        # Process each bigwig file
+        for (bigwig_file in bigwig_files) {
+            if (file.exists(bigwig_file)) {
+                tryCatch({
+                    # Import data for specific chromosome
+                    track_data <- rtracklayer::import(
+                        bigwig_file,
+                        which = genome_range
+                    )
+                    
+                    if (length(track_data) > 0) {
+                        values <- GenomicRanges::values(track_data)$score
+                        if (length(values) > 0) {
+                            all_track_values <- c(all_track_values, values)
+                            processed_files <- processed_files + 1
+                        }
+                    }
+                }, error = function(e) {
+                    if (verbose) {
+                        message("Skipping ", basename(bigwig_file), ": ", e$message)
+                    }
+                    skipped_files <- skipped_files + 1
+                })
+            } else {
+                if (verbose) {
+                    message("File not found: ", basename(bigwig_file))
+                }
+                skipped_files <- skipped_files + 1
+            }
+        }
+        
+        # Calculate limits if we have values
+        if (length(all_track_values) > 0) {
+            y_min <- min(all_track_values, na.rm = TRUE)
+            y_max <- max(all_track_values, na.rm = TRUE)
+            y_range <- y_max - y_min
+            y_limits <- c(
+                y_min - (y_range * padding_fraction),
+                y_max + (y_range * padding_fraction)
+            )
+            
+            if (verbose) {
+                message(sprintf("Processed %d files successfully", processed_files))
+                message(sprintf("Skipped %d files", skipped_files))
+                message(sprintf("Y-limits: [%.2f, %.2f]", y_limits[1], y_limits[2]))
+            }
+            
+            list(
+                success = TRUE,
+                data = y_limits,
+                error = NULL
+            )
+        } else {
+            if (verbose) {
+                message("No valid track data found for y-limit calculation")
+            }
+            
+            list(
+                success = FALSE,
+                data = NULL,
+                error = "No valid track data found"
+            )
+        }
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+
+#' @title Create Track Labels from Sample Metadata
+#' @description Creates labels based on sample categories with configurable display rules
+#' @param samples data.frame Sample metadata
+#' @param categories character vector Categories to consider for labels
+#' @param always_show character vector Categories to always show
+#' @param never_show character vector Categories to never show
+#' @param separator character String to use between label components
+#' @param verbose logical Print processing information
+create_track_labels <- function(samples, 
+                              categories = NULL,  # Now optional
+                              always_show = c("antibody"),
+                              never_show = c("sample_id", "full_name", "short_name", "X__cf_genotype"),
+                              separator = " - ",
+                              verbose = FALSE) {
+    result <- tryCatch({
+        # Input validation
+        stopifnot(
+            "samples must be data.frame" = is.data.frame(samples),
+            "always_show must be character" = is.character(always_show),
+            "never_show must be character" = is.character(never_show),
+            "separator must be character" = is.character(separator),
+            "always_show categories must exist in samples" = 
+                all(always_show %in% colnames(samples))
+        )
+        
+        # If categories not provided, use all columns except never_show
+        if (is.null(categories)) {
+            categories <- setdiff(colnames(samples), never_show)
+            if (verbose) {
+                message("Using all available categories except: ", 
+                       paste(never_show, collapse = ", "))
+            }
+        } else {
+            stopifnot("categories must be character" = is.character(categories))
+            # Remove any never_show categories if present
+            categories <- setdiff(categories, never_show)
+        }
+        
+        if (verbose) {
+            message("Analyzing categories: ", paste(categories, collapse = ", "))
+        }
+        
+        # Find categories with varying values
+        varying_categories <- categories[sapply(categories, function(cat) {
+            length(unique(samples[[cat]])) > 1
+        })]
+        
+        if (verbose) {
+            message("Found varying categories: ", 
+                   paste(varying_categories, collapse = ", "))
+        }
+        
+        # Combine always_show with varying categories, ensuring order
+        label_categories <- unique(c(
+            always_show,  # Always first
+            intersect(varying_categories, categories)  # Then varying
+        ))
+        
+        # Remove any never_show categories that might have slipped through
+        label_categories <- setdiff(label_categories, never_show)
+        
+        if (verbose) {
+            message("Final label categories: ", 
+                   paste(label_categories, collapse = ", "))
+        }
+        
+        # Create category information
+        category_info <- list(
+            distinguishing = label_categories,
+            varying = varying_categories,
+            always_shown = always_show,
+            never_shown = never_show,
+            all_available = colnames(samples),
+            used_separator = separator
+        )
+        
+        # Create labels
+        labels <- apply(samples, 1, function(row) {
+            # Get values for each category
+            values <- sapply(label_categories, function(cat) {
+                value <- row[[cat]]
+                # Clean up NA values
+                if (is.na(value)) return("NA")
+                as.character(value)
+            })
+            
+            # Remove empty or NA values
+            values <- values[values != "" & values != "NA"]
+            
+            # Combine with separator
+            paste(values, collapse = separator)
+        })
+        
+        if (verbose) {
+            message("\nLabel Summary:")
+            message("- Total labels: ", length(labels))
+            message("- Unique labels: ", length(unique(labels)))
+            message("- Example label: ", labels[1])
+        }
+        
+        list(
+            success = TRUE,
+            data = list(
+                labels = labels,
+                categories = category_info
+            ),
+            error = NULL
+        )
+    }, error = function(e) {
+        list(
+            success = FALSE,
+            data = NULL,
+            error = e$message
+        )
+    })
+    
+    return(result)
+}
+
+#' @title Create Consistent Color Scheme
+#' @description Generates and manages consistent colors for track visualization
+#' @param config list Configuration including fixed colors
+#' @param categories list Named list of category values requiring colors
+#' @return list Color assignments and management functions
+create_color_scheme <- function(config, categories, verbose = FALSE) {
+    # Input validation
+    stopifnot(
+        "config must be a list" = is.list(config),
+        "categories must be a list" = is.list(categories),
+        "config must contain placeholder color" = !is.null(config$placeholder),
+        "config must contain input color" = !is.null(config$input),
+        "verbose must be logical" = is.logical(verbose)
+    )
+    
+    # Initialize fixed colors
+    fixed_colors <- list(
+        placeholder = config$placeholder,
+        input = config$input
+    )
+    
+    if (verbose) {
+        message("Initializing color scheme...")
+        message("Fixed colors:")
+        message("- Placeholder: ", fixed_colors$placeholder)
+        message("- Input: ", fixed_colors$input)
+    }
+    
+    # Generate category-specific colors
+    category_colors <- list()
+    
+    for (category_name in names(categories)) {
+        # Get unique values for current category
+        category_values <- categories[[category_name]]
+        color_count <- length(category_values)
+        
+        # Set consistent seed for reproducibility
+        category_seed <- sum(utf8ToInt(category_name))
+        set.seed(category_seed)
+        
+        # Generate colors based on category size
+        if (color_count <= 8) {
+            category_palette <- RColorBrewer::brewer.pal(max(3, color_count), "Set2")
+            category_colors_vector <- category_palette[seq_len(color_count)]
+        } else {
+            category_colors_vector <- rainbow(color_count, s = 0.7, v = 0.9)
+        }
+        
+        # Assign names to colors
+        names(category_colors_vector) <- category_values
+        category_colors[[category_name]] <- category_colors_vector
+        
+        if (verbose) {
+            message(sprintf("\nColors for category '%s':", category_name))
+            for (value in category_values) {
+                message(sprintf("- %s: %s", value, category_colors[[category_name]][value]))
+            }
+        }
+    }
+    
+    # Create color getter function
+    get_color <- function(category, value) {
+        if (category == "antibody" && value == "Input") {
+            return(fixed_colors$input)
+        }
+        
+        if (!category %in% names(category_colors)) {
+            if (verbose) {
+                message(sprintf("Category '%s' not found, using placeholder", category))
+            }
+            return(fixed_colors$placeholder)
+        }
+        
+        if (!value %in% names(category_colors[[category]])) {
+            if (verbose) {
+                message(sprintf("Value '%s' not found in category '%s', using placeholder",
+                              value, category))
+            }
+            return(fixed_colors$placeholder)
+        }
+        
+        return(category_colors[[category]][value])
+    }
+    
+    # Create output structure
+    color_scheme <- list(
+        fixed = fixed_colors,
+        categories = category_colors,
+        get_color = get_color
+    )
+    
+    return(color_scheme)
+}
+
+#' @title Create Plot Title Using Category Information
+create_plot_title <- function(metadata, comparison_name, plot_info, 
+                            label_result = NULL, mode = "development") {
+    # Input validation
+    stopifnot(
+        "metadata must be data.frame" = is.data.frame(metadata),
+        "comparison_name must be character" = is.character(comparison_name),
+        "plot_info must be list" = is.list(plot_info),
+        "mode must be development or publication" = mode %in% c("development", "publication"),
+        "label_result must contain required components" = is.list(label_result) &&       all(c("labels", "categories") %in% names(label_result$data))
+    )
+    
+    # Use existing label information
+    distinguishing_cats <- label_result$data$categories$distinguishing
+    varying_cats <- label_result$data$categories$varying
+    all_cats <- label_result$data$categories$all_available
+    
+    # Find shared characteristics using existing categorization
+    shared_columns <- setdiff(all_cats, c(distinguishing_cats, "sample_id"))
+    # Get shared values
+    shared_values <- sapply(shared_columns, function(col) {
+        values <- unique(metadata[[col]])
+        if (length(values) == 1) {
+            sprintf("%s: %s", col, values)
+        } else {
+            NULL
+        }
+    })
+    shared_values <- unlist(shared_values[!sapply(shared_values, is.null)])
+    
+    # Create title based on mode
+    
+    if (mode == "development") {
+        # Create columns with explicit line breaks and fixed width
+        col1 <- sprintf(
+            "%-35s\n%-35s\n%-35s\n%-35s\n%-35s",
+            sprintf("Experiment: %s", plot_info$experiment_id),
+            sprintf("Comparison: %s", comparison_name),
+            sprintf("Chromosome: %s", plot_info$chromosome),
+            sprintf("Samples: %d", nrow(metadata)),
+            sprintf("Norm: %s", plot_info$normalization)
+        )
+        
+        # Format shared properties
+        shared_props <- sapply(shared_categories, function(col) {
+            values <- unique(metadata[[col]])
+            if (length(values) == 1) {
+                sprintf("%s: %s", col, values[1])
+            }
+        })
+        shared_props <- shared_props[!sapply(shared_props, is.null)]
+        
+        col2 <- sprintf(
+            "Shared Properties:\n%s",
+            paste(strwrap(paste(shared_props, collapse = ", "), width = 35), 
+                  collapse = "\n")
+        )
+        
+        col3 <- sprintf(
+            "%-35s\n%-35s",
+            format(Sys.time(), "%Y-%m-%d %H:%M"),
+            sprintf("Y-range: [%.2f, %.2f]", plot_info$y_limits[1], plot_info$y_limits[2])
+        )
+        
+        # Combine with explicit column separators
+        title <- paste(
+            col1,
+            "|",
+            col2,
+            "|",
+            col3,
+            sep = "  "
+        )
+    } else {
+        title <- sprintf(
+            "%s: %s\nChr %s",
+            plot_info$experiment_id,
+            sub("^comp_", "", comparison_name),
+            plot_info$chromosome
+        )
+    }
+    
+    # Format title according to configuration
+    formatted_title <- format_title_text(
+        title,
+        max_width = PLOT_CONFIG$title$format$max_width,
+        max_lines = PLOT_CONFIG$title$format$max_lines
+    )
+    
+    return(formatted_title)
+}
+
+format_title_text <- function(text, max_width = 40, max_lines = NULL) {
+    # Input validation
+    stopifnot(
+        "text must be character" = is.character(text),
+        "max_width must be positive number" = is.numeric(max_width) && max_width > 0,
+        "max_lines must be NULL or positive number" = 
+            is.null(max_lines) || (is.numeric(max_lines) && max_lines > 0)
+    )
+    
+    # Split into lines
+    lines <- strsplit(text, "\n")[[1]]
+    
+    # Truncate or wrap each line
+    formatted_lines <- sapply(lines, function(line) {
+        if (nchar(line) > max_width) {
+            paste0(substr(line, 1, max_width - 3), "...")
+        } else {
+            line
+        }
+    })
+    
+    # Limit number of lines if specified
+    if (!is.null(max_lines) && length(formatted_lines) > max_lines) {
+        formatted_lines <- c(
+            formatted_lines[1:(max_lines - 1)],
+            "..."
+        )
+    }
+    
+    paste(formatted_lines, collapse = "\n")
+}
+## Create color mapping for antibodies
+#unique_antibodies <- unique(sorted_metadata$antibody)
+#antibody_colors <- generate_distinct_colors(length(unique_antibodies))
+#names(antibody_colors) <- unique_antibodies
+#
+## Update PLOT_CONFIG with dynamic colors
+#PLOT_CONFIG$track_colors <- list(
+#    antibody = antibody_colors,
+#    placeholder = PLOT_CONFIG$placeholder_color  # Maintain consistent placeholder
+#)
+
+# Create color legend text
+#legend_text <- sprintf(
+#    "Track Colors:\n%s\n%s",
+#    paste("?", names(PLOT_CONFIG$track_colors$antibody), 
+#          sprintf("(%s)", PLOT_CONFIG$track_colors$antibody), 
+#          collapse = "\n"),
+#    sprintf("? No Data (%s)", PLOT_CONFIG$placeholder_color)
+#)
diff --git a/failsafe_scripts/functions_for_plotting_utilities.R b/failsafe_scripts/functions_for_plotting_utilities.R
new file mode 100644
index 0000000..1de68f8
--- /dev/null
+++ b/failsafe_scripts/functions_for_plotting_utilities.R
@@ -0,0 +1,326 @@
+#' Validate plot file search parameters
+#' @param base_dir character Base directory to search
+#' @param experiment character Optional experiment ID
+#' @param timestamp character Optional timestamp
+#' @param pattern character Optional file pattern
+#' @param patterns list Required patterns for validation
+#' @param verbose logical Print processing details
+validate_find_plot_files_parameters <- function(base_dir, experiment, timestamp, pattern, 
+                                   validation_patterns, additional_filtering_patterns, verbose) {
+    # Basic type checking
+    stopifnot(
+        "base_dir must be character" = is.character(base_dir),
+        "base_dir must exist" = dir.exists(base_dir),
+        "patterns must be a list" = is.list(validation_patterns),
+        "required patterns missing" = all(c("experiment", "timestamp", "svg") %in% names(validation_patterns)),
+        "verbose must be logical" = is.logical(verbose)
+    )
+    
+    # Optional parameter validation
+    if (!is.null(experiment)) {
+        stopifnot(
+            "experiment must be character" = is.character(experiment),
+            "experiment must match pattern" = grepl(validation_patterns$experiment, experiment)
+        )
+    }
+    
+    if (!is.null(timestamp)) {
+        stopifnot(
+            "timestamp must be character" = is.character(timestamp),
+            "timestamp must match format" = grepl(validation_patterns$timestamp, timestamp)
+        )
+    }
+    
+    if (!is.null(additional_filtering_patterns)) {
+        # Check if single pattern or list of patterns
+        if (is.character(additional_filtering_patterns)) {
+            stopifnot("pattern must be non-empty" = nchar(additional_filtering_patterns) > 0)
+        } else if (is.list(additional_filtering_patterns)) {
+            stopifnot(
+                "all patterns must be character" = all(sapply(additional_filtering_patterns, is.character)),
+                "all patterns must be non-empty" = all(sapply(additional_filtering_patterns, nchar) > 0)
+            )
+        } else {
+            stop("additional_filtering_patterns must be character or list of characters")
+        }
+    }
+    if (!is.null(pattern)) {
+        stopifnot(
+            "pattern must be character" = is.character(pattern)
+        )
+    }
+}
+
+#' Find plot files based on criteria
+#' @param base_dir character Base directory to search
+#' @param experiment character Optional experiment ID
+#' @param timestamp character Optional timestamp
+#' @param pattern character Optional file pattern
+#' @param verbose logical Print processing details
+find_plot_files <- function(base_dir, patterns, experiment = NULL, 
+                          timestamp = NULL, pattern = NULL, additional_filtering_patterns = NULL, verbose = FALSE) {
+    # Validate inputs
+    validate_find_plot_files_parameters(
+        base_dir = base_dir,
+        experiment = experiment,
+        timestamp = timestamp,
+        pattern = pattern,
+        validation_patterns = patterns,
+        additional_filtering_patterns = additional_filtering_patterns,
+        verbose = verbose
+    )
+    
+    # Start with all SVG files
+    files <- base::list.files(
+        path = base_dir,
+        pattern = patterns$svg,
+        recursive = TRUE,
+        full.names = TRUE
+    )
+    
+    if (verbose) {
+        base::message(sprintf("Found %d SVG files", length(files)))
+    }
+    
+    # Filter by experiment
+    if (!is.null(experiment)) {
+        files <- files[base::grepl(experiment, base::basename(files))]
+        if (verbose) {
+            base::message(sprintf("After experiment filter: %d files", length(files)))
+        }
+    }
+    
+    # Filter by timestamp
+    if (!is.null(timestamp)) {
+        files <- files[base::grepl(timestamp, base::basename(files))]
+        if (verbose) {
+            base::message(sprintf("After timestamp filter: %d files", length(files)))
+        }
+    }
+    
+    # Filter by additional pattern
+    if (!is.null(pattern)) {
+        files <- files[base::grepl(pattern, base::basename(files))]
+        if (verbose) {
+            base::message(sprintf("After pattern filter: %d files", length(files)))
+        }
+    }
+    # Filter by additional patterns
+    if (!is.null(additional_filtering_patterns)) {
+        if (is.character(additional_filtering_patterns)) {
+            additional_filtering_patterns <- list(additional_filtering_patterns)  # Convert single pattern to list
+        }
+        
+        # Apply all patterns
+        files <- Reduce(function(files, pattern) {
+            files[base::grepl(pattern, base::basename(files))]
+        }, additional_filtering_patterns, init = files)
+        
+        if (verbose) {
+            base::message(sprintf("After additional pattern filters: %d files", length(files)))
+        }
+    }
+
+    return(files)
+}
+
+#' Create command configuration structure
+#' @description Define available commands and their descriptions
+create_viewer_commands <- function() {
+    return(list(
+        "PRESS ENTER" = "next plot",
+        "p" = "previous plot",
+        "q" = "quit viewer",
+        "h" = "show this help",
+        "i" = "show current plot info",
+        "l" = "list all plots",
+        "g" = "go to specific plot number"
+    ))
+}
+
+#' Validate commands configuration
+#' @param commands list Command configuration
+validate_commands <- function(commands) {
+    # Basic structure validation
+    stopifnot(
+        "commands must be a list" = is.list(commands),
+        "commands cannot be empty" = length(commands) > 0
+    )
+    
+    # Validate contents
+    invalid_names <- !sapply(names(commands), is.character)
+    invalid_desc <- !sapply(commands, is.character)
+    
+    if (any(invalid_names)) {
+        stop("All command names must be character strings")
+    }
+    
+    if (any(invalid_desc)) {
+        stop("All command descriptions must be character strings")
+    }
+    
+    # Validate command name format (optional)
+    if (any(nchar(names(commands)) > 1)) {
+        warning("Some command names are longer than one character")
+    }
+}
+
+#' Display help information
+#' @param commands list Command configuration
+#' @param verbose logical Print additional information
+display_help <- function(commands, verbose = FALSE) {
+
+    # Validation
+    validate_commands(commands)
+
+    if (verbose) {
+        base::message("\nViewer Commands:")
+    }
+    
+    # Calculate maximum command length for alignment
+    max_cmd_length <- max(nchar(names(commands)))
+    
+    # Create format string for aligned output
+    format_string <- sprintf("  %%-%ds : %%s", max_cmd_length)
+    
+    # Display commands
+    for (cmd in names(commands)) {
+        # Handle empty command (Enter key) specially
+        display_cmd <- if (cmd == "") "[Enter]" else cmd
+        base::message(sprintf(format_string, display_cmd, commands[[cmd]]))
+    }
+}
+
+#' Validate display parameters
+#' @param files character vector of file paths
+#' @param device_config list Device configuration settings
+validate_display_parameters <- function(files, device_config) {
+    stopifnot(
+        "files must be character vector" = is.character(files),
+        "device_config must be list" = is.list(device_config),
+        "device dimensions must be numeric" = is.numeric(device_config$width) && is.numeric(device_config$height)
+    )
+    
+    # Validate file existence
+    missing_files <- files[!file.exists(files)]
+    if (length(missing_files) > 0) {
+        base::warning(
+            sprintf("Missing files:\n%s", 
+                    paste(basename(missing_files), collapse = "\n"))
+        )
+    }
+}
+
+# Updated display_plots function
+display_plots <- function(files, device_config, interactive = TRUE, 
+                         display_time = 2, verbose = FALSE) {
+    # Validate inputs
+    validate_display_parameters(files, device_config)
+    
+    if (length(files) == 0) {
+        base::message("No files to display")
+        return(base::invisible(NULL))
+    }
+
+    commands <- create_viewer_commands()
+
+    if(interactive) {
+        base::message("\nStarting interactive plot viewer")
+        display_help(commands, verbose = TRUE)
+    }
+    
+    i <- 1
+    while (i <= length(files)) {
+        file <- files[i]
+        
+        if (!file.exists(file)) {
+            base::message(sprintf("Skipping missing file: %s", basename(file)))
+            i <- i + 1
+            next
+        }
+        
+        if (verbose) {
+            base::message(sprintf("\nDisplaying file %d of %d:", i, length(files)))
+            base::message(base::basename(file))
+        }
+        
+        tryCatch({
+            # Read and display SVG
+            img <- magick::image_read_svg(file)
+            plot(img)
+            
+            # Interactive viewing control
+            if (interactive) {
+                user_input <- base::readline(
+                    prompt = sprintf("Plot %d/%d [h for help]: ", 
+                                   i, length(files))
+                )
+
+                # Process user input
+                if (!user_input %in% c("", names(commands))) {
+                    base::message(sprintf("Invalid command: '%s'", user_input))
+                    display_help(commands, verbose = TRUE)
+                    next
+                }
+                
+                # Process user input
+                if (user_input == "q") {
+                    break
+                } else if (user_input == "h") {
+                    display_help(commands, verbose = TRUE)
+                    next  # Stay on current plot
+                } else if (user_input == "p" && i > 1) {
+                    i <- i - 1
+                } else if (user_input == "i") {
+                    base::message(sprintf("\nCurrent plot: %s", basename(file)))
+                    next  # Stay on current plot
+                } else if (user_input == "l") {
+                    base::message("\nAvailable plots:")
+                    for (idx in seq_along(files)) {
+                        base::message(sprintf("%3d: %s", 
+                                            idx, basename(files[idx])))
+                    }
+                    next  # Stay on current plot
+                } else if (user_input == "g") {
+                    base::message("\nAvailable plots:")
+                    for (idx in seq_along(files)) {
+                        base::message(sprintf("%3d: %s", 
+                                            idx, basename(files[idx])))
+                    }
+                    
+                    # Get plot number from user
+                    plot_num <- base::readline(
+                        prompt = sprintf("Enter plot number (1-%d): ", length(files))
+                    )
+                    
+                    # Validate and convert input
+                    plot_num <- tryCatch({
+                        num <- as.integer(plot_num)
+                        if (is.na(num) || num < 1 || num > length(files)) {
+                            base::message(sprintf("Invalid plot number. Must be between 1 and %d", 
+                                                length(files)))
+                            i  # Keep current position
+                        } else {
+                            num  # Use provided number
+                        }
+                    }, error = function(e) {
+                        base::message("Invalid input. Please enter a number.")
+                        i  # Keep current position
+                    })
+                    
+                    i <- plot_num
+                    next
+                } else {
+                    i <- i + 1
+                }
+            } else {
+                # If not interactive, wait display time and continue.
+                base::Sys.sleep(display_time)
+                i <- i + 1
+            }
+        }, error = function(e) {
+            base::message(sprintf("Error displaying file: %s", e$message))
+            i <- i + 1  # Move to next file even if current fails
+        })
+    }
+}
diff --git a/failsafe_scripts/generate_genome_track_plots_for_all_experiment_comparisons.R b/failsafe_scripts/generate_genome_track_plots_for_all_experiment_comparisons.R
new file mode 100644
index 0000000..11f41cf
--- /dev/null
+++ b/failsafe_scripts/generate_genome_track_plots_for_all_experiment_comparisons.R
@@ -0,0 +1,687 @@
+#!/usr/bin/env Rscript
+# Configuration
+#-----------------------------------------------------------------------------
+DEBUG_CONFIG <- list(
+    enabled = TRUE,           # TRUE for testing single comparison
+    comparison = "comp_1108forNoneAndWT",  # Which comparison to process in debug mode
+    save_plots = FALSE,        # Whether to save plots to files
+    verbose = TRUE,           # Print debug information
+    chromosome = 10,
+    interactive = FALSE,
+    validate_config = TRUE,
+    display_time = 2
+)
+
+# Time formatting configuration
+TIME_CONFIG <- list(
+    timestamp_format = "%Y%m%d_%H%M%S",  # YYYYMMDD_HHMMSS
+    date_format = "%Y%m%d"               # YYYYMMDD
+)
+
+# Generate timestamps once at script start
+TIMESTAMPS <- list(
+    full = format(Sys.time(), TIME_CONFIG$timestamp_format),
+    date = format(Sys.Date(), TIME_CONFIG$date_format)
+)
+
+PLOT_CONFIG <- list(
+    # Basic plot settings
+    dimensions = list(
+        width = 10,
+        height = 8
+    ),
+    
+    # Track configuration
+    tracks = list(
+        # Visual settings for all tracks
+        display = list(
+            width = 0.9,
+            fontface = 1,
+            cex = 0.6,
+            background = "white",
+            fontcolor = "black",
+            border_color = "#E0E0E0"
+        ),
+        
+        # Track-specific colors
+        colors = list(
+            placeholder = "#cccccc",
+            input = "#808080"
+        ),
+        
+        # Track name formatting
+        names = list(
+            format = "%s: %s",
+            control_format = "%s: %s - %s",
+            placeholder_suffix = "(No data)"
+        )
+    ),
+    
+    # Main title configuration
+    main_title = list(
+        mode = "development",  # or "publication"
+        development = list(
+            format = paste(
+                "%s",
+                "Comparison: %s",
+                "Chromosome %s (%d samples)",
+                "%s",
+                "Normalization: %s",
+                sep = "\n"
+            ),
+            cex = 0.7,      # Size specific to main title
+            fontface = 2    # Bold for main title
+        ),
+        publication = list(
+            format = "%s: Chr%s (%s)",
+            cex = 1,
+            fontface = 2
+        ),
+        format = list(
+            max_width = 40,
+            max_lines = 5
+        )
+    )
+)
+
+# Load required packages
+#-----------------------------------------------------------------------------
+required_packages <- c("rtracklayer", "GenomicRanges", "Gviz")
+for (pkg in required_packages) {
+    if (!requireNamespace(pkg, quietly = TRUE)) {
+        stop(sprintf("Package '%s' is missing", pkg))
+    }
+}
+
+#source("~/lab_utils/failsafe_scripts/all_functions.R")
+source("~/lab_utils/failsafe_scripts/functions_for_genome_tracks.R")
+source("~/lab_utils/failsafe_scripts/bmc_config.R")
+if (DEBUG_CONFIG$validate_config) {
+    if (!exists("EXPERIMENT_CONFIG") ||
+        !("COMPARISONS" %in% names(EXPERIMENT_CONFIG))) {
+        stop("EXPERIMENT_CONFIG must contain COMPARISONS element")
+    }
+
+    if (DEBUG_CONFIG$verbose) {
+        message("Found ", length(EXPERIMENT_CONFIG$COMPARISONS),
+                " comparisons in configuration")
+    }
+}
+
+
+# Load metadata and files
+#-----------------------------------------------------------------------------
+#experiment_id <- "241007Bel"
+experiment_id <- "241010Bel"
+base_dir <- file.path(Sys.getenv("HOME"), "data", experiment_id)
+plots_dir <- file.path(base_dir, "plots", "genome_tracks", "comparisons")
+metadata_path <- file.path(base_dir, "documentation",
+                          paste0(experiment_id, "_sample_grid.csv"))
+dir.create(plots_dir, recursive = TRUE, showWarnings = FALSE)
+
+# 2. Find fastq files and extract sample IDs
+fastq_files <- list.files(
+    path = file.path(base_dir, "fastq"),
+    pattern = "consolidated_.*_sequence\\.fastq$",
+    full.names = FALSE
+)
+
+if (length(fastq_files) == 0) {
+    stop("No fastq files found in specified directory")
+}
+
+# Extract sample IDs from fastq filenames
+sample_ids <- gsub(
+    pattern = "consolidated_([0-9]{5,6})_sequence\\.fastq",
+    replacement = "\\1",
+    x = fastq_files
+)
+
+# 3. Load and process metadata
+metadata <- read.csv(metadata_path, stringsAsFactors = FALSE)
+
+# 4. Enforce factor levels from config
+for (col_name in names(EXPERIMENT_CONFIG$CATEGORIES)) {
+    if (col_name %in% colnames(metadata)) {
+        metadata[[col_name]] <- factor(
+            metadata[[col_name]],
+            levels = EXPERIMENT_CONFIG$CATEGORIES[[col_name]],
+            ordered = TRUE
+        )
+    }
+}
+
+# 5. Sort metadata using config column order
+sorted_metadata <- metadata[do.call(
+    order,
+    metadata[EXPERIMENT_CONFIG$COLUMN_ORDER]
+), ]
+
+# 6. Add sample IDs to metadata
+sorted_metadata$sample_id <- sample_ids
+
+if (DEBUG_CONFIG$verbose) {
+    message("Metadata processing summary:")
+    message(sprintf("Found %d fastq files", length(fastq_files)))
+    message(sprintf("Processed %d metadata rows", nrow(sorted_metadata)))
+    message("Columns with enforced factors:")
+    print(names(EXPERIMENT_CONFIG$CATEGORIES))
+}
+
+# Create color scheme
+color_scheme <- create_color_scheme(
+    config = list(
+        placeholder = PLOT_CONFIG$tracks$colors$placeholder,
+        input = PLOT_CONFIG$tracks$colors$input
+    ),
+    categories = list(
+        antibody = unique(sorted_metadata$antibody),
+        rescue_allele = unique(sorted_metadata$rescue_allele)
+    )
+)
+
+if (DEBUG_CONFIG$verbose) {
+    message("\nColor scheme initialized:")
+    message("Fixed colors:")
+    print(color_scheme$fixed)
+    message("Category colors:")
+    print(color_scheme$categories)
+}
+
+# Load reference genome
+ref_genome_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "REFGENS"),
+    pattern = "S288C_refgenome.fna",
+    full.names = TRUE,
+    recursive = TRUE
+)[1]
+genome_data <- Biostrings::readDNAStringSet(ref_genome_file)
+
+
+# Create chromosome range
+chromosome_to_plot <- DEBUG_CONFIG$chromosome
+chromosome_width <- genome_data[chromosome_to_plot]@ranges@width
+chromosome_roman <- paste0("chr", utils::as.roman(chromosome_to_plot))
+
+genome_range <- GenomicRanges::GRanges(
+    seqnames = chromosome_roman,
+    ranges = IRanges::IRanges(start = 1, end = chromosome_width),
+    strand = "*"
+)
+
+# Find bigwig files
+bigwig_pattern <- sprintf("_%s\\.bw$", EXPERIMENT_CONFIG$NORMALIZATION$active)
+bigwig_files <- list.files(
+    file.path(base_dir, "coverage"),
+    pattern = bigwig_pattern,
+    full.names = TRUE
+)
+normalization_method <- sub(".*_([^_]+)\\.bw$", "\\1",
+                          basename(bigwig_files[1]))
+
+# Load feature file (annotation)
+pattern_for_feature_file <- "eaton_peaks"
+feature_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "feature_files"),
+    pattern = pattern_for_feature_file,
+    full.names = TRUE
+)[1]
+# Convert to title case and replace "_" with " "
+feature_track_name <- gsub(
+                        "_",
+                        " ",
+                        tools::toTitleCase(pattern_for_feature_file)
+                        )
+
+if (!is.null(feature_file)) {
+    features <- rtracklayer::import(feature_file)
+    # Convert to chrRoman format
+    GenomeInfoDb::seqlevels(features) <- paste0(
+        "chr",
+        utils::as.roman(gsub("chr", "", GenomeInfoDb::seqlevels(features)))
+    )
+}
+
+
+# Process Comparisons
+#-----------------------------------------------------------------------------
+# Determine which comparisons to process
+comparisons_to_process <- if (DEBUG_CONFIG$enabled) {
+    if (!DEBUG_CONFIG$comparison %in% names(EXPERIMENT_CONFIG$COMPARISONS)) {
+        stop("Debug comparison not found in EXPERIMENT_CONFIG$COMPARISONS")
+    }
+    DEBUG_CONFIG$comparison
+} else {
+    names(EXPERIMENT_CONFIG$COMPARISONS)
+}
+
+if (DEBUG_CONFIG$verbose) {
+    message("\nProcessing comparisons:")
+    message(sprintf("- Total comparisons: %d", length(comparisons_to_process)))
+    message("- Comparisons: ", paste(comparisons_to_process, collapse = ", "))
+}
+
+# Calculate global y-limits for all plots (before the plotting loop)
+#-----------------------------------------------------------------------------
+if (DEBUG_CONFIG$verbose) {
+    message("\nCalculating global range for all tracks...")
+}
+
+limits_result <- calculate_track_limits(
+    bigwig_files = bigwig_files,
+    genome_range = genome_range,
+    padding_fraction = 0.1,
+    verbose = DEBUG_CONFIG$verbose
+)
+
+if (!limits_result$success) {
+    warning("Failed to calculate y-limits: ", limits_result$error)
+    y_limits <- c(0, 1000)  # Default fallback
+} else {
+    y_limits <- limits_result$data
+}
+
+# Add after processing metadata but before track creation
+short_sample_ids <- create_minimal_identifiers(
+    sorted_metadata$sample_id,
+    verbose = DEBUG_CONFIG$verbose
+)
+
+# Create mapping between full and short IDs
+sample_id_mapping <- setNames(short_sample_ids, sorted_metadata$sample_id)
+
+# Process each comparison
+#-----------------------------------------------------------------------------
+for (comparison_name in comparisons_to_process) {
+    if (DEBUG_CONFIG$verbose) {
+        message(sprintf("\nProcessing comparison: %s", comparison_name))
+    }
+
+    # Get comparison expression and filter metadata
+    comparison_expression <- EXPERIMENT_CONFIG$COMPARISONS[[comparison_name]]
+    comparison_samples <- sorted_metadata[eval(comparison_expression,
+                                             envir = sorted_metadata), ]
+
+    label_result <- create_track_labels(
+        samples = comparison_samples,
+        always_show = "antibody",
+        never_show = c("sample_id", "full_name", "short_name", "X__cf_genotype"),
+        separator = "-",
+        verbose = TRUE
+    )
+
+    if (!label_result$success) {
+        warning("Failed to create track labels for comparison %s: %s", comparison_name, label_result$error)
+        track_labels <- comparison_samples$short_name  # Fallback to sample_id
+    } else {
+        track_labels <- label_result$data$labels
+    }
+
+    if (nrow(comparison_samples) == 0) {
+        warning(sprintf("No samples found for comparison: %s", comparison_name))
+        next
+    }
+
+    if (DEBUG_CONFIG$verbose) {
+        message(sprintf("Found %d samples for comparison", nrow(comparison_samples)))
+    }
+
+    # Initialize track list with genome axis
+    tracks <- list(
+        Gviz::GenomeAxisTrack(
+            name = paste("Chr ", chromosome_to_plot, " Axis", sep = "")
+        )
+    )
+
+    # Find and add control track (single control per comparison)
+    if (DEBUG_CONFIG$verbose) {
+        message("\nFinding control sample for comparison...")
+    }
+
+    # Try to find control for first available sample
+    control_sample <- find_control_sample(
+        comparison_samples[1, ],
+        sorted_metadata,
+        EXPERIMENT_CONFIG$CONTROL_FACTORS
+    )
+
+    # Add single control track (real or placeholder)
+    if (!is.null(control_sample)) {
+        control_bigwig <- bigwig_files[grepl(control_sample$sample_id,
+                                           bigwig_files)]
+        if (length(control_bigwig) > 0 && file.exists(control_bigwig[1])) {
+            if (DEBUG_CONFIG$verbose) {
+                message(sprintf("Adding control track: %s",
+                              control_sample$sample_id))
+            }
+
+            control_track_data <- rtracklayer::import(
+                control_bigwig[1],
+                which = genome_range
+            )
+
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                control_track_data,
+                name = sprintf(
+                    PLOT_CONFIG$tracks$names$control_format,
+                    sample_id_mapping[control_sample$sample_id],
+                    "Input",
+                    control_sample$rescue_allele
+                ),
+                type = "l",
+                col = color_scheme$fixed$input,
+                chromosome = chromosome_roman,
+                showTitle = TRUE,        # Explicitly show title
+                background.title = PLOT_CONFIG$tracks$display$background,
+                fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+                cex.title = PLOT_CONFIG$tracks$display$cex
+            )
+        } else {
+            if (DEBUG_CONFIG$verbose) {
+                message("Creating placeholder for control track")
+            }
+
+            # Create placeholder control track
+            empty_ranges <- GenomicRanges::GRanges(
+                seqnames = chromosome_roman,
+                ranges = IRanges::IRanges(
+                    start = seq(1, chromosome_width, length.out = 1000),
+                    width = 1
+                ),
+                score = rep(0, 1000),
+                strand = "*"
+            )
+
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                empty_ranges,
+                name = sprintf(
+                    "%s: %s - %s %s",
+                    control_sample$sample_id,
+                    "Input",
+                    control_sample$rescue_allele,
+                    PLOT_CONFIG$tracks$colors$placeholder
+                ),
+                type = "l",
+                col = color_scheme$fixed$placeholder,
+                chromosome = chromosome_roman,
+                showTitle = TRUE,        # Explicitly show title
+                background.title = PLOT_CONFIG$tracks$display$background,
+                fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+                cex.title = PLOT_CONFIG$tracks$display$cex
+            )
+        }
+    } else {
+        if (DEBUG_CONFIG$verbose) {
+            message("No matching control found, adding placeholder track")
+        }
+
+        # Create placeholder track when no control found
+        empty_ranges <- GenomicRanges::GRanges(
+            seqnames = chromosome_roman,
+            ranges = IRanges::IRanges(
+                start = seq(1, chromosome_width, length.out = 1000),
+                width = 1
+            ),
+            score = rep(0, 1000),
+            strand = "*"
+        )
+
+        tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+            empty_ranges,
+            name = sprintf("Input Control %s", PLOT_CONFIG$tracks$names$placeholder_suffix),
+            type = "l",
+            col = color_scheme$fixed$placeholder,
+            chromosome = chromosome_roman,
+            showTitle = TRUE,        # Explicitly show title
+            background.title = PLOT_CONFIG$tracks$display$background,
+            fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+            cex.title = PLOT_CONFIG$tracks$display$cex
+        )
+    }
+
+    # Add sample tracks
+    for (i in seq_len(nrow(comparison_samples))) {
+        sample_id <- comparison_samples$sample_id[i]
+        sample_bigwig <- bigwig_files[grepl(sample_id, bigwig_files)]
+        current_antibody <- comparison_samples$antibody[i]
+
+        track_name <- sprintf(
+            PLOT_CONFIG$tracks$names$format,
+            sample_id_mapping[sample_id],
+            track_labels[i]
+        )
+
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("Creating track: %s", track_name))
+        }
+
+        if (length(sample_bigwig) > 0 && file.exists(sample_bigwig[1])) {
+            if (DEBUG_CONFIG$verbose) {
+                message(sprintf("Adding track for sample: %s", sample_id))
+            }
+
+            # Update track color based on antibody
+            track_color <- if (current_antibody == "Input") {
+                color_scheme$fixed$input
+            } else {
+                color_scheme$get_color("antibody", current_antibody)
+            }
+
+            track_data <- rtracklayer::import(
+                sample_bigwig[1],
+                which = genome_range
+            )
+
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                track_data,
+                name = track_name,
+                type = "l",
+                col = track_color,
+                chromosome = chromosome_roman,
+                showTitle = TRUE,        # Explicitly show title
+                background.title = PLOT_CONFIG$tracks$display$background,
+                fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+                cex.title = PLOT_CONFIG$tracks$display$cex
+            )
+        } else {
+            if (DEBUG_CONFIG$verbose) {
+                message(sprintf("Creating placeholder for sample: %s", sample_id))
+            }
+
+            # Create placeholder track
+            empty_ranges <- GenomicRanges::GRanges(
+                seqnames = chromosome_roman,
+                ranges = IRanges::IRanges(
+                    start = seq(1, chromosome_width, length.out = 1000),
+                    width = 1
+                ),
+                score = rep(0, 1000),
+                strand = "*"
+            )
+
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                empty_ranges,
+                name = paste(track_name, PLOT_CONFIG$tracks$names$placeholder_suffix),
+                type = "l",
+                col = color_scheme$fixed$placeholder,
+                chromosome = chromosome_roman,
+                showTitle = TRUE,        # Explicitly show title
+                background.title = PLOT_CONFIG$tracks$display$background,
+                fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+                cex.title = PLOT_CONFIG$tracks$display$cex
+            )
+        }
+    }
+
+    # Add feature track if available
+    if (!is.null(features)) {
+        tracks[[length(tracks) + 1]] <- Gviz::AnnotationTrack(
+            features,
+            name = feature_track_name,
+            rotation.title = 0,
+            showTitle = TRUE,        # Explicitly show title
+            background.title = PLOT_CONFIG$tracks$display$background,
+            fontcolor.title = PLOT_CONFIG$tracks$display$fontcolor,
+            cex.title = PLOT_CONFIG$tracks$display$cex
+        )
+    }
+
+    # For creating plot title, use category information
+    distinguishing_categories <- label_result$data$categories$distinguishing
+    varying_categories <- label_result$data$categories$varying
+    all_categories <- label_result$data$categories$all_available
+
+
+    # Find shared characteristics
+    shared_categories <- setdiff(
+        all_categories,
+        c(distinguishing_categories, "sample_id")
+    )
+
+
+    # Create shared characteristics text
+    shared_values <- sapply(shared_categories, function(col) {
+        values <- unique(comparison_samples[[col]])
+        if (length(values) == 1) {
+            sprintf("%s: %s", col, values)
+        } else {
+            NULL
+        }
+    })
+    shared_values <- unlist(shared_values[!sapply(shared_values, is.null)])
+
+    # Debug output if needed
+    if (DEBUG_CONFIG$verbose) {
+        message("\nLabel Result Summary:")
+        message("- Distinguishing categories: ",
+                paste(distinguishing_categories, collapse = ", "))
+        message("- Varying categories: ",
+                paste(varying_categories, collapse = ", "))
+        message("- Shared categories: ",
+                paste(shared_categories, collapse = ", "))
+        message("- Number of labels: ", length(track_labels))
+    }
+
+    # Create plot title
+    #-----------------------------------------------------------------------------
+    # Get title configuration for current mode
+    main_title_config <- PLOT_CONFIG$main_title[[PLOT_CONFIG$main_title$mode]]
+    track_config <- PLOT_CONFIG$tracks$display
+
+    plot_title <- sprintf(
+        main_title_config$format,
+        experiment_id,
+        sub("^comp_", "", comparison_name),
+        chromosome_to_plot,
+        nrow(comparison_samples),
+        format(Sys.time(), "%Y-%m-%d %H:%M"),
+        normalization_method
+    )
+
+    # Generate plot
+    #-----------------------------------------------------------------------------
+    if (DEBUG_CONFIG$verbose) {
+        message("\nGenerating visualization...")
+    }
+
+
+    Gviz::plotTracks(
+        trackList = tracks,
+        chromosome = chromosome_roman,
+        from = 1,
+        to = chromosome_width,
+        ylim = y_limits,
+        
+        # Main title settings
+        main = plot_title,
+        cex.main = main_title_config$cex,
+        fontface.main = main_title_config$fontface,
+        
+        # Track display settings
+        title.width = track_config$width,
+        fontface.title = track_config$fontface,
+        cex.title = track_config$cex,
+        background.title = track_config$background,
+        fontcolor = track_config$fontcolor,
+        col.border.title = track_config$border_color,
+        showTitle = TRUE,
+        
+        # Other parameters
+        margin = 15,
+        innerMargin = 5,
+        col.axis = "black",
+        cex.axis = 0.8
+    )
+
+    # Save plot if configured
+    if (DEBUG_CONFIG$save_plots) {
+        plot_file <- file.path(
+            plots_dir,
+            sprintf(
+                "%s_%s_chr%s_%s.svg",
+                TIMESTAMPS$full,
+                experiment_id,
+                chromosome_to_plot,
+                sub("^comp_", "", comparison_name)
+            )
+        )
+
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("Saving plot to: %s", basename(plot_file)))
+        }
+
+        svg(plot_file,
+            width = PLOT_CONFIG$dimensions$width,
+            height = PLOT_CONFIG$dimensions$height)
+
+        Gviz::plotTracks(
+            trackList = tracks,
+            chromosome = chromosome_roman,
+            from = 1,
+            to = chromosome_width,
+            ylim = y_limits,
+            
+            # Main title settings
+            main = plot_title,
+            cex.main = main_title_config$cex,
+            fontface.main = main_title_config$fontface,
+            
+            # Track display settings
+            title.width = track_config$width,
+            fontface.title = track_config$fontface,
+            cex.title = track_config$cex,
+            background.title = track_config$background,
+            fontcolor = track_config$fontcolor,
+            col.border.title = track_config$border_color,
+            
+            # Other parameters
+            margin = 15,
+            innerMargin = 5,
+            col.axis = "black",
+            cex.axis = 0.8
+        )
+        dev.off()
+    }
+
+    # Interactive viewing options
+    if (DEBUG_CONFIG$interactive) {
+        user_input <- readline(
+            prompt = "Options: [Enter] next comparison, 's' skip rest, 'q' quit: "
+        )
+        if (user_input == "q") break
+        if (user_input == "s") DEBUG_CONFIG$save_plots <- FALSE
+    } else {
+        Sys.sleep(DEBUG_CONFIG$display_time)
+    }
+}
+
+
+# Final processing summary
+#-----------------------------------------------------------------------------
+if (DEBUG_CONFIG$verbose) {
+    message("\nProcessing complete")
+    message(sprintf("Processed %d comparisons", length(comparisons_to_process)))
+    if (DEBUG_CONFIG$save_plots) {
+        message(sprintf("Output directory: %s", plots_dir))
+    }
+}
diff --git a/failsafe_scripts/generate_genome_track_plots_for_all_samples.R b/failsafe_scripts/generate_genome_track_plots_for_all_samples.R
new file mode 100644
index 0000000..4679b8d
--- /dev/null
+++ b/failsafe_scripts/generate_genome_track_plots_for_all_samples.R
@@ -0,0 +1,413 @@
+#!/usr/bin/env Rscript
+# Configuration
+#-----------------------------------------------------------------------------
+DEBUG_CONFIG <- list(
+    enabled = FALSE,           # TRUE for testing single group, FALSE for all
+    group = 10,               # Which group to process when in debug mode
+    samples_per_group = 4,    # Samples per plot
+    save_plots = TRUE,       # Whether to save plots to files
+    verbose = TRUE,           # Print debug information
+    chromosome = 10,
+    interactive = FALSE,
+    display_time = 2
+)
+
+
+# Time formatting configuration
+TIME_CONFIG <- list(
+    timestamp_format = "%Y%m%d_%H%M%S",  # YYYYMMDD_HHMMSS
+    date_format = "%Y%m%d"               # YYYYMMDD
+)
+
+# Generate timestamps once at script start
+TIMESTAMPS <- list(
+    full = format(Sys.time(), TIME_CONFIG$timestamp_format),
+    date = format(Sys.Date(), TIME_CONFIG$date_format)
+)
+
+PLOT_CONFIG <- list(
+    width = 10,
+    height = 8,
+    placeholder_color = "#cccccc",
+    input_color ="#808080",
+    track_name_format = "%s: %s - %s",
+    placeholder_suffix = "(No data)",
+    title_format = "%s\nChromosome %s (%d samples)\n%s\nNormalization: %s"
+
+)
+
+
+# Load required packages
+#-----------------------------------------------------------------------------
+required_packages <- c("rtracklayer", "GenomicRanges", "Gviz")
+for (pkg in required_packages) {
+    if (!requireNamespace(pkg, quietly = TRUE)) {
+        stop(sprintf("Package '%s' is missing", pkg))
+    }
+}
+
+#source("~/lab_utils/failsafe_scripts/all_functions.R")
+source("~/lab_utils/failsafe_scripts/functions_for_genome_tracks.R")
+source("~/lab_utils/failsafe_scripts/bmc_config.R")
+# Load metadata and files
+#-----------------------------------------------------------------------------
+#experiment_id <- "241007Bel"
+experiment_id <- "241010Bel"
+base_dir <- file.path(Sys.getenv("HOME"), "data", experiment_id)
+plots_dir <- file.path(base_dir, "plots", "genome_tracks", "overview")
+metadata_path <- file.path(base_dir, "documentation",
+                          paste0(experiment_id, "_sample_grid.csv"))
+dir.create(plots_dir, recursive = TRUE, showWarnings = FALSE)
+
+# 2. Find fastq files and extract sample IDs
+fastq_files <- list.files(
+    path = file.path(base_dir, "fastq"),
+    pattern = "consolidated_.*_sequence\\.fastq$",
+    full.names = FALSE
+)
+
+if (length(fastq_files) == 0) {
+    stop("No fastq files found in specified directory")
+}
+
+# Extract sample IDs from fastq filenames
+sample_ids <- gsub(
+    pattern = "consolidated_([0-9]{5,6})_sequence\\.fastq",
+    replacement = "\\1",
+    x = fastq_files
+)
+
+# 3. Load and process metadata
+metadata <- read.csv(metadata_path, stringsAsFactors = FALSE)
+
+# 4. Enforce factor levels from config
+for (col_name in names(EXPERIMENT_CONFIG$CATEGORIES)) {
+    if (col_name %in% colnames(metadata)) {
+        metadata[[col_name]] <- factor(
+            metadata[[col_name]],
+            levels = EXPERIMENT_CONFIG$CATEGORIES[[col_name]],
+            ordered = TRUE
+        )
+    }
+}
+
+# 5. Sort metadata using config column order
+sorted_metadata <- metadata[do.call(
+    order,
+    metadata[EXPERIMENT_CONFIG$COLUMN_ORDER]
+), ]
+
+# 6. Add sample IDs to metadata
+sorted_metadata$sample_id <- sample_ids
+
+if (DEBUG_CONFIG$verbose) {
+    message("Metadata processing summary:")
+    message(sprintf("Found %d fastq files", length(fastq_files)))
+    message(sprintf("Processed %d metadata rows", nrow(sorted_metadata)))
+    message("Columns with enforced factors:")
+    print(names(EXPERIMENT_CONFIG$CATEGORIES))
+}
+# Create color scheme
+color_scheme <- create_color_scheme(
+    config = list(
+        placeholder = PLOT_CONFIG$placeholder_color,
+        input = PLOT_CONFIG$input_color
+    ),
+    categories = list(
+        antibody = unique(sorted_metadata$antibody),
+        rescue_allele = unique(sorted_metadata$rescue_allele)
+    )
+)
+
+if (DEBUG_CONFIG$verbose) {
+    message("\nColor scheme initialized:")
+    message("Fixed colors:")
+    print(color_scheme$fixed)
+    message("Category colors:")
+    print(color_scheme$categories)
+}
+
+
+# Load reference genome
+ref_genome_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "REFGENS"),
+    pattern = "S288C_refgenome.fna",
+    full.names = TRUE,
+    recursive = TRUE
+)[1]
+genome_data <- Biostrings::readDNAStringSet(ref_genome_file)
+
+
+# Create chromosome range
+chromosome_to_plot <- DEBUG_CONFIG$chromosome
+chromosome_width <- genome_data[chromosome_to_plot]@ranges@width
+chromosome_roman <- paste0("chr", utils::as.roman(chromosome_to_plot))
+
+genome_range <- GenomicRanges::GRanges(
+    seqnames = chromosome_roman,
+    ranges = IRanges::IRanges(start = 1, end = chromosome_width),
+    strand = "*"
+)
+
+# Find bigwig files
+bigwig_pattern <- sprintf("_%s\\.bw$", EXPERIMENT_CONFIG$NORMALIZATION$active)
+bigwig_files <- list.files(
+    file.path(base_dir, "coverage"),
+    pattern = bigwig_pattern,
+    full.names = TRUE
+)
+normalization_method <- sub(".*_([^_]+)\\.bw$", "\\1",
+                          basename(bigwig_files[1]))
+
+# Load feature file (annotation)
+feature_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "feature_files"),
+    pattern = "eaton_peaks",
+    full.names = TRUE
+)[1]
+
+if (!is.null(feature_file)) {
+    features <- rtracklayer::import(feature_file)
+    # Convert to chrRoman format
+    GenomeInfoDb::seqlevels(features) <- paste0(
+        "chr",
+        utils::as.roman(gsub("chr", "", GenomeInfoDb::seqlevels(features)))
+    )
+}
+
+# Process samples in groups
+#-----------------------------------------------------------------------------
+# Create sample groups
+sample_groups <- split(
+    seq_len(nrow(sorted_metadata)),
+    ceiling(seq_len(nrow(sorted_metadata)) / DEBUG_CONFIG$samples_per_group)
+)
+
+# Determine which groups to process
+groups_to_process <- if (DEBUG_CONFIG$enabled) {
+    DEBUG_CONFIG$group
+} else {
+    seq_along(sample_groups)
+}
+
+# Calculate global y-limits for all plots (before the plotting loop)
+#-----------------------------------------------------------------------------
+if (DEBUG_CONFIG$verbose) {
+    message("\nCalculating global range for all tracks...")
+}
+
+limits_result <- calculate_track_limits(
+    bigwig_files = bigwig_files,
+    genome_range = genome_range,
+    padding_fraction = 0.1,
+    verbose = DEBUG_CONFIG$verbose
+)
+
+if (!limits_result$success) {
+    warning("Failed to calculate y-limits: ", limits_result$error)
+    y_limits <- c(0, 1000)  # Default fallback
+} else {
+    y_limits <- limits_result$data
+}
+# Add after processing metadata but before track creation
+short_sample_ids <- create_minimal_identifiers(sorted_metadata$sample_id, verbose = DEBUG_CONFIG$verbose)
+
+# Create mapping between full and short IDs
+sample_id_mapping <- setNames(short_sample_ids, sorted_metadata$sample_id)
+
+# Process each group
+for (group_idx in groups_to_process) {
+    if (DEBUG_CONFIG$verbose) {
+        message("\nProcessing group ", group_idx)
+    }
+
+    # Get current group's samples
+    current_samples <- sorted_metadata[sample_groups[[group_idx]], ]
+
+    # Initialize tracks list with chromosome axis
+
+    tracks <- list(
+        Gviz::GenomeAxisTrack(
+            name = paste("Chr ", chromosome_to_plot, " Axis", sep = "")
+        )
+    )
+
+    # Add tracks for each sample
+    for (i in seq_len(nrow(current_samples))) {
+        sample_id <- current_samples$sample_id[i]
+        current_antibody <- current_samples$antibody[i]
+
+        # Find matching bigwig file
+        bigwig_file <- bigwig_files[grepl(sample_id, bigwig_files)][1]
+
+        track_name <- sprintf(
+            PLOT_CONFIG$track_name_format,
+            sample_id_mapping[sample_id],
+            current_samples$short_name[i],
+            current_samples$antibody[i]
+        )
+
+        placeholder_name <- sprintf(
+            "%s %s",
+            track_name,
+            PLOT_CONFIG$placeholder_suffix
+        )
+
+        if (!is.na(bigwig_file) && file.exists(bigwig_file)) {
+            if (DEBUG_CONFIG$verbose) {
+                message("Adding track for sample: ", sample_id)
+            }
+
+            # Update track color based on antibody
+            track_color <- if (current_antibody == "Input") {
+                color_scheme$fixed$input
+            } else {
+                color_scheme$get_color("antibody", current_antibody)
+            }
+            track_data <- rtracklayer::import(bigwig_file)
+
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                track_data,
+                name = track_name,
+                type = "l",
+                col = track_color
+            )
+        } else {
+            if (DEBUG_CONFIG$verbose) {
+                message("Creating placeholder for sample: ", sample_id)
+            }
+
+            # Calculate reasonable number of points (e.g., one point per 100bp)
+            sampling_rate <- 100  # Adjust this value based on your needs
+            num_points <- ceiling(chromosome_width / sampling_rate)
+
+            # Create proper placeholder track with genome coordinates
+            empty_ranges <- GenomicRanges::GRanges(
+                seqnames = chromosome_roman,
+                ranges = IRanges::IRanges(
+                    # Create evenly spaced points across chromosome
+                    start = seq(1, chromosome_width, length.out = num_points),
+                    width = 1
+                ),
+                score = rep(0, num_points),  # One score per position
+                strand = "*"
+            )
+
+            empty_track <- Gviz::DataTrack(
+                empty_ranges,
+                name = placeholder_name,
+                type = "l",
+                col = color_scheme$fixed$placeholder,
+                chromosome = chromosome_roman
+            )
+            tracks[[length(tracks) + 1]] <- empty_track
+        }
+    }
+
+    # Add feature track if available
+    if (exists("features")) {
+        tracks[[length(tracks) + 1]] <- Gviz::AnnotationTrack(
+            features,
+            name = "Features"
+        )
+    }
+
+    plot_title <- sprintf(
+        PLOT_CONFIG$title_format,
+        experiment_id,
+        chromosome_to_plot,
+        nrow(current_samples),
+        TIMESTAMPS$full,
+        normalization_method
+    )
+
+    Gviz::plotTracks(
+        trackList = tracks,
+        chromosome = chromosome_roman,
+        from = 1,
+        to = chromosome_width,
+        ylim = y_limits,
+        main = plot_title,
+        # Track name appearance
+        fontcolor = "black",           # Track name text color
+        background.title = "white",    # Track name background
+        col.border.title = "#E0E0E0",  # Light gray border around track names
+
+        # Other visualization parameters
+        cex.main = 1,
+        margin = 15,
+        innerMargin = 5,
+
+        # Axis appearance
+        col.axis = "black",            # Axis text color
+        cex.axis = 0.8,               # Axis text size
+
+        # Title panel
+        col.title = "black",          # Title text color
+        fontface.title = 2            # Bold title
+    )
+
+    # Save plot if needed
+    if (DEBUG_CONFIG$save_plots) {
+        plot_filename <- sprintf(
+                "%s_%s_chr%s_n%d_group%d.svg",
+                TIMESTAMPS$full,
+                experiment_id,
+                chromosome_to_plot,
+                nrow(current_samples),
+                group_idx
+        )
+        plot_file <- file.path(
+            plots_dir,
+            plot_filename
+        )
+
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf(
+                "Saving plot to: %s",
+                basename(plot_file)
+            ))
+        }
+
+        svg(plot_file, width = PLOT_CONFIG$width, height = PLOT_CONFIG$height)
+        Gviz::plotTracks(
+            trackList = tracks,
+            chromosome = chromosome_roman,
+            from = 1,
+            to = chromosome_width,
+            ylim = y_limits,
+            main = plot_title,
+            # Track name appearance
+            fontcolor = "black",           # Track name text color
+            background.title = "white",    # Track name background
+            col.border.title = "#E0E0E0",  # Light gray border around track names
+
+            # Other visualization parameters
+            cex.main = 1,
+            margin = 15,
+            innerMargin = 5,
+
+            # Axis appearance
+            col.axis = "black",            # Axis text color
+            cex.axis = 0.8,               # Axis text size
+
+            # Title panel
+            col.title = "black",          # Title text color
+            fontface.title = 2            # Bold title
+        )
+        dev.off()
+    }
+
+    # Interactive viewing options
+    if (DEBUG_CONFIG$interactive) {
+        user_input <- readline(
+            prompt = "Options: [Enter] next plot, 's' skip rest, 'q' quit: "
+        )
+        if (user_input == "q") break
+        if (user_input == "s") DEBUG_CONFIG$save_plots <- FALSE
+    } else {
+        Sys.sleep(DEBUG_CONFIG$display_time)  # Pause between plots
+    }
+}
+
+message("Processing complete")
diff --git a/failsafe_scripts/generate_genome_track_plots_stepbystep.R b/failsafe_scripts/generate_genome_track_plots_stepbystep.R
new file mode 100644
index 0000000..2b16cbc
--- /dev/null
+++ b/failsafe_scripts/generate_genome_track_plots_stepbystep.R
@@ -0,0 +1,318 @@
+# 1. Load required packages with verification
+packages <- c("QuasR", "GenomicAlignments", "Gviz", "rtracklayer", "ShortRead", "tidyverse")
+for (pkg in packages) {
+    if (!require(pkg, character.only = TRUE)) {
+        stop(paste("Package", pkg, "not found"))
+    }
+    message(paste("Loaded package:", pkg))
+}
+
+# 2. Set up initial variables and paths
+#experiment_id <- "241007Bel"
+experiment_id <- "241010Bel"
+base_dir <- file.path(Sys.getenv("HOME"), "data", experiment_id)
+message("Working directory: ", base_dir)
+
+# 3. Load and verify config
+source("~/lab_utils/scripts/bmc_config.R")
+source("~/lab_utils/scripts/extract_bmcIDmetadata_process.R")
+source("~/lab_utils/scripts/genome_core.R")
+source("~/lab_utils/scripts/sample_processing.R")
+message("Loaded config with ", length(EXPERIMENT_CONFIG$COMPARISONS), " comparisons")
+
+# Add timestamp constant
+TIMESTAMP <- format(Sys.Date(), "%Y%m%d")
+
+# 4. Load and process sample table with new processing steps
+metadata_file <- file.path(
+    base_dir, 
+    "documentation", 
+    sprintf("%s_processed_grid.csv", experiment_id)
+)
+message("Processing metadata file: ", metadata_file)
+
+# First enforce factor levels
+sample_table <- read.csv(metadata_file, stringsAsFactors = FALSE)
+message("Initial sample table rows: ", nrow(sample_table))
+
+# Enforce factor levels from config
+sample_table <- enforce_factor_levels(
+    data_frame = sample_table,
+    categories = EXPERIMENT_CONFIG$CATEGORIES
+)
+message("Factor levels enforced")
+
+# Sort metadata using config column order
+sample_table <- sort_metadata_frame(
+    data_frame = sample_table,
+    column_order = EXPERIMENT_CONFIG$COLUMN_ORDER
+)
+message("Sample table sorted by: ", 
+        paste(EXPERIMENT_CONFIG$COLUMN_ORDER, collapse = ", "))
+if("sample_id" %in% names(sample_table)){
+    sample_table$experiment_number <- sort(sample_table$sample_id)
+} else {
+    sample_table$experiment_number <- sort(sample_table$experiment_number)
+}
+
+# 5. Load reference genome
+ref_genome_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "REFGENS"),
+    pattern = "S288C_refgenome.fna",
+    full.names = TRUE,
+    recursive = TRUE
+)[1]
+message("Found reference genome: ", ref_genome_file)
+ref_genome <- readFasta(ref_genome_file)
+ref_genome_df <- data.frame(
+    chrom = names(as(ref_genome, "DNAStringSet")),
+    basePairSize = width(ref_genome)
+) %>% filter(chrom != "chrM")
+message("Processed reference genome with ", nrow(ref_genome_df), " chromosomes")
+
+# 6. Create genome ranges
+genome_ranges <- GRanges(
+    seqnames = ref_genome_df$chrom,
+    ranges = IRanges(start = 1, end = ref_genome_df$basePairSize),
+    strand = "*"
+)
+message("Created genome ranges object")
+
+# 7. Load feature file
+feature_file <- list.files(
+    file.path(Sys.getenv("HOME"), "data", "feature_files"),
+    pattern = "eaton_peaks",
+    full.names = TRUE
+)[1]
+message("Found feature file: ", feature_file)
+# Load and adjust feature chromosome style
+feature_ranges <- rtracklayer::import.bed(feature_file)
+feature_style <- genome_detect_chr_style(seqlevels(feature_ranges))
+genome_style <- genome_detect_chr_style(seqlevels(genome_ranges))
+if (feature_style != genome_style) {
+    message("Adjusting feature chromosome style to match genome")
+    new_seqlevels <- genome_convert_chr_names(
+        seqlevels(feature_ranges),
+        genome_style
+    )
+    seqlevels(feature_ranges) <- new_seqlevels
+    feature_style <- genome_detect_chr_style(seqlevels(feature_ranges))
+}
+
+feature_track <- AnnotationTrack(
+    feature_ranges,
+    name = "Origin Peaks (Eaton 2010)"
+)
+message("Created feature track with style: ", 
+        genome_detect_chr_style(seqlevels(feature_ranges)))
+
+# Function for bigwig validation
+validate_bigwig <- function(bigwig_path, experiment_number) {
+    if (is.na(bigwig_path) || !file.exists(bigwig_path)) {
+        warning(sprintf("Bigwig file not found for sample %s", experiment_number))
+        return(NULL)
+    }
+    tryCatch({
+        rtracklayer::import(bigwig_path)
+        return(bigwig_path)
+    }, error = function(e) {
+        warning(sprintf("Invalid bigwig file for sample %s: %s", experiment_number, e$message))
+        return(NULL)
+    })
+}
+
+find_fallback_control <- function(sample_table, bigwig_dir, pattern) {
+    # Get all Input samples
+    input_samples <- sample_table[sample_table$antibody == "Input", ]
+    
+    for (i in seq_len(nrow(input_samples))) {
+        control_bigwig <- list.files(
+            bigwig_dir,
+            pattern = paste0(input_samples$experiment_number[i], ".*normalized.*\\.bw$"),
+            full.names = TRUE
+        )[1]
+        
+        valid_control <- validate_bigwig(control_bigwig, input_samples$experiment_number[i])
+        if (!is.null(valid_control)) {
+            message("Using fallback control: ", basename(valid_control))
+            return(
+                list(
+                    index = i,
+                    path = valid_control
+                ))
+        }
+    }
+    
+    warning("No valid Input controls found in dataset")
+    return(NULL)
+}
+
+# Setup chromosome
+chromosome <- 10
+chromosome_roman <- paste0("chr", as.roman(chromosome))
+message("Processing chromosome: ", chromosome_roman)
+
+# 8. Process all comparisons
+calculate_track_limits <- function(tracks) {
+    y_ranges <- lapply(tracks, function(track) {
+        if (inherits(track, "DataTrack")) {
+            values <- values(track)
+            if (length(values) > 0) {
+                return(range(values, na.rm = TRUE))
+            }
+        }
+        return(NULL)
+    })
+    
+    # Remove NULL entries and calculate overall range
+    y_ranges <- y_ranges[!sapply(y_ranges, is.null)]
+    if (length(y_ranges) > 0) {
+        y_min <- min(sapply(y_ranges, `[`, 1), na.rm = TRUE)
+        y_max <- max(sapply(y_ranges, `[`, 2), na.rm = TRUE)
+        
+        # Add 10% padding
+        y_range <- y_max - y_min
+        y_min <- y_min - (y_range * 0.1)
+        y_max <- y_max + (y_range * 0.1)
+        
+        return(c(y_min, y_max))
+    }
+    return(NULL)
+}
+# Plot all comparisons
+for (comp_name in names(EXPERIMENT_CONFIG$COMPARISONS)) {
+    message("\nProcessing comparison: ", comp_name)
+    # Get samples for this comparison
+    comp_samples <- sample_table[eval(
+        EXPERIMENT_CONFIG$COMPARISONS[[comp_name]], 
+        envir = sample_table
+    ), ]
+    if (nrow(comp_samples) == 0) {
+        warning("No samples found for comparison: ", comp_name)
+        next
+    }
+    message("Found ", nrow(comp_samples), " samples for comparison")
+    # Create tracks list (rest of track creation code remains the same)
+    tracks <- list(GenomeAxisTrack(name = sprintf("Chr %s Axis", chromosome)))
+    # Find control sample first
+    control_sample <- sample_table[
+        sample_table$antibody == "Input" &
+        sample_table$rescue_allele == comp_samples$rescue_allele[1],
+    ][1,]
+    
+    if (!is.null(control_sample)) {
+        control_bigwig <- list.files(
+            file.path(base_dir, "coverage"),
+            pattern = paste0(control_sample$experiment_number, ".*normalized.*\\.bw$"),
+            full.names = TRUE
+        )[1]
+        
+        # Try primary control
+        valid_control <- validate_bigwig(control_bigwig, control_sample$experiment_number)
+        input_track_name <- paste("Input", control_sample$rescue_allele, sep = "_")
+        
+        # If primary control fails, try fallback
+        if (is.null(valid_control)) {
+            message("Primary control not found, searching for fallback")
+            control_index_and_path <- find_fallback_control(sample_table, file.path(base_dir, "coverage"), pattern)
+            control_sample <- sample_table[control_index_and_path$index, ]
+            input_track_name <- paste("Input", control_sample$rescue_allele, sep = "_")
+        }
+        
+        if (!is.null(valid_control)) {
+
+            tracks[[length(tracks) + 1]] <- DataTrack(
+                import(valid_control),
+                type = "l",
+                name = input_track_name,
+                col = "#808080"
+            )
+        }
+    }
+
+    label_categories <- EXPERIMENT_CONFIG$COLUMN_ORDER  # Using the ordered columns from config
+    
+    # Generate labels for all samples in comparison
+    comparison_labels <- unique_labeling(
+        comp_samples,
+        label_categories
+    )
+
+    #
+    test_labels <- sample_generate_labels(
+        comp_samples,
+        EXPERIMENT_CONFIG$COLUMN_ORDER,
+        verbose = TRUE
+    )
+    message("Generated labels for tracks:")
+    print(test_labels)
+    
+    # Process each sample
+    for (i in seq_len(nrow(comp_samples))) {
+        bigwig_file <- list.files(
+            file.path(base_dir, "coverage"),
+            pattern = paste0(comp_samples$experiment_number[i], ".*normalized.*\\.bw$"),
+            full.names = TRUE
+        )[1]
+        # Validate sample bigwig
+        valid_bigwig <- validate_bigwig(bigwig_file, comp_samples$experiment_number[i])
+        if (!is.null(valid_bigwig)) {
+            message("Adding sample track from: ", basename(valid_bigwig))
+            tracks[[length(tracks) + 1]] <- DataTrack(
+                import(valid_bigwig),
+                type = "l",
+                name = comparison_labels[i],
+                col = "#fd0036",
+                chromosome = chromosome_roman
+            )
+        }
+    }
+
+    # Add feature track if it exists
+    if (!is.null(feature_track)) {
+        tracks[[length(tracks) + 1]] <- feature_track
+    }
+
+    output_file <- file.path(
+        base_dir,
+        "plots",
+        sprintf(
+            "%s_%s_%s_chr%s_all_comparisons.svg",
+            TIMESTAMP,
+            experiment_id,
+            comp_name,
+            chromosome
+        )
+    )
+    message("Generating plot: ", basename(output_file))
+    # Create plot
+    tryCatch({
+        y_limits <- calculate_track_limits(tracks)
+        if (!is.null(y_limits)) {
+            message(sprintf("Setting y-limits to: [%.2f, %.2f]", y_limits[1], y_limits[2]))
+            svg(output_file)
+            plotTracks(
+                tracks,
+                main = sprintf("Chr %s - %s", chromosome, sub("comp_", "", comp_name)),
+                chromosome = chromosome_roman,
+                ylim = y_limits
+            )
+            dev.off()
+            message("Plot created successfully")
+        } else {
+            svg(output_file)
+            plotTracks(
+                tracks,
+                main = sprintf("Chr %s - %s", chromosome, sub("comp_", "", comp_name)),
+                chromosome = chromosome_roman
+            )
+            dev.off()
+            message("Plot created successfully")
+        }
+    }, error = function(e) {
+        warning("Failed to create plot: ", e$message)
+    })
+}
+
+message("All plots generated.")
+# rsync -avz --progress luised94@luria.mit.edu:/home/luised94/data/241007Bel/plots/ $HOME/data/241007Bel/plots/
diff --git a/failsafe_scripts/genome_core.R b/failsafe_scripts/genome_core.R
new file mode 100644
index 0000000..10302a8
--- /dev/null
+++ b/failsafe_scripts/genome_core.R
@@ -0,0 +1,75 @@
+# Constants
+CHROMOSOME_MAPPING <- list(
+    numeric_to_roman = c(
+        "1" = "I", "2" = "II", "3" = "III", "4" = "IV",
+        "5" = "V", "6" = "VI", "7" = "VII", "8" = "VIII",
+        "9" = "IX", "10" = "X", "11" = "XI", "12" = "XII",
+        "13" = "XIII", "14" = "XIV", "15" = "XV", "16" = "XVI"
+    ),
+    roman_to_numeric = NULL  # Will be initialized in .onLoad
+)
+
+# Initialize reverse mapping
+.onLoad <- function(libname, pkgname) {
+    CHROMOSOME_MAPPING$roman_to_numeric <- setNames(
+        names(CHROMOSOME_MAPPING$numeric_to_roman),
+        CHROMOSOME_MAPPING$numeric_to_roman
+    )
+}
+
+genome_detect_chr_style <- function(chr_names) {
+    # Input validation
+    if (!is.character(chr_names) || length(chr_names) == 0) {
+        stop("Invalid chromosome names input")
+    }
+    
+    # Define style patterns
+    style_patterns <- list(
+        UCSC = "^chr[0-9]+$",
+        Roman = "^chr[IVX]+$",
+        Numeric = "^[0-9]+$"
+    )
+    
+    # Check each style
+    for (style in names(style_patterns)) {
+        if (all(grepl(style_patterns[[style]], chr_names))) {
+            return(style)
+        }
+    }
+    
+    return("Unknown")
+}
+
+genome_convert_chr_names <- function(chr_names, target_style) {
+    # Input validation
+    if (!is.character(chr_names)) {
+        stop("Chromosome names must be character vector")
+    }
+    if (!target_style %in% c("UCSC", "Roman", "Numeric")) {
+        stop("Invalid target style. Must be 'UCSC', 'Roman', or 'Numeric'")
+    }
+    
+    # Remove existing 'chr' prefix
+    clean_names <- gsub("^chr", "", chr_names)
+    
+    # Convert based on target style
+    converted_names <- switch(target_style,
+        "UCSC" = paste0("chr", clean_names),
+        "Roman" = sapply(clean_names, function(x) {
+            if (x %in% names(CHROMOSOME_MAPPING$numeric_to_roman)) {
+                paste0("chr", CHROMOSOME_MAPPING$numeric_to_roman[x])
+            } else {
+                paste0("chr", x)
+            }
+        }),
+        "Numeric" = sapply(clean_names, function(x) {
+            if (x %in% CHROMOSOME_MAPPING$numeric_to_roman) {
+                CHROMOSOME_MAPPING$roman_to_numeric[x]
+            } else {
+                x
+            }
+        })
+    )
+    
+    return(unname(converted_names))
+}
diff --git a/failsafe_scripts/genome_track_visualization_template.R b/failsafe_scripts/genome_track_visualization_template.R
new file mode 100644
index 0000000..faa5de8
--- /dev/null
+++ b/failsafe_scripts/genome_track_visualization_template.R
@@ -0,0 +1,415 @@
+
+#!/usr/bin/env Rscript
+
+# Script: genome_track_visualization.R
+# Purpose: Generate genome browser tracks from BigWig files with feature annotations
+# Usage: Rscript genome_track_visualization.R
+# Dependencies: rtracklayer, GenomicRanges, Gviz
+# Author: [AUTHOR]
+# Date: [DATE]
+
+# Configuration
+#-----------------------------------------------------------------------------
+DEBUG_CONFIG <- list(
+    enabled = FALSE,           # TRUE for single group testing, FALSE for all groups
+    group = 1,                # Group to process in debug mode
+    samples_per_group = 4,    # Number of samples to show per plot
+    save_plots = TRUE,        # Save plots to files
+    verbose = TRUE,           # Print processing details
+    chromosome = 1,           # Default chromosome to visualize
+    interactive = TRUE,       # Enable interactive plot viewing
+    display_time = 2          # Seconds to display each plot in non-interactive mode
+)
+
+TIME_CONFIG <- list(
+    timestamp_format = "%Y%m%d_%H%M%S",  # Format for full timestamp
+    date_format = "%Y%m%d"               # Format for date only
+)
+
+PLOT_CONFIG <- list(
+    width = 10,                # Plot width in inches
+    height = 8,                # Plot height in inches
+    track_color = "#fd0036",   # Color for data tracks
+    placeholder_color = "#cccccc",  # Color for missing data
+    track_name_format = "%s (%s)",  # Format: sample_id (short_name)
+    placeholder_suffix = "(No data)"  # Suffix for placeholder tracks
+)
+
+# Required Packages
+#-----------------------------------------------------------------------------
+required_packages <- c("rtracklayer", "GenomicRanges", "Gviz", "Biostrings")
+for (pkg in required_packages) {
+    if (!requireNamespace(pkg, quietly = TRUE)) {
+        stop(sprintf("Required package '%s' is missing", pkg))
+    }
+}
+
+# Load Custom Functions and Configurations
+#-----------------------------------------------------------------------------
+# MODIFY PATHS AS NEEDED
+source("path/to/config.R")     # Load experiment configurations
+#source("path/to/functions.R")  # Load custom functions if needed
+
+# Initialize Timestamps
+#-----------------------------------------------------------------------------
+TIMESTAMPS <- list(
+    full = format(Sys.time(), TIME_CONFIG$timestamp_format),
+    date = format(Sys.Date(), TIME_CONFIG$date_format)
+)
+
+# File Paths and Directory Setup
+#-----------------------------------------------------------------------------
+# MODIFY THESE VARIABLES FOR YOUR EXPERIMENT
+experiment_id <- "EXPERIMENT_ID"  # Replace with your experiment ID
+
+# Construct directory paths
+paths <- list(
+    base = file.path(Sys.getenv("HOME"), "data", experiment_id),
+    plots = NULL,
+    coverage = NULL,
+    documentation = NULL,
+    reference = file.path(Sys.getenv("HOME"), "data", "REFGENS"),
+    features = file.path(Sys.getenv("HOME"), "data", "feature_files")
+)
+
+# Initialize derived paths
+paths$plots <- file.path(paths$base, "plots", "genome_tracks")
+paths$coverage <- file.path(paths$base, "coverage")
+paths$documentation <- file.path(paths$base, "documentation")
+
+# Create output directory
+dir.create(paths$plots, recursive = TRUE, showWarnings = FALSE)
+
+# Data Loading and Preprocessing
+#-----------------------------------------------------------------------------
+# Load and validate metadata
+metadata_path <- file.path(
+    paths$documentation, 
+    sprintf("%s_sample_grid.csv", experiment_id)
+)
+
+if (!file.exists(metadata_path)) {
+    stop("Metadata file not found: ", metadata_path)
+}
+
+# Load sample metadata
+metadata <- read.csv(metadata_path, stringsAsFactors = FALSE)
+
+# Extract sample IDs from fastq files
+#-----------------------------------------------------------------------------
+fastq_files <- list.files(
+    path = file.path(paths$base, "fastq"),
+    pattern = "consolidated_.*_sequence\\.fastq$",
+    full.names = FALSE
+)
+
+if (length(fastq_files) == 0) {
+    stop("No fastq files found in: ", file.path(paths$base, "fastq"))
+}
+
+# Extract sample IDs
+sample_ids <- gsub(
+    pattern = "consolidated_([0-9]{5,6})_sequence\\.fastq",
+    replacement = "\\1",
+    x = fastq_files
+)
+
+# Process Metadata
+#-----------------------------------------------------------------------------
+# Enforce factor levels from configuration
+for (col_name in names(EXPERIMENT_CONFIG$CATEGORIES)) {
+    if (col_name %in% colnames(metadata)) {
+        metadata[[col_name]] <- factor(
+            metadata[[col_name]],
+            levels = EXPERIMENT_CONFIG$CATEGORIES[[col_name]],
+            ordered = TRUE
+        )
+    }
+}
+
+# Sort metadata and add sample IDs
+sorted_metadata <- metadata[do.call(order, metadata[EXPERIMENT_CONFIG$COLUMN_ORDER]), ]
+sorted_metadata$sample_id <- sample_ids
+
+# Print processing summary if verbose
+if (DEBUG_CONFIG$verbose) {
+    message("\nMetadata Processing Summary:")
+    message(sprintf("- Found %d fastq files", length(fastq_files)))
+    message(sprintf("- Processed %d metadata rows", nrow(sorted_metadata)))
+    message("- Enforced factors for columns:")
+    print(names(EXPERIMENT_CONFIG$CATEGORIES))
+}
+
+# Load Reference Genome and Create Range
+#-----------------------------------------------------------------------------
+# Find reference genome file
+ref_genome_file <- list.files(
+    paths$reference,
+    pattern = "S288C_refgenome.fna",
+    full.names = TRUE,
+    recursive = TRUE
+)[1]
+
+if (is.na(ref_genome_file)) {
+    stop("Reference genome file not found in: ", paths$reference)
+}
+
+# Load genome and create range
+genome_data <- Biostrings::readDNAStringSet(ref_genome_file)
+chromosome_to_plot <- DEBUG_CONFIG$chromosome
+chromosome_width <- genome_data[chromosome_to_plot]@ranges@width
+chromosome_roman <- paste0("chr", utils::as.roman(chromosome_to_plot))
+
+genome_range <- GenomicRanges::GRanges(
+    seqnames = chromosome_roman,
+    ranges = IRanges::IRanges(start = 1, end = chromosome_width),
+    strand = "*"
+)
+
+# Load BigWig Files and Features
+#-----------------------------------------------------------------------------
+# Find bigwig files
+bigwig_files <- list.files(
+    paths$coverage,
+    pattern = "_CPM\\.bw$",
+    full.names = TRUE
+)
+
+if (length(bigwig_files) == 0) {
+    warning("No BigWig files found in: ", paths$coverage)
+}
+
+# Load feature annotations if available
+feature_file <- list.files(
+    paths$features,
+    pattern = "eaton_peaks",
+    full.names = TRUE
+)[1]
+
+features <- NULL
+if (!is.null(feature_file)) {
+    features <- tryCatch({
+        feature_data <- rtracklayer::import(feature_file)
+        # Convert to chrRoman format
+        GenomeInfoDb::seqlevels(feature_data) <- paste0(
+            "chr", 
+            utils::as.roman(gsub("chr", "", GenomeInfoDb::seqlevels(feature_data)))
+        )
+        feature_data
+    }, error = function(e) {
+        warning("Failed to load feature file: ", e$message)
+        NULL
+    })
+}
+
+# Sample Grouping and Processing Setup
+#-----------------------------------------------------------------------------
+# Create sample groups for visualization
+sample_groups <- split(
+    seq_len(nrow(sorted_metadata)),
+    ceiling(seq_len(nrow(sorted_metadata)) / DEBUG_CONFIG$samples_per_group)
+)
+
+# Determine processing scope based on debug configuration
+groups_to_process <- if (DEBUG_CONFIG$enabled) {
+    DEBUG_CONFIG$group
+} else {
+    seq_along(sample_groups)
+}
+
+if (DEBUG_CONFIG$verbose) {
+    message("\nProcessing Configuration:")
+    message(sprintf("- Total groups: %d", length(sample_groups)))
+    message(sprintf("- Processing groups: %s", 
+                   paste(groups_to_process, collapse = ", ")))
+}
+
+# Calculate Global Y-axis Limits
+#-----------------------------------------------------------------------------
+if (DEBUG_CONFIG$verbose) {
+    message("\nCalculating global range for tracks...")
+}
+
+# Collect all track values for scaling
+all_track_values <- c()
+
+# Process each bigwig file
+for (bigwig_file in bigwig_files) {
+    if (file.exists(bigwig_file)) {
+        tryCatch({
+            track_data <- rtracklayer::import(
+                bigwig_file,
+                which = genome_range
+            )
+            
+            if (length(track_data) > 0) {
+                values <- GenomicRanges::values(track_data)$score
+                if (length(values) > 0) {
+                    all_track_values <- c(all_track_values, values)
+                }
+            }
+        }, error = function(e) {
+            if (DEBUG_CONFIG$verbose) {
+                message("Skipping ", basename(bigwig_file), ": ", e$message)
+            }
+        })
+    }
+}
+
+# Calculate visualization limits with padding
+y_limits <- if (length(all_track_values) > 0) {
+    y_min <- min(all_track_values, na.rm = TRUE)
+    y_max <- max(all_track_values, na.rm = TRUE)
+    y_range <- y_max - y_min
+    c(
+        y_min - (y_range * 0.1),  # Add 10% padding
+        y_max + (y_range * 0.1)
+    )
+} else {
+    c(0, 1)  # Default limits if no data
+}
+
+if (DEBUG_CONFIG$verbose) {
+    message(sprintf("Global y-limits: [%.2f, %.2f]", y_limits[1], y_limits[2]))
+}
+
+# Track Creation and Visualization
+#-----------------------------------------------------------------------------
+for (group_idx in groups_to_process) {
+    if (DEBUG_CONFIG$verbose) {
+        message(sprintf("\nProcessing group %d of %d", 
+                       group_idx, max(groups_to_process)))
+    }
+    
+    # Get current group samples
+    current_samples <- sorted_metadata[sample_groups[[group_idx]], ]
+    
+    # Initialize track list with genome axis
+    tracks <- list(
+        Gviz::GenomeAxisTrack(
+            name = paste("Chr ", chromosome_to_plot, " Axis", sep = "")
+        )
+    )
+    
+    # Process each sample in group
+    for (i in seq_len(nrow(current_samples))) {
+        sample_id <- current_samples$sample_id[i]
+        track_name <- sprintf(
+            PLOT_CONFIG$track_name_format,
+            sample_id,
+            current_samples$short_name[i]
+        )
+        
+        # Find matching bigwig file
+        bigwig_file <- bigwig_files[grepl(sample_id, bigwig_files)][1]
+        
+        if (!is.na(bigwig_file) && file.exists(bigwig_file)) {
+            if (DEBUG_CONFIG$verbose) {
+                message("Creating track for: ", track_name)
+            }
+            
+            track_data <- rtracklayer::import(
+                bigwig_file,
+                which = genome_range
+            )
+            
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                track_data,
+                name = track_name,
+                type = "l",
+                col = PLOT_CONFIG$track_color
+            )
+        } else {
+            if (DEBUG_CONFIG$verbose) {
+                message("Creating placeholder for: ", track_name)
+            }
+            
+            # Create placeholder track
+            empty_ranges <- GenomicRanges::GRanges(
+                seqnames = chromosome_roman,
+                ranges = IRanges::IRanges(
+                    start = seq(1, chromosome_width, length.out = 1000),
+                    width = 1
+                ),
+                score = rep(0, 1000),
+                strand = "*"
+            )
+            
+            tracks[[length(tracks) + 1]] <- Gviz::DataTrack(
+                empty_ranges,
+                name = paste(track_name, PLOT_CONFIG$placeholder_suffix),
+                type = "l",
+                col = PLOT_CONFIG$placeholder_color,
+                chromosome = chromosome_roman
+            )
+        }
+    }
+    
+    # Add feature track if available
+    if (!is.null(features)) {
+        tracks[[length(tracks) + 1]] <- Gviz::AnnotationTrack(
+            features,
+            name = "Features"
+        )
+    }
+    
+    # Generate plot title
+    plot_title <- sprintf(
+        "Chromosome %s - Group %d of %d",
+        chromosome_to_plot,
+        group_idx,
+        max(groups_to_process)
+    )
+    
+    # Create visualization
+    Gviz::plotTracks(
+        tracks,
+        chromosome = chromosome_roman,
+        from = 1,
+        to = chromosome_width,
+        ylim = y_limits,
+        title = plot_title
+    )
+    
+    # Save plot if configured
+    if (DEBUG_CONFIG$save_plots) {
+        plot_file <- file.path(
+            paths$plots,
+            sprintf(
+                "%s_%s_chr%s_group%d.svg",
+                TIMESTAMPS$full,
+                experiment_id,
+                chromosome_to_plot,
+                group_idx
+            )
+        )
+        
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("Saving plot to: %s", basename(plot_file)))
+        }
+        
+        svg(plot_file, width = PLOT_CONFIG$width, height = PLOT_CONFIG$height)
+        Gviz::plotTracks(
+            tracks,
+            chromosome = chromosome_roman,
+            from = 1,
+            to = chromosome_width,
+            ylim = y_limits,
+            title = plot_title
+        )
+        dev.off()
+    }
+    
+    # Handle interactive viewing
+    if (DEBUG_CONFIG$interactive) {
+        user_input <- readline(
+            prompt = "Options: [Enter] next plot, 's' skip rest, 'q' quit: "
+        )
+        if (user_input == "q") break
+        if (user_input == "s") DEBUG_CONFIG$save_plots <- FALSE
+    } else {
+        Sys.sleep(DEBUG_CONFIG$display_time)
+    }
+}
+
+message("\nProcessing complete")
diff --git a/failsafe_scripts/job_monitoring_helper.sh b/failsafe_scripts/job_monitoring_helper.sh
new file mode 100644
index 0000000..3556b31
--- /dev/null
+++ b/failsafe_scripts/job_monitoring_helper.sh
@@ -0,0 +1,110 @@
+#!/bin/bash
+# job_monitoring_helper.sh
+# Purpose: Provide comprehensive job monitoring and log viewing utilities
+
+# Color codes for enhanced readability
+RED='\033[0;31m'
+GREEN='\033[0;32m'
+YELLOW='\033[1;33m'
+BLUE='\033[0;34m'
+NC='\033[0m' # No Color
+
+# Job Monitoring and Log Viewing Helper Function
+job_monitoring_helper() {
+    local job_id="${1:-}"
+    
+    # Validate job ID
+    if [[ -z "$job_id" ]]; then
+        echo -e "${RED}Error: Please provide a SLURM job ID${NC}"
+        return 1
+    }
+
+    # Clear screen and display header
+    clear
+    echo -e "${BLUE}===== SLURM JOB MONITORING HELPER =====${NC}"
+    echo -e "${YELLOW}Job ID: ${job_id}${NC}"
+    echo "-------------------------------------------"
+
+    # Section 1: Basic Job Information
+    echo -e "${GREEN}1. JOB DETAILS${NC}"
+    scontrol show job "$job_id" | grep -E "JobId|JobName|UserId|State|Partition|NodeList"
+    echo "-------------------------------------------"
+
+    # Section 2: Real-time Job Status
+    echo -e "${GREEN}2. CURRENT JOB STATUS${NC}"
+    squeue -j "$job_id"
+    echo "-------------------------------------------"
+
+    # Section 3: Resource Utilization
+    echo -e "${GREEN}3. RESOURCE UTILIZATION${NC}"
+    sstat -j "$job_id" --format=AveCPU,AvePages,AveRSS,MaxRSS
+    echo "-------------------------------------------"
+
+    # Section 4: Potential Log Locations
+    echo -e "${GREEN}4. POTENTIAL LOG LOCATIONS${NC}"
+    echo "SLURM Output Log: slurm-${job_id}.out"
+    echo "Custom Logs: ~/logs/[tool]/job_${job_id}/"
+    echo "-------------------------------------------"
+
+    # Section 5: Useful Monitoring Commands
+    echo -e "${GREEN}5. USEFUL MONITORING COMMANDS${NC}"
+    echo -e " ${YELLOW}Live Monitoring:${NC}"
+    echo "  watch -n 5 'squeue -j ${job_id}'"
+    echo "  seff ${job_id}"
+    
+    echo -e "\n ${YELLOW}Log Viewing Commands:${NC}"
+    echo "  tail -f slurm-${job_id}.out"
+    echo "  less slurm-${job_id}.out"
+    
+    echo -e "\n ${YELLOW}Comprehensive Log Navigation:${NC}"
+    echo "  vim ~/logs/[tool]/job_${job_id}/task_*/main_*.log"
+    echo "  less ~/logs/[tool]/job_${job_id}/task_*/error_*.log"
+    
+    echo -e "\n ${YELLOW}Job Cancellation (if needed):${NC}"
+    echo "  scancel ${job_id}"
+    
+    echo -e "\n${BLUE}===== END OF JOB MONITORING HELPER =====${NC}"
+}
+
+# Companion Function: Log File Navigator
+navigate_job_logs() {
+    local job_id="${1:-}"
+    local log_base="${HOME}/logs"
+    
+    if [[ -z "$job_id" ]]; then
+        echo -e "${RED}Error: Please provide a SLURM job ID${NC}"
+        return 1
+    }
+
+    # Find all log directories for the job
+    local log_dirs=$(find "${log_base}" -type d -name "job_${job_id}")
+    
+    if [[ -z "$log_dirs" ]]; then
+        echo -e "${RED}No log directories found for job ${job_id}${NC}"
+        return 1
+    }
+
+    echo -e "${GREEN}Log Directories for Job ${job_id}:${NC}"
+    echo "$log_dirs"
+    
+    echo -e "\n${YELLOW}Suggested Log Exploration Commands:${NC}"
+    echo "1. Find all main log files:"
+    echo "   find ${log_base} -name \"main_*.log\" -path \"*job_${job_id}*\""
+    
+    echo -e "\n2. Find all error log files:"
+    echo "   find ${log_base} -name \"error_*.log\" -path \"*job_${job_id}*\""
+    
+    echo -e "\n3. Tail the most recent main log:"
+    echo "   tail -n 50 \$(find ${log_base} -name \"main_*.log\" -path \"*job_${job_id}*\" | sort | tail -n 1)"
+}
+
+# Usage Examples
+: <<'USAGE_EXAMPLES'
+# In terminal after job submission
+job_monitoring_helper 12345
+navigate_job_logs 12345
+
+# Example aliases to add to .bashrc
+alias jobmon='job_monitoring_helper'
+alias navlogs='navigate_job_logs'
+USAGE_EXAMPLES
diff --git a/failsafe_scripts/parse_fastqc_files.R b/failsafe_scripts/parse_fastqc_files.R
new file mode 100644
index 0000000..dec1dd1
--- /dev/null
+++ b/failsafe_scripts/parse_fastqc_files.R
@@ -0,0 +1,463 @@
+################################################################################
+# Parse FastQC files for ChIP-seq quality control
+################################################################################
+# PURPOSE:
+#   Process FastQC output files and generate parsed tab-delimited summaries for
+#   ChIP-seq quality control analysis. Extracts module-specific data and creates
+#   standardized output files with sample identifiers.
+#
+# USAGE:
+#   1. Update experiment_id to point to correct data directory
+#   2. Adjust DEBUG_CONFIG as needed:
+#      - single_file_mode: TRUE for testing single files
+#      - verbose: TRUE for detailed processing information
+#      - dry_run: TRUE to check without writing files
+#   3. Run script
+#
+# !! ----> REQUIRED UPDATES:
+#   - Set experiment_id for data directory
+#   - Review debug configuration for testing needs
+#   - Verify FastQC version requirements
+#
+# STRUCTURE:
+#   1. Configuration and debug settings
+#   2. Directory and version validation
+#   3. File discovery and sample ID extraction
+#   4. Module parsing and data extraction:
+#      - Basic Statistics
+#      - Per base sequence quality
+#      - Per sequence quality scores
+#      - Other FastQC modules
+#   5. Data validation and output
+#
+# VALIDATION:
+#   - Directory structure and permissions
+#   - FastQC file presence and version
+#   - Sample ID format and uniqueness
+#   - Module structure and content
+#   - Data parsing integrity
+#
+# DEPENDENCIES:
+#   - Base R (>= 4.2.0)
+#   - FastQC (version 0.11.5)
+#   - submit_fastqc.sh
+#   - run_fastqc_array.sh
+#
+# COMMON ISSUES:
+#   - Missing or incorrect quality control directory
+#   - Malformed FastQC files or unexpected versions
+#   - Write permission errors in output directory
+#   - Inconsistent sample ID formats
+#   - Memory limitations with large datasets
+#
+# OUTPUT:
+#   - Module-specific tab-delimited files
+#   - Summary statistics for each sample
+#   - Processing logs (when verbose)
+#   - Module Name File for future reference and mappings.
+# AUTHOR: Luis
+# DATE: 2024-12-02
+# VERSION: 1.0.0
+################################################################################
+################################################################################
+# Configuration and Debug Settings
+################################################################################
+DEBUG_CONFIG <- list( # !! UPDATE THIS
+    single_file_mode = FALSE,           # Test single file in main logic.
+    verbose = TRUE,           # Print processing details
+    interactive = TRUE,       # Allow interactive processing
+    dry_run = FALSE,         # Skip file writes
+    files_to_process_idx = 1  # Process specific files in debug mode
+)
+
+FASTQC_CONFIG <- list(
+    VERSION = "0.11.5",                    # Expected FastQC version
+    VERSION_PATTERN = "^##FastQC\\s+",     # Pattern to match version line
+    HEADER_PATTERN = "^##FastQC",          # Pattern to identify FastQC header
+    module_separator = ">>",
+    module_end = ">>END_MODULE",
+    header_prefix = "#",
+    fastqc_pattern = "fastqc_data",
+    output_suffix = ".tab",
+    qc_subdir = "quality_control",
+    existing_version_limit = 1,
+    module_names = character(0),
+    module_reference_file = file.path(
+        Sys.getenv("HOME"),
+        "data",
+        "fastqc_module_reference.rds"
+    )
+)
+
+FASTQC_CONFIG$FILE_PATTERN <- list(
+    REGEX = "consolidated_([0-9]{5,6})_sequence_fastqc_data\\.txt$",
+    EXPECTED_FORMAT = "consolidated_XXXXXX_sequence_fastqc_data.txt"  # For error messages
+)
+
+TIME_CONFIG <- list(
+    timestamp_format = "%Y%m%d_%H%M%S",
+    date_format = "%Y%m%d"
+)
+
+# Generate timestamps
+TIMESTAMPS <- list(
+    full = format(Sys.time(), TIME_CONFIG$timestamp_format),
+    date = format(Sys.Date(), TIME_CONFIG$date_format)
+)
+
+################################################################################
+# Load and Validate Experiment Configuration
+################################################################################
+# Bootstrap phase
+bootstrap_path <- normalizePath("~/lab_utils/failsafe_scripts/functions_for_file_operations.R", 
+                              mustWork = FALSE)
+if (!file.exists(bootstrap_path)) {
+    stop(sprintf("[FATAL] Bootstrap file not found: %s", bootstrap_path))
+}
+source(bootstrap_path)
+
+################################################################################
+# Directory Setup and Validation
+################################################################################
+experiment_id <- "241007Bel"  # !! UPDATE THIS
+base_dir <- file.path(Sys.getenv("HOME"), "data", experiment_id)
+qc_dir <- file.path(base_dir, FASTQC_CONFIG$qc_subdir)
+
+stopifnot(
+    "Base directory does not exist" = dir.exists(base_dir),
+    "Quality control directory does not exist" = dir.exists(qc_dir)
+)
+
+################################################################################
+# File Discovery
+################################################################################
+fastqc_files <- list.files(
+    qc_dir,
+    pattern = FASTQC_CONFIG$fastqc_pattern,
+    recursive = TRUE,
+    full.names = TRUE
+)
+
+stopifnot(
+    "No FastQC files found" = length(fastqc_files) > 0
+)
+
+# Check FastQC versions across files
+fastqc_versions <- list()
+for (file_path in fastqc_files) {
+    first_line <- readLines(file_path, n = 1)
+    if (grepl(FASTQC_CONFIG$HEADER_PATTERN, first_line)) {
+        version <- gsub(FASTQC_CONFIG$VERSION_PATTERN, "", first_line)
+        fastqc_versions[[basename(file_path)]] <- version
+    } else {
+        warning(sprintf("File %s does not start with expected FastQC header", 
+                       basename(file_path)))
+    }
+}
+
+# Check if all versions match
+expected_version <- FASTQC_CONFIG$VERSION
+version_check <- sapply(fastqc_versions, function(v) v == expected_version)
+
+if (!all(version_check)) {
+    different_versions <- fastqc_versions[!version_check]
+    warning(sprintf(
+        "Found different FastQC versions:\nExpected: %s\nDifferent versions found in:\n%s",
+        expected_version,
+        paste(sprintf("- %s: %s", 
+                     names(different_versions), 
+                     unlist(different_versions)), 
+              collapse = "\n")
+    ))
+}
+
+if (DEBUG_CONFIG$verbose) {
+    message("FastQC version check complete")
+    message(sprintf("Number of files checked: %d", length(fastqc_versions)))
+}
+
+# Extract and validate sample IDs
+sample_ids <- character(length(fastqc_files))
+invalid_format_files <- character(0)
+
+# Extract sample IDs with direct pattern matching
+sample_ids <- gsub(
+    pattern = FASTQC_CONFIG$FILE_PATTERN$REGEX,
+    replacement = "\\1",
+    x = basename(fastqc_files)
+)
+
+# Validate specific failure modes with descriptive errors
+invalid_format_files <- basename(fastqc_files)[
+    !grepl("consolidated_.*_sequence_fastqc_data\\.txt$", basename(fastqc_files))
+]
+invalid_id_files <- basename(fastqc_files)[!grepl("^[0-9]{5,6}$", sample_ids)]
+duplicate_ids <- sample_ids[duplicated(sample_ids)]
+
+# Clear validation with specific error messages
+stopifnot(
+    `Files not matching expected format` = length(invalid_format_files) == 0,
+    `Files with invalid sample IDs` = length(invalid_id_files) == 0,
+    `Duplicate sample IDs found` = length(duplicate_ids) == 0,
+    `All sample IDs must be extracted` = !any(sample_ids == ""),
+    `Number of sample IDs must match number of files` = length(fastqc_files) == length(sample_ids)
+)
+
+if (DEBUG_CONFIG$verbose) {
+    message("Sample ID validation passed:")
+    print(data.frame(
+        file = basename(fastqc_files),
+        sample_id = sample_ids,
+        stringsAsFactors = FALSE
+    ))
+}
+
+################################################################################
+# Process Files
+################################################################################
+files_to_process <- if (DEBUG_CONFIG$single_file_mode) {
+    DEBUG_CONFIG$files_to_process_idx
+} else {
+    seq_along(fastqc_files)
+}
+
+stopifnot(
+    `Files to process must be within valid range` = all(files_to_process <= length(fastqc_files)),
+    `Files to process cannot be negative` = all(files_to_process > 0)
+)
+
+for (file_idx in files_to_process) {
+    current_file <- fastqc_files[file_idx]
+    
+    if (DEBUG_CONFIG$verbose) {
+        message(sprintf("\nProcessing file %d of %d: %s", 
+                       file_idx, 
+                       length(files_to_process), 
+                       basename(current_file)))
+    }
+    
+    # Read and parse file
+    lines <- readLines(current_file)
+    module_starts <- which(grepl(FASTQC_CONFIG$module_separator, lines))
+    module_ends <- which(grepl(FASTQC_CONFIG$module_end, lines))
+    module_starts <- module_starts[!(module_starts %in% module_ends)]
+    
+    stopifnot(
+        `File must contain content` = length(lines) > 0,
+        `File must contain FastQC modules` = length(module_starts) > 0,
+        `Module start and end markers must match` = length(module_starts) == length(module_ends),
+        `Module markers must be in correct order` = all(module_starts < module_ends),
+        `Module must have valid content` = all(module_starts > 0 & module_ends <= length(lines))
+        #`Module must contain data lines` = all(module_ends - module_starts > 2)
+    )
+
+    if (DEBUG_CONFIG$verbose) {
+        message(sprintf("Found %d modules", length(module_starts)))
+    }
+    
+    output_dir <- dirname(current_file)
+    fastqc_summary <- list()
+    
+    # Process each module
+    for (module_idx in seq_along(module_starts)) {
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("\n  Processing module %d of %d", 
+                          module_idx, 
+                          length(module_starts)))
+        }
+        
+        module_lines <- lines[module_starts[module_idx]:module_ends[module_idx]]
+        module_summary <- gsub(FASTQC_CONFIG$module_separator, "", module_lines[1])
+        fastqc_summary <- append(fastqc_summary, module_summary)
+        
+        # Parse module data
+        module_name <- gsub(" ", "", strsplit(module_summary, "\t")[[1]][1])
+        FASTQC_CONFIG$module_names <- unique(c(FASTQC_CONFIG$names, module_name))
+
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("    Module name: %s", module_name))
+            message(sprintf("    Module summary: %s", module_summary))
+            message(sprintf("    Module lines: %d", length(module_lines)))
+        }
+        
+        # Find potential headers
+        module_data <- module_lines[2:(length(module_lines)-1)]
+        potential_headers <- which(grepl(paste0("^", FASTQC_CONFIG$header_prefix), module_data))
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("    Found %d potential headers", length(potential_headers)))
+            if (length(potential_headers) > 0) {
+                message("    Headers content:")
+                for(h_idx in potential_headers) {
+                    message(sprintf("      Line %d: %s", h_idx, module_data[h_idx]))
+                }
+            }
+        }
+        
+        # Only process if we found headers
+        if (length(potential_headers) > 0) {
+            last_potential_header <- potential_headers[length(potential_headers)]
+            data <- NULL
+            
+            # Validate headers against data structure
+            for (header_idx in potential_headers) {
+                header_elements <- strsplit(module_data[header_idx], "\t")[[1]]
+                last_line_elements <- strsplit(module_data[last_potential_header], "\t")[[1]]
+                
+                if (DEBUG_CONFIG$verbose) {
+                    message(sprintf("    Checking header at line %d:", header_idx))
+                    message(sprintf("      Header elements: %d", length(header_elements)))
+                    message(sprintf("      Data line elements: %d", length(last_line_elements)))
+                }
+                
+                if(length(header_elements) == length(last_line_elements)) {
+                    if (DEBUG_CONFIG$verbose) {
+                        message("    Found matching header structure")
+                    }
+                    
+                    header <- gsub(FASTQC_CONFIG$header_prefix, "", module_data[header_idx])
+                    # Adjust indexes to skip the module header line
+                    data_start_idx <- header_idx + 1
+                    data_end_idx <- length(module_data)
+                    
+                    data <- read.table(
+                        text = module_data[data_start_idx:data_end_idx],
+                        header = FALSE,
+                        col.names = strsplit(header, "\t")[[1]],
+                        sep = "\t"
+                    )
+
+                    if (!is.null(data)) {
+                        stopifnot(
+                            `Parsed data must have rows` = nrow(data) > 0,
+                            `Parsed data must have columns` = ncol(data) > 0,
+                            `Column names must not be empty` = all(nchar(colnames(data)) > 0)
+                        )
+                    }
+                     
+                    if (DEBUG_CONFIG$verbose) {
+                        message(sprintf("    Parsed data: %d rows, %d columns", 
+                                        nrow(data), ncol(data)))
+                        message(sprintf("\n    Data preview for module %s:", module_name))
+                        print(head(data))
+                    }
+                } else {
+                    message("Skip header.")
+                }
+            }
+        }
+            
+        output_file <- file.path(
+            output_dir,
+            sprintf("%s_%s_fastqc_%s%s", 
+                    TIMESTAMPS$full,
+                    sample_ids[file_idx],  # Add sample ID to filename
+                    module_name,
+                    FASTQC_CONFIG$output_suffix)
+        )
+        # Save module data if we successfully parsed it
+        if (!is.null(data) && !DEBUG_CONFIG$dry_run) {
+            existing_files <- find_timestamped_files(output_file)
+            
+            if (length(existing_files) >= FASTQC_CONFIG$existing_version_limit) {
+                if (DEBUG_CONFIG$verbose) {
+                    cat("[SKIP] Analysis output limit reached. Existing versions:\n")
+                    invisible(lapply(existing_files, function(f) cat(sprintf("  %s\n", basename(f)))))
+                }
+            } else {
+                safe_write_file(
+                    data = data,
+                    path = output_file,
+                    write_fn = write.table,
+                    verbose = DEBUG_CONFIG$verbose,
+                    interactive = FALSE,
+                    sep = "\t",
+                    row.names = FALSE,
+                    quote = FALSE
+                )
+            }
+        } else {
+            message(sprintf("    Would write module data to: %s", 
+                    basename(output_file)))
+        }
+    }
+    
+    summary_file <- file.path(
+        output_dir,
+        sprintf("%s_%s_fastqc_summary%s",
+                TIMESTAMPS$full,
+                sample_ids[file_idx],  # Add sample ID to filename
+                FASTQC_CONFIG$output_suffix)
+    )
+
+    summary_data <- read.table(
+        text = unlist(fastqc_summary),
+        header = FALSE,
+        col.names = c("Stat", "Value"),
+        sep = "\t"
+    )
+        
+    stopifnot(
+        `Summary must contain data` = nrow(summary_data) > 0,
+        `Summary must have required columns` = all(c("Stat", "Value") %in% colnames(summary_data))
+    )
+
+    if (DEBUG_CONFIG$verbose) {
+        message("\n  Summary data:")
+        print(head(summary_data))
+    }
+
+    # Save summary for this file
+    if (!DEBUG_CONFIG$dry_run) {
+        existing_files <- find_timestamped_files(summary_file)
+            
+        if (length(existing_files) >= FASTQC_CONFIG$existing_version_limit || DEBUG_CONFIG$verbose) {
+            cat("Found existing versions:\n")
+            invisible(lapply(existing_files, function(f) cat(sprintf("  %s\n", basename(f)))))
+        } else {
+            safe_write_file(
+                data = data,
+                path = summary_file,
+                write_fn = write.table,
+                verbose = DEBUG_CONFIG$verbose,
+                interactive = FALSE,
+                sep = "\t",
+                row.names = FALSE,
+                quote = FALSE
+            )
+        }
+    } else {
+        if (DEBUG_CONFIG$verbose) {
+            message(sprintf("  Would write summary to: %s", basename(summary_file)))
+        }
+    }
+}
+
+message("\nProcessing complete")
+
+if (!DEBUG_CONFIG$dry_run) {
+    if (!file.exists(FASTQC_CONFIG$module_reference_file)) {
+        success <- safe_write_file(
+            data = FASTQC_CONFIG$module_names,
+            path = FASTQC_CONFIG$module_reference_file,
+            write_fn = saveRDS,
+            verbose = DEBUG_CONFIG$verbose,
+            interactive = FALSE
+        )
+        
+        if (!success) {
+            warning("Failed to save FastQC module reference")
+        } else {
+            # Simple validation
+            tryCatch({
+                readRDS(FASTQC_CONFIG$module_reference_file)
+                if (DEBUG_CONFIG$verbose) {
+                    message("FastQC module reference was read successfully")
+                }
+            }, error = function(e) {
+                warning("Failed to validate saved FastQC module reference")
+            })
+        }
+    } else {
+        message("FastQC module reference already exists")
+    }
+}
diff --git a/failsafe_scripts/plot_viewer_utility.R b/failsafe_scripts/plot_viewer_utility.R
new file mode 100644
index 0000000..f8f29f1
--- /dev/null
+++ b/failsafe_scripts/plot_viewer_utility.R
@@ -0,0 +1,63 @@
+#!/usr/bin/env Rscript
+
+# Plot Viewer Configuration
+#-----------------------------------------------------------------------------
+DEBUG_CONFIG <- list(
+    enabled = FALSE,          # Enable debug output
+    verbose = TRUE,           # Print processing details
+    display_time = 2,         # Seconds between auto-advance
+    interactive = TRUE        # Enable interactive viewing
+)
+
+VIEWER_CONFIG <- list(
+    base_dir = file.path(Sys.getenv("HOME"), "data"),
+    patterns = list(
+        svg = "\\.svg$",
+        timestamp = "^[0-9]{8}_[0-9]{6}",  # YYYYMMDD_HHMMSS
+        experiment = "^[0-9]{6}Bel"
+    ),
+    device = list(
+        width = 10,
+        height = 8
+    )
+)
+
+# Required Packages and Functions
+#-----------------------------------------------------------------------------
+required_packages <- c("rsvg", "magick")
+for (pkg in required_packages) {
+    if (!base::requireNamespace(pkg, quietly = TRUE)) {
+        stop(sprintf("Package '%s' is missing", pkg))
+    }
+}
+
+source("~/lab_utils/failsafe_scripts/functions_for_plotting_utilities.R")
+# Main Processing
+#-----------------------------------------------------------------------------
+# Command line argument processing could be added here
+
+files <- find_plot_files(
+    base_dir = VIEWER_CONFIG$base_dir,
+    patterns = VIEWER_CONFIG$patterns,
+    experiment = "241007Bel",
+    #timestamp = "20231116",  # Optional
+    #pattern = "chr10",      # Optional
+    verbose = DEBUG_CONFIG$verbose
+)
+
+if (length(files) > 0) {
+    if (DEBUG_CONFIG$verbose) {
+        base::message("\nStarting plot display...")
+    }
+    
+    # Usage in main script
+    display_plots(
+        files = files,
+        device_config = VIEWER_CONFIG$device,
+        interactive = DEBUG_CONFIG$interactive,
+        display_time = DEBUG_CONFIG$display_time,
+        verbose = DEBUG_CONFIG$verbose
+    )
+} else {
+    base::message("No files found matching criteria")
+}
diff --git a/failsafe_scripts/reset_environment.R b/failsafe_scripts/reset_environment.R
new file mode 100644
index 0000000..27fe516
--- /dev/null
+++ b/failsafe_scripts/reset_environment.R
@@ -0,0 +1,78 @@
+#==============================================================================
+# Environment Reset and Setup Script
+# Purpose: Create clean, reproducible environment for R analysis
+# Updated: 2024-11-14
+#==============================================================================
+
+# 1. Complete Environment Reset
+#------------------------------------------------------------------------------
+rm(list = ls(all.names = TRUE))  # Remove all objects, including hidden ones
+gc(full = TRUE)                  # Force garbage collection
+if (!is.null(dev.list())) dev.off() # Close all open graphics devices
+
+# 2. Reset Random Seed
+#------------------------------------------------------------------------------
+set.seed(42)  # Set consistent seed for reproducibility
+
+# 3. Reset Graphics Parameters
+#------------------------------------------------------------------------------
+par(mar = c(5.1, 4.1, 4.1, 2.1))  # Reset to R defaults
+options(scipen = 999)              # Disable scientific notation
+options(digits = 7)                # Reset numerical precision
+
+# 4. Reset Working Directory (optional - uncomment if needed)
+#------------------------------------------------------------------------------
+# setwd("/your/project/path")
+
+# 5. Package Management
+#------------------------------------------------------------------------------
+# Function to install missing packages
+install_if_missing <- function(packages) {
+    new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
+    if (length(new_packages)) install.packages(new_packages)
+    invisible()
+}
+
+# Core packages for analysis (customize as needed)
+required_packages <- c(
+    "tidyverse",
+    "data.table",
+    "Matrix",
+    "parallel"
+)
+
+# Install missing packages
+install_if_missing(required_packages)
+
+# Load packages with suppressed messages
+suppressMessages({
+    for (pkg in required_packages) {
+        library(pkg, character.only = TRUE)
+    }
+})
+
+# 6. Set Global Options
+#------------------------------------------------------------------------------
+options(
+    stringsAsFactors = FALSE,     # Prevent automatic factor conversion
+    warn = 1,                     # Show warnings immediately
+    encoding = "UTF-8",           # Set default encoding
+    timeout = 3600               # Set timeout for downloads
+)
+
+# 7. Custom Error Handler (optional)
+#------------------------------------------------------------------------------
+options(error = function() {
+    cat("Error occurred at:", date(), "\n")
+    traceback(2)
+})
+
+# 8. Memory Management
+#------------------------------------------------------------------------------
+memory.limit(size = NA)  # Maximum memory available (Windows only)
+
+# 9. Session Information
+#------------------------------------------------------------------------------
+cat("\nEnvironment successfully reset at:", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n")
+cat("R Version:", R.version.string, "\n")
+cat("Working Directory:", getwd(), "\n\n")
diff --git a/failsafe_scripts/run_bam_qc_array.sh b/failsafe_scripts/run_bam_qc_array.sh
new file mode 100644
index 0000000..26e985f
--- /dev/null
+++ b/failsafe_scripts/run_bam_qc_array.sh
@@ -0,0 +1,141 @@
+#!/bin/bash
+# run_bam_qc_array.sh
+# Purpose: Execute BAM Quality Control using Samtools
+# Version: 1.1.0
+# Compatibility: Bash 4.2+, SLURM
+
+# Strict error handling
+set -euo pipefail
+
+# Validate input arguments
+if [[ $# -ne 2 ]]; then
+    echo "Usage: $0 <experiment_directory> <bam_subdirectory>"
+    exit 1
+fi
+
+# Parse arguments
+readonly EXPERIMENT_DIR="$1"
+readonly BAM_SUBDIR="$2"
+
+# Validate SLURM array job context
+if [[ -z "${SLURM_ARRAY_TASK_ID:-}" ]]; then
+    echo "Error: This script must be run as a SLURM array job"
+    exit 1
+fi
+
+# Logging configuration
+readonly CURRENT_MONTH=$(date +%Y-%m)
+readonly LOG_ROOT="${HOME}/logs"
+readonly MONTH_DIR="${LOG_ROOT}/${CURRENT_MONTH}"
+readonly TOOL_DIR="${MONTH_DIR}/bam_qc"
+readonly JOB_LOG_DIR="${TOOL_DIR}/job_${SLURM_ARRAY_JOB_ID}"
+readonly TASK_LOG_DIR="${JOB_LOG_DIR}/task_${SLURM_ARRAY_TASK_ID}"
+readonly TIMESTAMP=$(date +%Y%m%d_%H%M%S)
+
+# Log file paths
+readonly MAIN_LOG="${TASK_LOG_DIR}/main_${TIMESTAMP}.log"
+readonly ERROR_LOG="${TASK_LOG_DIR}/error_${TIMESTAMP}.log"
+readonly PERFORMANCE_LOG="${TASK_LOG_DIR}/performance_${TIMESTAMP}.log"
+
+# Quality Control Configuration
+readonly MIN_MAPPING_QUALITY=20
+readonly MIN_INSERT_SIZE=0
+readonly MAX_INSERT_SIZE=1000
+
+# Create log and output directories
+mkdir -p "${TASK_LOG_DIR}"
+readonly QUALITY_CONTROL_DIR="${EXPERIMENT_DIR}/quality_control/bam"
+mkdir -p "${QUALITY_CONTROL_DIR}"
+
+# Logging functions
+log_message() {
+    local level="$1"
+    local message="$2"
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] [${level}] [Task ${SLURM_ARRAY_TASK_ID}] ${message}" | tee -a "${MAIN_LOG}"
+}
+
+# Performance logging functions
+log_performance() {
+    local stage="$1"
+    local duration="$2"
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] ${stage}: ${duration} seconds" >> "${PERFORMANCE_LOG}"
+}
+
+# Measure command execution time
+measure_performance() {
+    local stage="$1"
+    shift
+    local start_time=$(date +%s)
+    "$@" 2>> "${ERROR_LOG}"
+    local status=$?
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    log_performance "${stage}" "${duration}"
+    return $status
+}
+
+# Find BAM files
+BAM_DIR="${EXPERIMENT_DIR}/${BAM_SUBDIR}"
+mapfile -t BAM_FILES < <(find "$BAM_DIR" -maxdepth 1 \( -name "*.sorted.bam" -o -name "*.bam" \))
+TOTAL_FILES=${#BAM_FILES[@]}
+
+# Validate array task
+if [[ ${SLURM_ARRAY_TASK_ID} -gt ${TOTAL_FILES} ]]; then
+    log_message "ERROR" "Task ID exceeds number of files"
+    exit 1
+fi
+
+# Select current file
+BAM_INDEX=$((SLURM_ARRAY_TASK_ID - 1))
+BAM_PATH="${BAM_FILES[${BAM_INDEX}]}"
+SAMPLE_NAME=$(basename "${BAM_PATH}" | sed -E 's/\.(sorted\.)?bam$//')
+
+# Load required modules
+module purge
+module load samtools
+
+# Comprehensive Samtools Quality Control
+log_message "INFO" "Generating comprehensive BAM quality control for ${SAMPLE_NAME}"
+
+# 1. Basic Statistics
+log_message "INFO" "Generating Samtools stats"
+measure_performance "samtools_stats" samtools stats \
+    -q "${MIN_MAPPING_QUALITY}" \
+    "${BAM_PATH}" > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.samtools_stats.txt"
+
+# 2. Flagstat (Quick mapping statistics)
+log_message "INFO" "Generating Flagstat report"
+measure_performance "samtools_flagstat" samtools flagstat \
+    "${BAM_PATH}" > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.flagstat.txt"
+
+# 3. Idxstats (Chromosome-level statistics)
+log_message "INFO" "Generating Idxstats report"
+measure_performance "samtools_idxstats" samtools idxstats \
+    "${BAM_PATH}" > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.idxstats.txt"
+
+# 4. Insert Size Distribution
+log_message "INFO" "Calculating insert size distribution"
+measure_performance "samtools_insert_size" samtools stats \
+    -i "${MIN_INSERT_SIZE}:${MAX_INSERT_SIZE}" \
+    "${BAM_PATH}" > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.insert_size.txt"
+
+# 5. Coverage Depth
+log_message "INFO" "Calculating coverage depth"
+measure_performance "samtools_depth" samtools depth \
+    -a "${BAM_PATH}" > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.depth.txt"
+
+# 6. Generate a comprehensive report
+log_message "INFO" "Generating comprehensive BAM report"
+{
+    echo "Sample: ${SAMPLE_NAME}"
+    echo "BAM File: ${BAM_PATH}"
+    echo "----------------------------"
+    echo "Samtools Stats Summary:"
+    grep "^SN" "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.samtools_stats.txt" | \
+        sed 's/^SN\t//'
+} > "${QUALITY_CONTROL_DIR}/${SAMPLE_NAME}.bam_qc_summary.txt"
+
+# Log completion
+log_message "INFO" "BAM Quality Control completed for ${SAMPLE_NAME}"
diff --git a/failsafe_scripts/run_bamcoverage_normalizations.sh b/failsafe_scripts/run_bamcoverage_normalizations.sh
new file mode 100755
index 0000000..e93e1c3
--- /dev/null
+++ b/failsafe_scripts/run_bamcoverage_normalizations.sh
@@ -0,0 +1,169 @@
+#!/bin/bash
+#SBATCH --nodes=1
+#SBATCH --ntasks=1
+#SBATCH --cpus-per-task=4
+#SBATCH --mem-per-cpu=50G
+#SBATCH --nice=10000
+#SBATCH --exclude=c[5-22]
+#SBATCH --mail-type=ALL
+#SBATCH --mail-user=luised94@mit.edu
+# Script: run_bamcoverage_array.sh
+# Purpose: Executes deepTools bamCoverage as SLURM array job for multiple BAM files
+# Usage: sbatch --array=1-N%16 run_bamcoverage_array.sh <experiment_directory>
+# Date: 2024-11-03
+
+# Function to display usage
+display_usage() {
+    echo "Usage: sbatch --array=1-N%16 $0 <experiment_directory>"
+    echo "Example: sbatch --array=1-10%16 $0 /home/user/data/240304Bel"
+    exit 1
+}
+
+# Validate input arguments
+if [ "$#" -ne 1 ]; then
+    display_usage
+fi
+
+# Parse arguments
+EXPERIMENT_DIR="$1"
+
+# Validate SLURM_ARRAY_TASK_ID
+if [ -z "$SLURM_ARRAY_TASK_ID" ]; then
+    echo "Error: This script must be run as a SLURM array job"
+    echo "Use: sbatch --array=1-N%16 $0 <experiment_directory>"
+    exit 1
+fi
+
+# Normalization methods array
+declare -a NORM_METHODS=("RPKM" "CPM" "BPM" "RPGC")
+
+# coverage parameters - hardcoded values
+BIN_SIZE=10
+EFFECTIVE_GENOME_SIZE=12157105
+MIN_MAPPING_QUALITY=20
+
+# Logging setup
+readonly CURRENT_MONTH=$(date +%Y-%m)
+readonly LOG_ROOT="$HOME/logs"
+readonly MONTH_DIR="${LOG_ROOT}/${CURRENT_MONTH}"
+readonly TOOL_DIR="${MONTH_DIR}/bamcoverage"
+readonly JOB_LOG_DIR="${TOOL_DIR}/job_${SLURM_ARRAY_JOB_ID}"
+readonly TASK_LOG_DIR="${JOB_LOG_DIR}/task_${SLURM_ARRAY_TASK_ID}"
+readonly TIMESTAMP=$(date +%Y%m%d_%H%M%S)
+readonly MAIN_LOG="${TASK_LOG_DIR}/main_${TIMESTAMP}.log"
+readonly ERROR_LOG="${TASK_LOG_DIR}/error_${TIMESTAMP}.log"
+readonly PERFORMANCE_LOG="${TASK_LOG_DIR}/performance_${TIMESTAMP}.log"
+
+# Create log directories
+mkdir -p "${TASK_LOG_DIR}"
+mkdir -p "${EXPERIMENT_DIR}/coverage"
+
+
+# Logging functions
+log_message() {
+    local level=$1
+    local message=$2
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] [${level}] [Task ${SLURM_ARRAY_TASK_ID}] ${message}" | tee -a "${MAIN_LOG}"
+}
+
+log_error() {
+    log_message "ERROR" "$1" >&2
+    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >> "${ERROR_LOG}"
+}
+
+log_performance() {
+    local stage=$1
+    local duration=$2
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] ${stage}: ${duration} seconds" >> "${PERFORMANCE_LOG}"
+}
+
+measure_performance() {
+    local stage=$1
+    shift
+    local start_time=$(date +%s)
+    "$@" 2>> "${ERROR_LOG}"
+    local status=$?
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    log_performance "${stage}" "${duration}"
+    return $status
+}
+
+# Log script start
+log_message "INFO" "Starting bamCoverage process for experiment: ${EXPERIMENT_DIR}"
+log_message "INFO" "Job ID: ${SLURM_ARRAY_JOB_ID}, Task ID: ${SLURM_ARRAY_TASK_ID}"
+log_message "INFO" "Log directory: ${TASK_LOG_DIR}"
+
+# Load required modules
+module purge
+module load python
+module load deeptools
+
+# Find BAM files
+BAM_DIR="${EXPERIMENT_DIR}/alignment"
+mapfile -t BAM_FILES < <(find "$BAM_DIR" -maxdepth 1 -type f -name "*_sorted.bam" | sort)
+TOTAL_FILES=${#BAM_FILES[@]}
+TOTAL_JOBS=$((TOTAL_FILES * ${#NORM_METHODS[@]}))
+if [ $TOTAL_FILES -eq 0 ]; then
+    log_message "ERROR" "No BAM files found in ${BAM_DIR}"
+    exit 1
+fi
+
+# Validate array range
+if [ $SLURM_ARRAY_TASK_ID -gt $TOTAL_JOBS ]; then
+    log_message "WARNING" "Task ID ${SLURM_ARRAY_TASK_ID} exceeds number of jobs ${TOTAL_FILES}"
+    exit 1
+fi
+
+# Get current BAM file
+# Calculate array indices
+BAM_INDEX=$(( (SLURM_ARRAY_TASK_ID - 1) / ${#NORM_METHODS[@]} ))
+NORM_INDEX=$(( (SLURM_ARRAY_TASK_ID - 1) % ${#NORM_METHODS[@]} ))
+
+# Get current BAM file and normalization method
+BAM_PATH="${BAM_FILES[$BAM_INDEX]}"
+NORM_METHOD="${NORM_METHODS[$NORM_INDEX]}"
+if [ ! -f $BAM_PATH ]; then
+    log_message "WARNING" "Task ID ${SLURM_ARRAY_TASK_ID} bam path does not exist."
+    log_message "WARNING" "Input: ${BAM_PATH}"
+    exit 1
+fi
+# Set output name
+SAMPLE_NAME=$(basename --suffix=_sorted.bam "$BAM_PATH" )
+OUTPUT_BIGWIG="${EXPERIMENT_DIR}/coverage/${SAMPLE_NAME}_${NORM_METHOD}.bw"
+
+log_message "INFO" "Processing sample: ${SAMPLE_NAME}"
+log_message "INFO" "Normalization method: ${NORM_METHOD}"
+log_message "INFO" "Input: ${BAM_PATH}"
+log_message "INFO" "Output: ${OUTPUT_BIGWIG}"
+
+# Build bamCoverage command with specific parameters for each method
+COMMON_PARAMS="--bam ${BAM_PATH} \
+    --outFileName ${OUTPUT_BIGWIG} \
+    --binSize ${BIN_SIZE} \
+    --minMappingQuality ${MIN_MAPPING_QUALITY} \
+    --ignoreDuplicates \
+    --normalizeUsing ${NORM_METHOD} \
+    --numberOfProcessors ${SLURM_CPUS_PER_TASK}"
+
+# Add RPGC-specific parameters
+if [ "${NORM_METHOD}" == "RPGC" ]; then
+    COMMON_PARAMS+=" --effectiveGenomeSize ${EFFECTIVE_GENOME_SIZE}"
+    log_message "INFO" "Added effectiveGenomeSize parameter for RPGC normalization"
+fi
+
+# Execute bamCoverage
+log_message "INFO" "Starting bamCoverage processing"
+if measure_performance "bamcoverage" bamCoverage $COMMON_PARAMS; then
+    log_message "INFO" "Successfully completed processing for ${SAMPLE_NAME}"
+else
+    log_error "bamCoverage processing failed for ${SAMPLE_NAME}"
+    exit 1
+fi
+
+log_message "INFO" "Parameters used:\n"
+log_message "INFO" "$COMMON_PARAMS"
+# Log completion
+log_message "INFO" "Task completed successfully"
diff --git a/failsafe_scripts/run_bowtie2_array_alignment.sh b/failsafe_scripts/run_bowtie2_array_alignment.sh
new file mode 100755
index 0000000..579d5b4
--- /dev/null
+++ b/failsafe_scripts/run_bowtie2_array_alignment.sh
@@ -0,0 +1,171 @@
+#!/bin/bash
+#SBATCH --nodes=1
+#SBATCH --ntasks=1
+#SBATCH --cpus-per-task=4
+#SBATCH --mem-per-cpu=50G
+#SBATCH --nice=10000
+#SBATCH --exclude=c[5-22]
+#SBATCH --mail-type=ALL
+#SBATCH --mail-user=luised94@mit.edu
+# Script: run_bowtie2_array_alignment.sh
+# Purpose: Executes bowtie2 alignment as SLURM array job for multiple fastq files
+# Usage: sbatch --array=1-N%16 run_bowtie2_array_alignment.sh <experiment_directory>
+# Author: [Your Name]
+# Date: 2024-11-03
+
+# Function to display usage
+display_usage() {
+    echo "Usage: sbatch --array=1-N%16 $0 <experiment_directory>"
+    echo "Example: sbatch --array=1-10%16 $0 /home/user/data/240304Bel"
+    echo "Note: Array range should not exceed the number of fastq files"
+    exit 1
+}
+
+# Validate input arguments
+if [ "$#" -ne 1 ]; then
+    display_usage
+fi
+
+# Parse arguments
+EXPERIMENT_DIR="$1"
+
+# Validate SLURM_ARRAY_TASK_ID
+if [ -z "$SLURM_ARRAY_TASK_ID" ]; then
+    echo "Error: This script must be run as a SLURM array job"
+    echo "Use: sbatch --array=1-N%16 $0 <experiment_directory>"
+    exit 1
+fi
+
+# Function to validate array range
+validate_array_range() {
+    local total_files=$1
+    local array_start=$(echo $SLURM_ARRAY_TASK_MIN)
+    local array_end=$(echo $SLURM_ARRAY_TASK_MAX)
+    log_message "Validating array range..."
+    log_message "Total fastq files: $total_files"
+    log_message "Array range: $array_start-$array_end"
+    if [ $array_end -gt $total_files ]; then
+        log_message "WARNING: Array range ($array_end) exceeds number of fastq files ($total_files)"
+        log_message "Suggestion: Use --array=1-${total_files}%16"
+    fi
+}
+
+# Constants
+GENOME_DIR="$HOME/data/REFGENS/SaccharomycescerevisiaeS288C"
+GENOME_INDEX="$GENOME_DIR/SaccharomycescerevisiaeS288C_index"
+
+# Logging setup
+readonly CURRENT_MONTH=$(date +%Y-%m)
+readonly LOG_ROOT="$HOME/logs"
+readonly MONTH_DIR="${LOG_ROOT}/${CURRENT_MONTH}"
+readonly TOOL_DIR="${MONTH_DIR}/bowtie2_alignment"
+readonly JOB_LOG_DIR="${TOOL_DIR}/job_${SLURM_ARRAY_JOB_ID}"
+readonly TASK_LOG_DIR="${JOB_LOG_DIR}/task_${SLURM_ARRAY_TASK_ID}"
+readonly TIMESTAMP=$(date +%Y%m%d_%H%M%S)
+readonly MAIN_LOG="${TASK_LOG_DIR}/main_${TIMESTAMP}.log"
+readonly ERROR_LOG="${TASK_LOG_DIR}/error_${TIMESTAMP}.log"
+readonly PERFORMANCE_LOG="${TASK_LOG_DIR}/performance_${TIMESTAMP}.log"
+
+# Create log directories
+mkdir -p "${TASK_LOG_DIR}"
+mkdir -p "${EXPERIMENT_DIR}/alignment"
+
+# Function to log messages
+log_message() {
+    local level=$1
+    local message=$2
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] [${level}] [Task ${SLURM_ARRAY_TASK_ID}] ${message}" | tee -a "${MAIN_LOG}"
+}
+
+# Function to log performance metrics
+log_performance() {
+    local stage=$1
+    local duration=$2
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] ${stage}: ${duration} seconds" >> "${PERFORMANCE_LOG}"
+}
+
+# Function to measure command execution time
+measure_performance() {
+    local stage=$1
+    shift
+    local start_time=$(date +%s)
+    "$@" 2>> "${ERROR_LOG}"
+    local status=$?
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    log_performance "${stage}" "${duration}"
+    return $status
+}
+
+
+if [ ! -f "${GENOME_INDEX}.1.bt2" ]; then
+    log_message "Error: Genome index not found: $GENOME_INDEX"
+    exit 1
+fi
+
+# Log script start
+log_message "INFO" "Starting alignment process for experiment: ${EXPERIMENT_DIR}"
+log_message "INFO" "Job ID: ${SLURM_ARRAY_JOB_ID}, Task ID: ${SLURM_ARRAY_TASK_ID}"
+log_message "INFO" "Log directory: ${TASK_LOG_DIR}"
+
+# Load required modules
+module purge
+module load bowtie2
+module load samtools
+
+# Find fastq files
+FASTQ_DIR="${EXPERIMENT_DIR}/fastq"
+mapfile -t FASTQ_FILES < <(find "$FASTQ_DIR" -maxdepth 1 -type f -name "*.fastq" | sort)
+TOTAL_FILES=${#FASTQ_FILES[@]}
+
+if [ $TOTAL_FILES -eq 0 ]; then
+    log_message "ERROR" "No fastq files found in ${FASTQ_DIR}"
+    exit 1
+fi
+
+# Validate array range against number of files
+#validate_array_range $TOTAL_FILES
+
+# Get current fastq file
+FASTQ_INDEX=$((SLURM_ARRAY_TASK_ID - 1))
+FASTQ_PATH="${FASTQ_FILES[$FASTQ_INDEX]}"
+
+if [ -z "$FASTQ_PATH" ]; then
+    log_message "Error: No fastq file found for index $FASTQ_INDEX"
+    exit 1
+fi
+
+# Generate output name
+SAMPLE_NAME=$(basename --suffix=.fastq "$FASTQ_PATH" )
+OUTPUT_BAM="${EXPERIMENT_DIR}/alignment/${SAMPLE_NAME}_to_S288C_sorted.bam"
+
+log_message "INFO" "Processing sample: ${SAMPLE_NAME}"
+log_message "INFO" "Input: ${FASTQ_PATH}"
+log_message "INFO" "Output: ${OUTPUT_BAM}"
+
+# Alignment and sorting
+log_message "INFO" "Starting alignment and sorting"
+if measure_performance "alignment_and_sorting" \
+    bowtie2 -x "$GENOME_INDEX" \
+            -U "$FASTQ_PATH" \
+            -p "$SLURM_CPUS_PER_TASK" 2>> "${ERROR_LOG}" | \
+    samtools view -@ "$SLURM_CPUS_PER_TASK" -bS - 2>> "${ERROR_LOG}" | \
+    samtools sort -@ "$SLURM_CPUS_PER_TASK" -o "$OUTPUT_BAM" - 2>> "${ERROR_LOG}"; then
+# "-q --mp 4 --met-stderr"
+
+    log_message "INFO" "Starting BAM indexing"
+    if measure_performance "indexing" samtools index "$OUTPUT_BAM"; then
+        log_message "INFO" "Successfully completed processing for ${SAMPLE_NAME}"
+    else
+        log_message "ERROR" "BAM indexing failed for ${SAMPLE_NAME}"
+        exit 1
+    fi
+else
+    log_message "ERROR" "Alignment/sorting failed for ${SAMPLE_NAME}"
+    exit 1
+fi
+
+# Log completion
+log_message "INFO" "Task completed successfully"
diff --git a/failsafe_scripts/run_fastqc_array.sh b/failsafe_scripts/run_fastqc_array.sh
new file mode 100755
index 0000000..8650ab7
--- /dev/null
+++ b/failsafe_scripts/run_fastqc_array.sh
@@ -0,0 +1,119 @@
+#!/bin/bash
+#SBATCH --nodes=1
+#SBATCH --ntasks=1
+#SBATCH --cpus-per-task=4
+#SBATCH --mem-per-cpu=50G
+#SBATCH --nice=10000
+#SBATCH --exclude=c[5-22]
+#SBATCH --mail-type=ALL
+#SBATCH --mail-user=luised94@mit.edu
+# run_fastqc_array.sh
+# Purpose: Execute FastQC as a SLURM array job
+# Version: 1.1.0
+# Compatibility: Bash 4.2+, SLURM
+
+# Strict error handling
+set -euo pipefail
+
+# Validate input arguments
+if [[ $# -ne 2 ]]; then
+    echo "Usage: $0 <experiment_directory> <fastq_subdirectory>"
+    exit 1
+fi
+
+# Parse arguments
+readonly EXPERIMENT_DIR="$1"
+readonly FASTQ_SUBDIR="$2"
+
+# Validate SLURM array job context
+if [[ -z "${SLURM_ARRAY_TASK_ID:-}" ]]; then
+    echo "Error: This script must be run as a SLURM array job"
+    exit 1
+fi
+
+# Logging configuration
+readonly CURRENT_MONTH=$(date +%Y-%m)
+readonly LOG_ROOT="${HOME}/logs"
+readonly MONTH_DIR="${LOG_ROOT}/${CURRENT_MONTH}"
+readonly TOOL_DIR="${MONTH_DIR}/fastqc"
+readonly JOB_LOG_DIR="${TOOL_DIR}/job_${SLURM_ARRAY_JOB_ID}"
+readonly TASK_LOG_DIR="${JOB_LOG_DIR}/task_${SLURM_ARRAY_TASK_ID}"
+readonly TIMESTAMP=$(date +%Y%m%d_%H%M%S)
+
+# Log file paths
+readonly MAIN_LOG="${TASK_LOG_DIR}/main_${TIMESTAMP}.log"
+readonly ERROR_LOG="${TASK_LOG_DIR}/error_${TIMESTAMP}.log"
+readonly PERFORMANCE_LOG="${TASK_LOG_DIR}/performance_${TIMESTAMP}.log"
+
+# Create log directories
+mkdir -p "${TASK_LOG_DIR}"
+readonly QUALITY_CONTROL_DIR="${EXPERIMENT_DIR}/quality_control"
+mkdir -p "${QUALITY_CONTROL_DIR}"
+
+# Logging functions
+log_message() {
+    local level="$1"
+    local message="$2"
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] [${level}] [Task ${SLURM_ARRAY_TASK_ID}] ${message}" | tee -a "${MAIN_LOG}"
+}
+
+# Function to log performance metrics
+log_performance() {
+    local stage=$1
+    local duration=$2
+    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
+    echo "[${timestamp}] ${stage}: ${duration} seconds" >> "${PERFORMANCE_LOG}"
+}
+
+# Function to measure command execution time
+measure_performance() {
+    local stage=$1
+    shift
+    local start_time=$(date +%s)
+    "$@" 2>> "${ERROR_LOG}"
+    local status=$?
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    log_performance "${stage}" "${duration}"
+    return $status
+}
+
+# Find FASTQ files
+FASTQ_DIR="${EXPERIMENT_DIR}/${FASTQ_SUBDIR}"
+mapfile -t FASTQ_FILES < <(find "$FASTQ_DIR" -maxdepth 1 \( -name "*.fastq" -o -name "*.fq" \))
+TOTAL_FILES=${#FASTQ_FILES[@]}
+
+# Validate array task
+if [[ ${SLURM_ARRAY_TASK_ID} -gt ${TOTAL_FILES} ]]; then
+    log_message "ERROR" "Task ID exceeds number of files"
+    exit 1
+fi
+
+# Select current file
+FASTQ_INDEX=$((SLURM_ARRAY_TASK_ID - 1))
+FASTQ_PATH="${FASTQ_FILES[${FASTQ_INDEX}]}"
+SAMPLE_NAME=$(basename "${FASTQ_PATH}" | sed -E 's/\.(fastq|fq)$//')
+
+# Load required modules
+module purge
+module load fastqc
+module load java
+
+# Execute FastQC with performance measurement
+log_message "INFO" "Processing sample: ${SAMPLE_NAME}"
+log_message "INFO" "Input file: ${FASTQ_PATH}"
+
+if measure_performance "fastqc" fastqc \
+    --outdir "${QUALITY_CONTROL_DIR}" \
+    --threads "${SLURM_CPUS_PER_TASK:-1}" \
+    "${FASTQ_PATH}"; then
+    
+    log_message "INFO" "FastQC completed successfully for ${SAMPLE_NAME}"
+else
+    log_message "ERROR" "FastQC processing failed for ${SAMPLE_NAME}"
+    exit 1
+fi
+
+# Log completion
+log_message "INFO" "Task completed successfully"
diff --git a/failsafe_scripts/sample_processing.R b/failsafe_scripts/sample_processing.R
new file mode 100644
index 0000000..560311e
--- /dev/null
+++ b/failsafe_scripts/sample_processing.R
@@ -0,0 +1,119 @@
+# Validate and prepare categories for labeling
+sample_validate_categories <- function(table, categories) {
+    if (!is.data.frame(table)) {
+        stop("Input table must be a data frame")
+    }
+    if (!is.character(categories) || length(categories) == 0) {
+        stop("Categories must be a non-empty character vector")
+    }
+    
+    # Ensure antibody category is included
+    if (!"antibody" %in% categories) {
+        categories <- c("antibody", categories)
+    }
+    
+    # Validate categories exist in table
+    missing_categories <- setdiff(categories, colnames(table))
+    if (length(missing_categories) > 0) {
+        stop("Missing categories in table: ", 
+             paste(missing_categories, collapse = ", "))
+    }
+    
+    return(categories)
+}
+
+# Get unique values for each category
+sample_get_unique_values <- function(table, categories) {
+    unique_values <- lapply(table[categories], unique)
+    #names(unique_values) <- categories
+    return(unique_values)
+}
+
+# Create label for a single sample
+sample_create_label <- function(sample, categories, unique_values) {
+    relevant_categories <- sapply(categories, function(cat) {
+        if (length(unique_values[[cat]]) > 1 || cat == "antibody") {
+            return(as.character(sample[cat]))
+        }
+        return(NULL)
+    })
+    
+    # Filter out NULL values and combine
+    valid_categories <- relevant_categories[!sapply(relevant_categories, is.null)]
+    return(paste(valid_categories, collapse = "_"))
+}
+
+# Main function to generate unique labels
+sample_generate_labels <- function(table, categories_for_label, verbose = FALSE) {
+    # Validate and prepare categories
+    validated_categories <- sample_validate_categories(table, categories_for_label)
+    
+    # Get unique values
+    unique_vals <- sample_get_unique_values(table, validated_categories)
+    
+    # Generate labels for each row
+    labels <- apply(table, 1, function(row) {
+        sample_create_label(row, validated_categories, unique_vals)
+    })
+    
+    if (verbose) {
+        message("Categories used: ", paste(validated_categories, collapse = ", "))
+        message("Unique values by category:")
+        print(unique_vals)
+        message("Generated labels:")
+        print(labels)
+    }
+    
+    return(unname(labels))
+}
+
+unique_labeling <- function(table, categories_for_label) {
+    # Input validation
+    if (!is.data.frame(table)) {
+        stop("Input 'table' must be a data frame")
+    }
+    if (!is.character(categories_for_label) || length(categories_for_label) == 0) {
+        stop("Input 'categories_for_label' must be a non-empty character vector")
+    }
+    
+    # Ensure antibody category is always included
+    if (!"antibody" %in% categories_for_label) {
+    categories_for_label <- c("antibody", categories_for_label)
+    }
+    
+    print(paste("Categories for label:", paste(categories_for_label, collapse = ", ")))
+    
+    # Check if all categories exist in the table
+    missing_categories <- setdiff(categories_for_label, colnames(table))
+    if (length(missing_categories) > 0) {
+        stop(paste("The following categories are missing from the table:", 
+        paste(missing_categories, collapse = ", ")))
+    }
+    
+    # Identify unique values for each category
+    unique_values <- lapply(table[categories_for_label], unique)
+    print("Unique values for each category:")
+    print(unique_values)
+    
+    # Function to construct label for a single sample
+    construct_label <- function(sample) {
+    differing_categories <- sapply(categories_for_label, function(cat) {
+        if (length(unique_values[[cat]]) > 1 || cat == "antibody") {
+            return(sample[cat])
+            #return(paste(cat, sample[cat], sep = ": "))
+        } else {
+            return(NULL)
+        }
+    })
+        differing_categories <- differing_categories[!sapply(differing_categories, is.null)]
+        return(paste(differing_categories, collapse = "_"))
+    }
+    
+    # Apply the construct_label function to each sample (row)
+    labels <- apply(table, 1, construct_label)
+    
+    print("Constructed labels:")
+    print(labels)
+    
+    return(unlist(labels))
+}
diff --git a/failsafe_scripts/setup_bmc_experiment.R b/failsafe_scripts/setup_bmc_experiment.R
new file mode 100644
index 0000000..d7aa4ae
--- /dev/null
+++ b/failsafe_scripts/setup_bmc_experiment.R
@@ -0,0 +1,409 @@
+################################################################################
+# BMC Experiment Setup Script
+################################################################################
+#
+# PURPOSE:
+#   Creates standardized directory structure and metadata files for BMC ChIP-seq
+#   experiments, including sample tracking and submission documents.
+#
+# USAGE:
+#   1. Use '/!!' in vim/neovim to jump to required updates
+#   2. Update experiment_id (format: YYMMDD'Bel', e.g., "241122Bel")
+#   3. Set DEBUG_CONFIG options as needed
+#   4. Source or run script
+#
+# !! ----> REQUIRED UPDATES:
+# !! experiment_id <- "241122Bel"
+# !! DEBUG_CONFIG <- list(
+# !!      enabled = FALSE,
+# !!      verbose = TRUE,
+# !!      interactive = TRUE,
+# !!      dry_run = FALSE
+# !!  )
+
+# INPUTS:
+#   - experiment_id: 9-character experiment identifier
+#   - bmc_config.R: External configuration defining experimental design
+#
+# OUTPUTS:
+#   1. Directory Structure:
+#      ~/data/[experiment_id]/
+#      +-- peak/
+#      +-- fastq/
+#      |   +-- raw/
+#      |   +-- processed/
+#      +-- alignment/
+#      +-- bigwig/
+#      +-- plots/
+#      +-- documentation/
+#
+#   2. Files:
+#      - [experiment_id]_sample_grid.csv: Complete experimental design
+#      - [experiment_id]_bmc_table.tsv: BMC submission metadata
+#      - [experiment_id]_bmc_config.R: Configuration snapshot
+#
+# CONTROLS:
+#   DEBUG_CONFIG$dry_run    = TRUE   # Preview without creating files
+#   DEBUG_CONFIG$verbose    = TRUE   # Show detailed progress
+#   DEBUG_CONFIG$interactive = TRUE  # Confirm before proceeding
+#
+# DEPENDENCIES:
+#   - R base packages only
+#   - ~/lab_utils/failsafe_scripts/bmc_config.R
+#
+# COMMON ISSUES:
+#   1. Wrong experiment ID format -> Check YYMMDD pattern
+#   2. Unexpected sample count -> Review antibody distribution
+#   3. File access denied -> Check ~/data permissions
+#
+# AUTHOR: Luis
+# DATE: 2024-11-27
+# VERSION: 2.0.0
+#
+################################################################################
+################################################################################
+# Configuration and Debug Settings
+################################################################################
+# !! Review debug configuration
+DEBUG_CONFIG <- list(
+    enabled = FALSE,
+    verbose = TRUE,
+    interactive = TRUE,
+    dry_run = FALSE
+)
+
+################################################################################
+# Experiment ID Validation
+################################################################################
+# !! Update experiment ID
+experiment_id <- "241122Bel"
+stopifnot(
+    "Experiment ID must be a character string" = is.character(experiment_id),
+    "Invalid experiment ID format. Expected: YYMMDD'Bel'" = grepl("^\\d{6}Bel$", experiment_id)
+)
+
+################################################################################
+# Load and Validate Experiment Configuration
+################################################################################
+# Bootstrap phase
+bootstrap_path <- normalizePath("~/lab_utils/failsafe_scripts/functions_for_file_operations.R", 
+                              mustWork = FALSE)
+if (!file.exists(bootstrap_path)) {
+    stop(sprintf("[FATAL] Bootstrap file not found: %s", bootstrap_path))
+}
+source(bootstrap_path)
+
+# Define required dependencies
+required_modules <- list(
+    list(
+        path = "~/lab_utils/failsafe_scripts/bmc_config.R",
+        description = "BMC Configuration",
+        required = TRUE
+    )
+)
+
+# Validate module structure
+stopifnot(
+    "modules must have required fields" = all(sapply(required_modules, function(m) {
+        all(c("path", "description", "required") %in% names(m))
+    }))
+)
+
+# Load dependencies with status tracking
+load_status <- lapply(required_modules, function(module) {
+    if (DEBUG_CONFIG$verbose) {
+        cat(sprintf("\n[LOADING] %s\n", module$description))
+    }
+    
+    success <- safe_source(module$path, verbose = TRUE)
+    
+    if (!success && module$required) {
+        stop(sprintf(
+            "[FATAL] Failed to load required module: %s\n  Path: %s",
+            module$description, module$path
+        ))
+    } else if (!success) {
+        warning(sprintf(
+            "[WARNING] Optional module not loaded: %s\n  Path: %s",
+            module$description, module$path
+        ))
+    }
+    
+    return(list(
+        module = module$description,
+        path = module$path,
+        loaded = success
+    ))
+})
+
+# Display loading summary using ASCII
+if (DEBUG_CONFIG$verbose) {
+    cat("\n=== Module Loading Summary ===\n")
+    invisible(lapply(load_status, function(status) {
+        cat(sprintf(
+            "[%s] %s\n    Path: %s\n",
+            if(status$loaded) "+" else "-",
+            status$module,
+            status$path
+        ))
+    }))
+}
+
+stopifnot("Script experiment_id is not the same as CONFIG EXPERIMENT_ID" = experiment_id == EXPERIMENT_CONFIG$METADATA$EXPERIMENT_ID)
+
+################################################################################
+# Directory Setup and User Confirmation
+################################################################################
+base_dir <- file.path(Sys.getenv("HOME"), "data", experiment_id)
+if (DEBUG_CONFIG$interactive) {
+    cat(sprintf("\nExperiment ID: %s\n", experiment_id))
+    cat("Base directory will be:", base_dir, "\n")
+
+    user_response <- readline("Continue with this experiment? (y/n): ")
+    if (tolower(user_response) != "y") {
+        stop("Script terminated by user")
+    }
+    cat("Proceeding with directory creation...\n\n")
+}
+
+################################################################################
+# Directory Structure Definition and Creation
+################################################################################
+# Define directory structure
+data_directories <- c(
+    "peak",
+    "fastq/raw",
+    "fastq/processed",
+    "quality_control",
+    "alignment",
+    "bigwig",
+    "plots/genome_tracks/overview",
+    "plots/genome_tracks/experimental_comparisons",
+    "documentation/dna_qc_traces"
+)
+
+# Create directory structure
+full_paths <- file.path(base_dir, data_directories)
+invisible(lapply(full_paths, function(path) {
+    if (DEBUG_CONFIG$dry_run) {
+        cat(sprintf("[DRY RUN] Would create directory: %s\n", path))
+    } else {
+        dir_created <- dir.create(path, recursive = TRUE, showWarnings = FALSE)
+        if (DEBUG_CONFIG$verbose) {
+            status <- if (dir_created) "Created" else "Already exists"
+            cat(sprintf("[%s] %s\n", status, path))
+        }
+    }
+}))
+
+# Report directory creation status
+if (DEBUG_CONFIG$verbose) {
+    mode <- if (DEBUG_CONFIG$dry_run) "DRY RUN" else "LIVE RUN"
+    cat(sprintf("\n[%s] Directory structure for experiment: %s\n", mode, experiment_id))
+    cat(sprintf("[%s] Base directory: %s\n", mode, base_dir))
+}
+
+cat("Directories created successfully!\n")
+
+################################################################################
+# Sample Metadata Generation and Validation
+################################################################################
+# Generate experimental combinations
+metadata <- do.call(expand.grid, EXPERIMENT_CONFIG$CATEGORIES)
+
+# Filter invalid combinations
+invalid_idx <- Reduce(
+    `|`,
+    lapply(EXPERIMENT_CONFIG$INVALID_COMBINATIONS, eval, envir = metadata)
+)
+metadata <- subset(metadata, !invalid_idx)
+
+# Apply experimental conditions
+valid_idx <- Reduce(
+    `|`,
+    lapply(EXPERIMENT_CONFIG$EXPERIMENTAL_CONDITIONS, eval, envir = metadata)
+)
+metadata <- subset(metadata, valid_idx)
+
+# Verify sample count
+n_samples <- nrow(metadata)
+expected <- EXPERIMENT_CONFIG$METADATA$EXPECTED_SAMPLES
+if (n_samples != expected) {
+    # Print diagnostic information
+    cat("\nDiagnostic Information:\n")
+    cat("----------------------\n")
+    print(table(metadata$antibody))  # Show antibody distribution
+    cat("\nFull sample breakdown:\n")
+    print(summary(metadata))         # Show all category distributions
+    cat("\n")
+
+    stop(sprintf("Expected %d samples, got %d", expected, n_samples))
+}
+
+################################################################################
+# Sample Classification
+################################################################################
+sample_classifications <- EXPERIMENT_CONFIG$SAMPLE_CLASSIFICATIONS
+
+# First, create a matrix/data frame to store all classification results
+classification_results <- matrix(FALSE, 
+                               nrow = nrow(metadata), 
+                               ncol = length(sample_classifications),
+                               dimnames = list(NULL, names(sample_classifications)))
+
+# Evaluate each classification condition
+for (type in names(sample_classifications)) {
+    classification_results[, type] <- eval(sample_classifications[[type]], 
+                                         envir = metadata)
+}
+
+# Create the final classification vector
+metadata$sample_type <- "treatment"  # Default classification
+for (type in names(sample_classifications)) {
+    # Find rows where this classification is TRUE
+    matching_rows <- classification_results[, type]
+    # Assign the type name (removing 'is_' prefix)
+    metadata$sample_type[matching_rows] <- sub("^is_", "", type)
+}
+
+# Validation check
+multiple_classifications <- rowSums(classification_results) > 1
+if (any(multiple_classifications)) {
+    cat("\nERROR: Multiple Classification Detected!\n")
+    cat("----------------------------------------\n")
+    
+    # Show problematic samples with their classifications
+    problem_samples <- metadata[multiple_classifications, ]
+    cat("Samples with multiple classifications:\n\n")
+    
+    # Show which classifications were TRUE for each problematic sample
+    for (i in which(multiple_classifications)) {
+        cat(sprintf("\nSample %d:\n", i))
+        cat("Sample details:\n")
+        print(metadata[i, ])
+        cat("\nMatching classifications:\n")
+        matching_types <- names(classification_results[i,])[classification_results[i,]]
+        print(matching_types)
+        cat("----------------------------------------\n")
+    }
+    
+    stop("Please fix multiple classifications in experiment configuration")
+}
+
+# Success diagnostic display
+cat("\nSample Classification Summary:\n")
+cat("============================\n")
+
+# Overall counts
+cat("\n1. Distribution of sample types:\n")
+print(table(metadata$sample_type))
+
+# Detailed breakdown by relevant factors
+cat("\n2. Sample types by antibody:\n")
+print(table(metadata$sample_type, metadata$antibody))
+
+# Show a few samples from each classification
+cat("\n3. Example samples from each classification:\n")
+for (type in unique(metadata$sample_type)) {
+    cat(sprintf("\n%s samples:\n", toupper(type)))
+    print(metadata[metadata$sample_type == type, ][1:min(3, sum(metadata$sample_type == type)), ])
+    cat("----------------------------------------\n")
+}
+
+# Verification message
+cat("\nClassification Verification:\n")
+cat(sprintf("- Total samples: %d\n", nrow(metadata)))
+cat(sprintf("- Classified samples: %d\n", sum(table(metadata$sample_type))))
+cat(sprintf("- Unclassified samples: %d\n", sum(is.na(metadata$sample_type))))
+
+################################################################################
+# Metadata Formatting and Organization
+################################################################################
+# Enforce factor levels from config
+for (col_name in names(EXPERIMENT_CONFIG$CATEGORIES)) {
+    if (col_name %in% colnames(metadata)) {
+        metadata[[col_name]] <- factor(
+            metadata[[col_name]],
+            levels = EXPERIMENT_CONFIG$CATEGORIES[[col_name]],
+            ordered = TRUE
+        )
+    }
+}
+
+# Sort metadata according to column order
+metadata <- metadata[do.call(
+    order,
+    metadata[EXPERIMENT_CONFIG$COLUMN_ORDER]
+), ]
+
+# Generate sample names
+metadata$full_name <- apply(metadata, 1, paste, collapse = "_")
+metadata$short_name <- apply(metadata[, EXPERIMENT_CONFIG$COLUMN_ORDER], 1,
+    function(x) paste0(substr(x, 1, 1), collapse = ""))
+
+################################################################################
+# BMC Metadata Generation
+################################################################################
+bmc_metadata <- data.frame(
+    SampleName = metadata$full_name,
+    Vol_uL = 10,
+    Conc = 0,
+    Type = "ChIP",
+    Genome = "Saccharomyces cerevisiae",
+    Notes = ifelse(
+        metadata$antibody == "Input",
+        "Run on fragment analyzer.",
+        "Run on femto pulse."
+    ),
+    Pool = "A",
+    stringsAsFactors = FALSE
+)
+
+################################################################################
+# File Output Generation
+################################################################################
+# Define output file paths
+sample_grid_path <- file.path(base_dir, "documentation",
+                             paste0(experiment_id,"_", "sample_grid.csv"))
+
+bmc_table_path <- file.path(base_dir, "documentation",
+                           paste0(experiment_id,"_", "bmc_table.tsv"))
+
+bmc_experiment_config_path <- file.path(base_dir, "documentation",
+                           paste0(experiment_id,"_", "bmc_config.R"))
+
+# Handle file writing with dry run checks
+if (DEBUG_CONFIG$dry_run) {
+    cat(sprintf("[DRY RUN] Would write sample grid to: %s\n", sample_grid_path))
+    cat(sprintf("[DRY RUN] Would write BMC table to: %s\n", bmc_table_path))
+    cat(sprintf("[DRY RUN] Would write BMC config script: %s\n", bmc_experiment_config_path))
+} else {
+    # Write sample grid file
+    safe_write_file(
+        data = metadata,
+        path = sample_grid_path,
+        write_fn = write.csv,
+        verbose = DEBUG_CONFIG$verbose,
+        row.names = FALSE
+    )
+
+    # Write BMC table file
+    safe_write_file(
+        data = bmc_metadata,
+        path = bmc_table_path,
+        write_fn = write.table,
+        verbose = DEBUG_CONFIG$verbose,
+        sep = "\t",
+        row.names = FALSE,
+        quote = FALSE
+    )
+
+    # Copy BMC Experiment Config
+    safe_write_file(
+        data = bmc_configuration_definition_path,
+        path = bmc_experiment_config_path,
+        write_fn = file.copy,
+        verbose = DEBUG_CONFIG$verbose,
+        overwrite = TRUE
+    )
+}
diff --git a/failsafe_scripts/submit_alignment.sh b/failsafe_scripts/submit_alignment.sh
new file mode 100755
index 0000000..39aea1f
--- /dev/null
+++ b/failsafe_scripts/submit_alignment.sh
@@ -0,0 +1,29 @@
+#!/bin/bash
+# submit_alignment.sh
+# Usage: $./submit_alignment.sh $HOME/data/<experiment_directory>
+
+EXPERIMENT_DIR="$1"
+if [ -z "$EXPERIMENT_DIR" ]; then
+    echo "Error: Experiment directory not provided"
+    echo "Usage: $0 <experiment_directory>"
+    exit 1
+fi
+
+if [ ! -d "$EXPERIMENT_DIR" ]; then
+    echo "Error: Experiment directory does not exist."
+    echo "Usage: $0 <experiment_directory>"
+    exit 1
+fi
+
+
+# Count fastq files
+FASTQ_COUNT=$(find "${EXPERIMENT_DIR}/fastq" -maxdepth 1 -type f -name "*.fastq" | wc -l)
+echo "Found ${FASTQ_COUNT} fastq files"
+
+if [ $FASTQ_COUNT -eq 0 ]; then
+    echo "Error: No fastq files found in ${EXPERIMENT_DIR}/fastq"
+    exit 1
+fi
+
+# Submit job
+sbatch --array=1-${FASTQ_COUNT}%16 run_bowtie2_array_alignment.sh "$EXPERIMENT_DIR"
diff --git a/failsafe_scripts/submit_bam_qc.sh b/failsafe_scripts/submit_bam_qc.sh
new file mode 100644
index 0000000..2f9fb59
--- /dev/null
+++ b/failsafe_scripts/submit_bam_qc.sh
@@ -0,0 +1,52 @@
+#!/bin/bash
+# submit_bam_qc.sh
+# Purpose: Submit BAM quality control processing as SLURM array job
+# Version: 1.0.0
+# Compatibility: Bash 4.2+, SLURM
+
+# Strict error handling
+set -euo pipefail
+
+# Export scripts directory
+export PIPELINE_SCRIPTS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
+
+# Usage function
+usage() {
+    echo "Usage: $0 <experiment_directory> [bam_subdirectory]"
+    echo "Example: $0 /path/to/experiment alignment"
+    exit 1
+}
+
+# Validate input
+EXPERIMENT_DIR="${1:?Error: Experiment directory must be provided}"
+BAM_SUBDIR="${2:-alignment}"  # Default to 'alignment' if not specified
+
+# Validate experiment directory
+if [[ ! -d "${EXPERIMENT_DIR}" ]]; then
+    echo "Error: Experiment directory does not exist: ${EXPERIMENT_DIR}"
+    exit 1
+fi
+
+# Locate BAM files
+BAM_DIR="${EXPERIMENT_DIR}/${BAM_SUBDIR}"
+if [[ ! -d "${BAM_DIR}" ]]; then
+    echo "Error: BAM directory not found: ${BAM_DIR}"
+    exit 1
+fi
+
+# Count BAM files (sorted and indexed)
+BAM_COUNT=$(find "${BAM_DIR}" -maxdepth 1 \( -name "*.sorted.bam" -o -name "*.bam" \) | wc -l)
+
+if [[ ${BAM_COUNT} -eq 0 ]]; then
+    echo "Error: No BAM files found in ${BAM_DIR}"
+    exit 1
+fi
+
+echo "Found ${BAM_COUNT} BAM files for quality control"
+
+# Submit SLURM array job
+# Limit to 16 concurrent jobs, adjust as needed
+sbatch --array=1-${BAM_COUNT}%16 \
+    "${PIPELINE_SCRIPTS_DIR}/run_bam_qc_array.sh" \
+    "${EXPERIMENT_DIR}" \
+    "${BAM_SUBDIR}"
diff --git a/failsafe_scripts/submit_bamcoverage_normalizations.sh b/failsafe_scripts/submit_bamcoverage_normalizations.sh
new file mode 100755
index 0000000..0d35c28
--- /dev/null
+++ b/failsafe_scripts/submit_bamcoverage_normalizations.sh
@@ -0,0 +1,31 @@
+#!/bin/bash
+# submit_bamcoverage_normalizations.sh
+# Usage: $./submit_bamcoverage_normalizations.sh $HOME/data/<experiment_directory>
+EXPERIMENT_DIR="$1"
+if [ -z "$EXPERIMENT_DIR" ]; then
+    echo "Error: Experiment directory not provided"
+    echo "Usage: $0 <experiment_directory>"
+    exit 1
+fi
+
+if [ ! -d "$EXPERIMENT_DIR" ]; then
+    echo "Error: Experiment directory does not exist."
+    echo "Usage: $0 <experiment_directory>"
+    exit 1
+fi
+
+# Count BAM files
+
+declare -a NORM_METHODS=("RPKM" "CPM" "BPM" "RPGC")
+BAM_COUNT=$(find "${EXPERIMENT_DIR}/alignment" -maxdepth 1 -type f -name "*_sorted.bam" | wc -l)
+echo "Found ${BAM_COUNT} BAM files"
+TOTAL_JOBS=$((BAM_COUNT * ${#NORM_METHODS[@]}))
+echo "Found ${TOTAL_JOBS} jobs to run"
+
+if [ $BAM_COUNT -eq 0 ]; then
+    echo "Error: No BAM files found in ${EXPERIMENT_DIR}/alignment"
+    exit 1
+fi
+
+# Submit job
+sbatch --array=1-${TOTAL_JOBS}%16 run_bamcoverage_normalizations.sh "$EXPERIMENT_DIR"
diff --git a/failsafe_scripts/submit_fastqc.sh b/failsafe_scripts/submit_fastqc.sh
new file mode 100755
index 0000000..13f304f
--- /dev/null
+++ b/failsafe_scripts/submit_fastqc.sh
@@ -0,0 +1,52 @@
+#!/bin/bash
+# submit_fastqc.sh
+# Purpose: Submit FastQC processing as a SLURM array job
+# Version: 1.0.0
+# Compatibility: Bash 4.2+, SLURM
+
+# Strict error handling
+set -euo pipefail
+
+# Export scripts directory
+export PIPELINE_SCRIPTS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
+
+# Usage function
+usage() {
+    echo "Usage: $0 <experiment_directory> [fastq_subdirectory]"
+    echo "Example: $0 /path/to/experiment raw_data"
+    exit 1
+}
+
+# Validate input
+EXPERIMENT_DIR="${1:?Error: Experiment directory must be provided}"
+FASTQ_SUBDIR="${2:-fastq}"  # Default to 'fastq' if not specified
+
+# Validate experiment directory
+if [[ ! -d "${EXPERIMENT_DIR}" ]]; then
+    echo "Error: Experiment directory does not exist: ${EXPERIMENT_DIR}"
+    exit 1
+fi
+
+# Locate FASTQ files
+FASTQ_DIR="${EXPERIMENT_DIR}/${FASTQ_SUBDIR}"
+if [[ ! -d "${FASTQ_DIR}" ]]; then
+    echo "Error: FASTQ directory not found: ${FASTQ_DIR}"
+    exit 1
+fi
+
+# Count FASTQ files
+FASTQ_COUNT=$(find "${FASTQ_DIR}" -maxdepth 1 \( -name "*.fastq" -o -name "*.fq" \) | wc -l)
+
+if [[ ${FASTQ_COUNT} -eq 0 ]]; then
+    echo "Error: No FASTQ files found in ${FASTQ_DIR}"
+    exit 1
+fi
+
+echo "Found ${FASTQ_COUNT} FASTQ files for processing"
+
+# Submit SLURM array job
+# Limit to 16 concurrent jobs, adjust as needed
+sbatch --array=1-${FASTQ_COUNT}%16 \
+    "${PIPELINE_SCRIPTS_DIR}/run_fastqc_array.sh" \
+    "${EXPERIMENT_DIR}" \
+    "${FASTQ_SUBDIR}"
diff --git a/failsafe_scripts/subsample_fastq_and_bam_files.R b/failsafe_scripts/subsample_fastq_and_bam_files.R
new file mode 100644
index 0000000..c5f4091
--- /dev/null
+++ b/failsafe_scripts/subsample_fastq_and_bam_files.R
@@ -0,0 +1,109 @@
+################################################################################
+# SCRIPT: genomic_subsampling.R
+# PURPOSE: Create subsampled FASTQ files for testing pipelines
+# USAGE: Run in R/RStudio with renv. Processes first FASTQ and BAM found.
+# VALIDATION: Tested with 191209Bel fastq file.
+# UPDATES: 
+#   2024-12-03: Initial version
+#   2024-12-03: Modified for renv, explicit package calls
+#   2024-12-03: Changed to process only first found files
+################################################################################
+
+#===============================================================================
+# DEBUG CONFIGURATION
+#===============================================================================
+DEBUG <- TRUE                    # Enable/disable debug messages
+VERBOSE <- TRUE                  # Enable/disable verbose output
+DRY_RUN <- FALSE                # If TRUE, shows actions without execution
+SEED <- 42                      # Random seed for reproducibility
+
+#===============================================================================
+# PROCESSING CONFIGURATION
+#===============================================================================
+N_READS <- 1000                 # Number of reads to sample
+OUTPUT_PREFIX <- "subsampled"   # Prefix for output files
+FILE_PATTERNS <- list(
+    fastq = c("\\.fastq$", "\\.fq$", "\\.fastq\\.gz$", "\\.fq\\.gz$"),
+    bam = "\\.bam$"
+)
+
+#===============================================================================
+# ENVIRONMENT SETUP AND VALIDATION
+#===============================================================================
+if (DEBUG) message("Setting up environment...")
+
+# Set random seed
+set.seed(SEED)
+
+# Get home directory
+HOME_DIR <- path.expand("~")
+
+# Validate directory existence
+stopifnot("Home directory does not exist" = dir.exists(HOME_DIR))
+
+#===============================================================================
+# FILE DISCOVERY
+#===============================================================================
+if (DEBUG) message("Searching for input files...")
+
+# Find first FASTQ file
+fastq_pattern <- paste(FILE_PATTERNS$fastq, collapse = "|")
+FASTQ_FILE <- list.files(
+    path = HOME_DIR,
+    pattern = fastq_pattern,
+    full.names = TRUE
+)[1]
+
+# Find first BAM file
+BAM_FILE <- list.files(
+    path = HOME_DIR,
+    pattern = FILE_PATTERNS$bam,
+    full.names = TRUE
+)[1]
+
+# Validate file existence
+stopifnot("No FASTQ or BAM files found in home directory" = 
+          !is.na(FASTQ_FILE) || !is.na(BAM_FILE))
+
+if (VERBOSE) {
+    if (!is.na(FASTQ_FILE)) message("Found FASTQ file: ", basename(FASTQ_FILE))
+    if (!is.na(BAM_FILE)) message("Found BAM file: ", basename(BAM_FILE))
+}
+
+#===============================================================================
+# FASTQ PROCESSING
+#===============================================================================
+if (!is.na(FASTQ_FILE)) {
+    if (DEBUG) message("\nProcessing FASTQ file...")
+    
+    output_file <- file.path(HOME_DIR, 
+                            paste0(OUTPUT_PREFIX, "_", basename(FASTQ_FILE)))
+    
+    if (!DRY_RUN) {
+        tryCatch({
+            # Read FASTQ
+            fastq_data <- ShortRead::readFastq(FASTQ_FILE)
+            
+            # Sample reads
+            subset_idx <- sample(length(fastq_data), 
+                               min(N_READS, length(fastq_data)))
+            subset_fastq <- fastq_data[subset_idx]
+            
+            # Write subsampled FASTQ
+            ShortRead::writeFastq(subset_fastq, output_file)
+            
+            if (VERBOSE) message("Created: ", basename(output_file))
+        }, error = function(e) {
+            warning("Error processing ", basename(FASTQ_FILE), ": ", e$message)
+        })
+    }
+}
+
+#===============================================================================
+# BAM PROCESSING
+#===============================================================================
+
+#===============================================================================
+# COMPLETION
+#===============================================================================
+if (DEBUG) message("\nScript completed successfully")
diff --git a/failsafe_scripts/verification_script.R b/failsafe_scripts/verification_script.R
new file mode 100644
index 0000000..949db96
--- /dev/null
+++ b/failsafe_scripts/verification_script.R
@@ -0,0 +1,141 @@
+# Test validation
+test_validation <- validate_comparison_inputs(
+    mtcars,
+    list(high_mpg = quote(mpg > 20))
+)
+print(test_validation$valid) # Should be TRUE
+
+# Test comparison execution
+test_exec <- execute_comparison(mtcars, quote(mpg > 20))
+print(sum(test_exec$matches)) # Should show count of high MPG cars
+
+# Test summary creation
+test_summary <- create_sample_summary(
+    mtcars[1, ],
+    c("mpg", "cyl")
+)
+print(test_summary$summary)
+
+# Test full analysis
+test_analysis <- analyze_comparisons(
+    mtcars,
+    list(
+        high_mpg = quote(mpg > 20),
+        high_cyl = quote(cyl > 6)
+    )
+)
+print(sapply(test_analysis$results, function(x) x$match_count))
+
+
+# Test data
+test_df <- data.frame(
+    antibody = c("AB1", "AB2"),
+    type = c("T1", "T2"),
+    treatment = c("TR1", "TR1"),
+    stringsAsFactors = FALSE
+)
+
+# Test validation
+test_valid <- validate_category_names(test_df, c("type", "treatment"))
+print(test_valid$success) # Should be TRUE
+
+# Test category extraction
+test_extract <- extract_category_values(test_df, c("antibody", "type"))
+print(test_extract$data) # Should show unique values
+
+# Test full label generation
+test_labels <- generate_sample_labels(
+    test_df,
+    c("type", "treatment"),
+    list(verbose = TRUE)
+)
+print(test_labels$data) # Should show formatted labels
+
+# Test directory validation
+test_dir_valid <- directory_structure_validate("test/path")
+
+# Test file scanning
+test_files <- experiment_files_scan("test/path", 
+                                  "consolidated_.*_sequence\\.fastq$")
+
+# Test identifier extraction
+test_ids <- experiment_identifiers_extract(
+    c("consolidated_12345_sequence.fastq"),
+    "consolidated_([0-9]{5,6})_sequence\\.fastq"
+)
+
+# Test metadata processing
+test_metadata <- experiment_metadata_process(
+    "test/path",
+    list(
+        categories = list(treatment = c("control", "treated")),
+        column_order = c("sample_id", "treatment")
+    ),
+    list(
+        output_file = FALSE,
+        output_path = NULL
+    )
+)
+
+# Test mapping initialization
+test_mappings <- chromosome_mappings_initialize()
+
+# Test style detection
+test_detect <- chromosome_style_detect(
+    c("chr1", "chr2"),
+    list(
+        UCSC = "^chr[0-9]+$",
+        Roman = "^chr[IVX]+$",
+        Numeric = "^[0-9]+$"
+    )
+)
+
+# Test conversion
+test_convert <- chromosome_names_convert(
+    c("chr1", "chr2"),
+    "Roman",
+    list(validate = TRUE)
+)
+
+# Test file validation
+test_file <- bigwig_file_validate("test.bw", "exp001")
+
+# Test control filtering
+test_controls <- input_controls_filter(
+    data.frame(antibody = c("Input", "H3K4me3")),
+    list(antibody = "Input")
+)
+
+# Test track limits
+test_limits <- track_limits_calculate(
+    list(track1, track2),
+    list(padding = 0.1)
+)
+
+# Test sample filtering
+test_filter <- comparison_samples_filter(
+    data.frame(treatment = c("control", "treated")),
+    quote(treatment == "control")
+)
+
+# Test track creation
+test_track <- sample_track_create(
+    data.frame(experiment_number = "001", row.names = 1),
+    "data/bigwig",
+    list(label = "Sample 1", color = "#fd0036")
+)
+
+# Test full analysis
+test_analysis <- comparison_analysis_process(
+    sample_table,
+    list(
+        name = "Test Comparison",
+        expression = quote(treatment == "control"),
+        labels = c("Control 1")
+    ),
+    list(
+        bigwig_directory = "data/bigwig",
+        chromosome = "I",
+        output_path = "output/test.svg"
+    )
+)
diff --git a/find_and_move.sh b/find_and_move.sh
deleted file mode 100755
index bac60f8..0000000
--- a/find_and_move.sh
+++ /dev/null
@@ -1,47 +0,0 @@
-#!/usr/bin/env bash
-
-set -o errexit
-set -o nounset
-set -o pipefail
-
-TARGET_DIR="./files_to_remove/"  # User-specified target directory for moving files
-SEARCH_DIR="."  # Define the directory to search in
-MAX_DEPTH=4  # Define the maximum depth for the search
-
-# Update file extensions for NGS data analysis
-FILE_EXTENSIONS="*.fastq *.bam *.sam *.vcf *.fq"
-SEARCH_PATHS="*/code/* */script*/*"
-
-# Function to move files
-move_files() {
-    local target_dir=$1
-    local search_dir=$2
-    local max_depth=$3
-    local -a conditions=()
-    
-    # Construct the find command conditions for NGS file extensions
-    for ext in $FILE_EXTENSIONS; do
-        echo "$ext"
-        conditions+=("-o -name" "$ext")
-    done
-    
-    # Remove the first '-o' from the conditions array
-    conditions=("${conditions[@]:1}")
-     
-    for ext in $conditions; do
-         echo "$ext"
-    done
-
-
-    # Execute the find command with constructed conditions
-    find "$search_dir" -maxdepth "$max_depth" -type f \( "${conditions[@]}" \) \( -path "*/code/*" -o -path "*/script*/*" \) \
-     | xargs -I {} echo {} | nl
-     #   | head -n 25 \
-     #  | xargs -I {} mv {} "$target_dir"
-}
-
-# Call the move_files function with the specified parameters
-move_files "$TARGET_DIR" "$SEARCH_DIR" "$MAX_DEPTH"
-
-find "." -type f \( -path "*/code/*" -o -path "*/script*/*" \) \( -name "*.fastq" -o -name "*.bam" -o -name "*.sam" -o -name "*.vcf" -o -name "*.fq" \)
-find . -type d \( -path '*/lib/*' -o -path '*/renv/*' -o -path '*/git/*' \) -prune -o -type f -exec du -a {} + | sort -nr | head -n 1000 | awk -F/ '{print $NF}' | rev | cut -d. -f1 | rev | sort | uniq
diff --git a/last_functional_commit_hash.txt b/last_functional_commit_hash.txt
new file mode 100644
index 0000000..047a2d5
--- /dev/null
+++ b/last_functional_commit_hash.txt
@@ -0,0 +1 @@
+c285814f9762afc8874d469fef9abd61d2da8f08
diff --git a/linux_cluster/000_slurmWrapper.sh b/linux_cluster/000_slurmWrapper.sh
deleted file mode 100755
index 1da2a5e..0000000
--- a/linux_cluster/000_slurmWrapper.sh
+++ /dev/null
@@ -1,50 +0,0 @@
-#!/bin/bash
-#USAGE: 
-#~/lab_utils/next_generation_sequencing/linux_cluster/000_sh_node_slurmWrapper.sh 1-N%16 000_scriptToRun.sh 240304Bel
-echo -e "Executing from $(pwd) \n"
-
-if [ $# -ne 3 ]; then
-    echo -e "Usage: \n $0 <array_number> <script_name> <DIRECTORY_TO_PROCESS> \n" 
-    echo -e 'Array number is 1, an integer (for a specific task) or a range (1-N%16) depending on the number of array tasks to create.(--array= option for SBATCH) \n' 
-    echo -e 'script_name is basename of lab_utils script ( eg 003_sh_slurm_alignFastq.sh ) \n' 
-    echo -e 'Directory is name of directory without /. Must be in ~/data ( eg 240304Bel ) \n' 
-    exit 1
-fi
-
-array_range="$1"
-SCRIPT_TO_RUN=$(find $HOME/lab_utils -type f -name "$2")
-DIRECTORY_TO_PROCESS="$(find -H $HOME/data -maxdepth 1 -type d -name "$3")"
-
-echo -e "Will process ${DIRECTORY_TO_PROCESS}\n"
-echo -e "Will run ${SCRIPT_TO_RUN} \n"
-
-# Check if script and DIRECTORY_TO_PROCESS exist
-if [ ! -f "$SCRIPT_TO_RUN" -o ! -d "$DIRECTORY_TO_PROCESS" ]; then
-  echo -e "Error: Script or DIRECTORY_TO_PROCESS not found!\n"
-  echo -e "Echo statement above without a noun shows which one is missing.\n"
-  exit 1
-fi
-
-
-timeid=$(date +%Y%m%d%M%S)
-#awk '{print substr($0, index($0, last"/")) "/"}' <<< "$TEST_DIR"
-#echo $TEST_DIR | rev | cut -d/ -f1 | rev | xargs -I {} echo {}/
-JOB_ID=$(sbatch --parsable --array="$array_range" "$SCRIPT_TO_RUN" "${DIRECTORY_TO_PROCESS##*/}/" "${timeid}")
-echo "Time is ${timeid}"
-echo "JOB is ${JOB_ID}"
-echo "View logs using vim ${DIRECTORY_TO_PROCESS}/logs/*_${JOB_ID}.out."
-echo "View standard error using vim ${DIRECTORY_TO_PROCESS}/logs/*_${JOB_ID}.err."
-echo "To find log files created on the same day: use find ${DIRECTORY_TO_PROCESS}/logs -type f -daystart -ctime 0 -name "*.out" -exec vim {} +"
-echo "To find log files created by timeid: use find ${DIRECTORY_TO_PROCESS}/logs -type f -name "${timeid}*.out" -exec vim {} +"
-echo "To see size of files by timeid: use find ${DIRECTORY_TO_PROCESS}/ -type f -name "${timeid}*" -exec ls lh {} +. Ensure you add proper extension to name argument."
-
-#EXTRACT_TO_SCRIPT: cleanupscript
-#sbatch --dependency=afterany:$SLURM_JOB_ID cleanup_script.sh
-#slurm_files=$(find . -maxdepth 1 -type f -name "slurm*.out")
-#SBATCH --job-name=main_job
-#SBATCH --output=/dev/null
-#TARGET_DIR="${1:-.}"
-# Find and remove SLURM output files in the specified directory
-#find "$TARGET_DIR" -name 'slurm-*.out' -exec rm {} +
-
-
diff --git a/linux_cluster/001_cleanUpSlurmOut.sh b/linux_cluster/001_cleanUpSlurmOut.sh
deleted file mode 100644
index 8843ce7..0000000
--- a/linux_cluster/001_cleanUpSlurmOut.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/bin/bash
-
-find . -maxdepth 1 -type f -name "slurm*out" -exec echo {} \;
-
-# Define the directory to search and the pattern to match
-SEARCH_DIR="/path/to/search"
-PATTERN="*.tmp"  # Example pattern, adjust as needed
-
-# Perform the dry run: list files that would be deleted
-echo "Performing dry run..."
-find "$SEARCH_DIR" -type f -name "$PATTERN" -exec echo "Would delete: {}" \;
-
-# Ask the user for confirmation to proceed with actual deletion
-read -p "Proceed with deleting these files? (y/n): " confirm && [[ $confirm == [yY] ]] || exit 1
-
-# If confirmed, proceed with deletion
-echo "Deleting files..."
-find "$SEARCH_DIR" -type f -name "$PATTERN" -exec rm {} \;
-
-echo "Files have been deleted.
diff --git a/linux_cluster/002_cleanupFastqDirectories.sh b/linux_cluster/002_cleanupFastqDirectories.sh
deleted file mode 100755
index c2829f4..0000000
--- a/linux_cluster/002_cleanupFastqDirectories.sh
+++ /dev/null
@@ -1,9 +0,0 @@
-#Ran the commands manually as well instead of via script.
-#Dry runs
-find "$HOME/data/$1" -type d -name "*D24*" -exec echo "Deleting: {}" \; 
-find "$HOME/data/$1" -type d -name "*D24*" | wc -l 
-
-
-find "$HOME/data/$1" -type d -name "*D24*" -exec rm -rf {} + 
-find "$HOME/data/$1" -type f -name "*unmapped*" -exec rm -rf + 
-
diff --git a/linux_cluster/002_exportRLibsUserPath.sh b/linux_cluster/002_exportRLibsUserPath.sh
deleted file mode 100644
index 25eaef4..0000000
--- a/linux_cluster/002_exportRLibsUserPath.sh
+++ /dev/null
@@ -1,3 +0,0 @@
-#!/bin/bash
-
-export R_LIBS_USER=$HOME/R/x86_64-pc-linux-gnu-library/4.2
diff --git a/linux_cluster/003_consolidateLogFiles.sh b/linux_cluster/003_consolidateLogFiles.sh
deleted file mode 100755
index bdf6bad..0000000
--- a/linux_cluster/003_consolidateLogFiles.sh
+++ /dev/null
@@ -1,55 +0,0 @@
-#!/bin/bash
-if [ $# -ne 1 ]; then 
-	echo -e "No directory provided. Provide the basename of a directory with no / in the ~/data directory"
-fi
-
-DIRECTORY_TO_PROCESS="$(find -H $HOME/data -maxdepth 1 -type d -name "$1")"
-LOG_DIR="${DIRECTORY_TO_PROCESS}/logs/"
-
-if [ ! -d "$DIRECTORY_TO_PROCESS" ]; then
-  echo "Error: DIRECTORY_TO_PROCESS not found!"
-  exit 1
-fi
-
-echo "Will process $DIRECTORY_TO_PROCESS"
-
-EXTENSIONS=(".out" ".err")
-
-for EXT in "${EXTENSIONS[@]}"; do
-	
-	# INITIALIZE_ARRAY
-	mapfile -t LOG_PATHS < <(find "${LOG_DIR}" -type f -name "*${EXT}" ! -name "*consolidated*" )
-	
-	#Could substitute cut with awk -F '-' '{print $2}'
-	if [ ${#LOG_PATHS[@]} -eq 0 ]; then
-		echo "No Log files to consolidate. Exiting\n"
-		continue
-	fi
-
-	#Alternative to awk: echo "2024-04-15-19-25_filtering_8980861_8980862_1.out" | cut -d'_' -f1-3 --output-delimiter='_'
-	UNIQUE_JOBIDS=($(printf '%s\n' "${LOG_PATHS[@]}" | awk -F'/' '{print $NF}' | awk -F'_' '{print $3}' | uniq ))
-	echo "Total number of unique JOB_IDS is: ${#UNIQUE_JOBIDS[@]}"
-	
-	for JOB_ID in ${UNIQUE_JOBIDS[@]}; do 
-		echo "Processing ${JOB_ID}"
-		# Find all .out files for the current job ID
-		JOB_LOG_PATHS=($(printf '%s\n' "${LOG_PATHS[@]}" | grep "${JOB_ID}" ))
-		echo "Total number of LOG Files with ${JOB_ID} is ${#JOB_LOG_PATHS[@]}"
-		echo "First LOG PATH is ${JOB_LOG_PATHS[0]}"
-		# Extract the oldest date ID for the current job ID
-		OLDEST_DATE_ID=$(printf '%s\n' "${JOB_LOG_PATHS[@]}" | awk -F'/' '{print $NF}' | awk -F'_' '{print $1}' | sort -r | head -n 1)
-	
-		# Construct the output file name
-		OUTPUT_FILE="${LOG_DIR}${OLDEST_DATE_ID}_${JOB_ID}_consolidated$EXT"
-		echo -e "Output file is ${OUTPUT_FILE}\n"
-		
-		# Consolidate the log files for the current job ID
-		for LOG_FILE in "${JOB_LOG_PATHS[@]}"; do
-#			echo "Outputting ${LOG_FILE}."
-	  		echo -e "\n\n----- $(basename "${LOG_FILE}") -----\n" >> "${OUTPUT_FILE}"
-	  		cat "${LOG_FILE}" >> "${OUTPUT_FILE}"
-		done
-		rm "${JOB_LOG_PATHS[@]}"
-	done
-done
-echo "Number of files in ${LOG_DIR} is: $(ls ${LOG_DIR} | wc -l)"
diff --git a/linux_cluster/1-package-installation.r b/linux_cluster/1-package-installation.r
deleted file mode 100755
index e8dc79c..0000000
--- a/linux_cluster/1-package-installation.r
+++ /dev/null
@@ -1,81 +0,0 @@
-#https://rdrr.io/category/biocview/GenomeAnnotation/
-#https://uclouvain-cbio.github.io/WSBIM1322/index.html
-#https://compgenomr.github.io/book/
-#https://rockefelleruniversity.github.io/RU_ChIPseq/index.html
-#https://learn.gencore.bio.nyu.edu/
-
-#Loading packages in a clean way. 
-# Install packages that will be used most of the time.
-# constant_packages <- c("renv","pacman")
-# install.packages(constant_packages)
-# 
-# 
-# pacman::p_load(ggplot2, tidyverse,BiocManager,stringr,R.utils)
-renv::install("styler")
-bioinformatics_packages <- c("BiocGenerics","MatrixGenerics",'qvalue','plot3D','ggplot2','pheatmap','cowplot',
-  'cluster', 'NbClust', 'fastICA', 'NMF','matrixStats',
-  'Rtsne', 'mosaic', 'knitr', 'genomation',
-  'ggbio', 'Gviz', 'DESeq2', 'RUVSeq',
-  'gProfileR', 'ggfortify', 'corrplot',
-  'gage', 'EDASeq', 'formatR', 'BiocFileCache',
-  'svglite', 'Rqc', 'ShortRead', 'QuasR',
-  'methylKit','FactoMineR', 'iClusterPlus',
-  'enrichR','caret','xgboost','glmnet',
-  'DALEX','kernlab','pROC','nnet','RANN',
-  'ranger','GenomeInfoDb', 'GenomicRanges',
-  'GenomicAlignments', 'ComplexHeatmap', 'circlize', 
-  'rtracklayer', 'tidyr', 'dplyr',
-  'AnnotationHub', 'GenomicFeatures', 'normr',
-  'MotifDb', 'TFBSTools', 'rGADEM', 'JASPAR2018', 
-  'BSgenome', 'htmltab', 'usethis',
-  'Rsubread', 'Rsamtools', 'Rbowtie', 'Rbowtie2' , "ChIPpeakAnno", 
-  "seqinr","GenomeInfoDbData","BSgenome.Scerevisiae.UCSC.sacCer3")
-
-BiocManager::install(bioinformatics_packages)
-#Cool way to install Stolen/borrowed from https://www.r-bloggers.com/2020/01/an-efficient-way-to-install-and-load-r-packages-2/
-#  
-# packages <- c("ggplot2", "readxl", "dplyr", "tidyr", "ggfortify", "DT", "reshape2", "knitr", "lubridate", "pwr", "psy", "car", "doBy", "imputeMissings", "RcmdrMisc", "questionr", "vcd", "multcomp", "KappaGUI", "rcompanion", "FactoMineR", "factoextra", "corrplot", "ltm", "goeveg", "corrplot", "FSA", "MASS", "scales", "nlme", "psych", "ordinal", "lmtest", "ggpubr", "dslabs", "stringr", "assist", "ggstatsplot", "forcats", "styler", "remedy", "snakecaser", "addinslist", "esquisse", "here", "summarytools", "magrittr", "tidyverse", "funModeling", "pander", "cluster", "abind")
-# 
-# # Install packages not yet installed
-# installed_packages <- packages %in% rownames(installed.packages())
-# if (any(installed_packages == FALSE)) {
-#   install.packages(packages[!installed_packages])
-# }
-
-# 
-# # Packages loading
-# invisible(lapply(packages, library, character.only = TRUE))
-
-
-#May need to change some variables.
-#Run this, after creating virtual environment and directory.
-# #Install packages for documentations and parallel processing 
-# renv::install(c("tidyverse", "R.utils", "doParallel", "snow","fs","rmarkdown", "gt","formattable","ggplot2", "webshot2",
-#               "rmarkdown","xaringan","officer",
-#               "quarto","BiocManager"))
-# #Install some files for development
-# renv::install("devtools","rcrossref","taskscheduleR","bio3d")
-# devtools::install_github("crsh/citr")
-# remotes::install_github("paleolimbot/rbbt")
-
-#Non Bioconductor package 
-#install.packages("formattable")
-
-#Enable SSL on windows: https://answers.microsoft.com/en-us/windows/forum/all/ssl-error-preventing-connection-in-windows-10/192436e5-e37b-4b4c-a4e0-c4ec744b0f5c
-#Then install basilisk.utils
-# BiocManager::install(c("basilisk.utils", "MACSr"))
-# BiocManager::install(c("nanopoRe"))
-# update.packages()
-# lapply(paste0("package:", names(sessionInfo()$otherPkgs)),   # Unload add-on packages
-#        detach,
-#        character.only = TRUE, unload = TRUE)
-
-
-deattachAll <- function(){
-  lapply(paste0("package:", names(sessionInfo()$otherPkgs)),   # Unload add-on packages
-                detach,
-                character.only = TRUE, unload = TRUE)
-}
-
-
-
diff --git a/linux_cluster/rename_files.awk b/linux_cluster/rename_files.awk
deleted file mode 100644
index 2613a99..0000000
--- a/linux_cluster/rename_files.awk
+++ /dev/null
@@ -1,26 +0,0 @@
-BEGIN {
-# Field separator designation.
-    FS = "/"
-}
-
-{
-# Filter the files that shouldnt be renamed.
-# The other solution is to use a combination of find and grep commands to filter files and folders.
-    if (index($NF, "_") == 0) next
-    if ($NF ~ "rsync") next
-    if ($0 ~ "test") {
-        split($NF, parts, "_")
-        new_name = parts[1] "_" "test" "_" parts[length(parts)]
-    }
-    else {
-        split($NF, parts, "_")
-        new_name = parts[1] "_" parts[length(parts)]
-    }
-    if (new_name != $NF) {
-#    printf "Full name: %s\n", $0
-#    printf "Old name: %s\n", $NF
-# Use printf to create the command to pipe into sh.
-# Have to escape all the quotes. Use substr to grab the path of the file and then combine with new name.
-    printf "mv \"%s\" \"%s%s\"\n", $0, substr($0, 1, length($0)-length($NF)), new_name
-    }
-}
diff --git a/linux_cluster/rsync_with_wsl_and_R.r b/linux_cluster/rsync_with_wsl_and_R.r
deleted file mode 100755
index 5eb369a..0000000
--- a/linux_cluster/rsync_with_wsl_and_R.r
+++ /dev/null
@@ -1,72 +0,0 @@
-# Running rsync through R ----
-#The following requires WSL to be installed on Windows computer 
-#
-# dir_to_rsync_from  <- "../ngs-pipeline/scripts/R-files/*"
-# rsync_dest <- "./scripts/"
-# rsync_command_dry <- 'wsl rsync --stats -nv'
-# rsync_command <- 'wsl rsync --stats -v'
-#
-# #Check the string
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-#
-# #Check the command output
-# system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Transfer if it looks good
-# system(paste(rsync_command, dir_to_rsync_from, rsync_dest))
-
-# #Transfer files to luria server 
-# rsync_command_dry <- 'wsl rsync --stats -nvr'
-# rsync_command <- 'wsl rsync --stats -vr'
-# dir_to_rsync_from  <- stringr::str_replace(paste0(getwd(), "/scripts/"), pattern = "C:", replacement = "/mnt/c")
-# rsync_dest <- "luised94@luria.mit.edu:/home/luised94/data/R-scripts/"
-# #For this you can run from command line. It requires password. Just paste output from R console to the terminel
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-# # system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Run rsync on terminal ----
-#Sync to dropbox
-wsl rsync --stats -nrv /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-#Rsync my Zotero folder
-wsl rsync -nrv /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-wsl rsync -nrv --update /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-
-#Inverted the order to sync from the cluster to the local machine
-wsl rsync --stats -nvr --update luised94@luria.mit.edu:/home/luised94/data/R-scripts/* /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/cluster-modified/
-rsync --stats --dry-run -vr luised94@luria.mt.edu:/home/luised94/data/240304Bel/plots/* '/mnt/c/Users/liusm/Dropbox (MIT)/240304Bel/plots'
-#Transfering and updating to luria
-wsl rsync --stats -nv --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/* luised94@luria.mit.edu:/home/luised94/data/rscripts/
-
-#Count the lines that have fastq in it. 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files | grep -e ".fastq"
-
-find -type d -name "*fastq*" | grep "/f"
-
-
-#Run for files after rsync ----
-find -type f -exec dos2unix -k -o {} \;
-find -type f -exec chmod +x {} \;
-#----
-#Downloading Eaton data 
-wget --output-document=./fastq-files/WT-G2-ORC-rep1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz
-wget --output-document=./fastq-files/WT-G2-ORC-rep2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz 
-
-#Download BMC data 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files
-
-#Move the folder content up one directory
-mv 221024Bel/* . 
-rmdir 221024Bel
-
-
-sacct -A luised94 --format=JobName,Account,AllocNodes,AllocCPUs,AllocTres,AveDiskRead,Elapsed,JobID,ReqMem | awk '$1~/sbatch/'
-
-test_var="$(echo date +%g_%m_%d_%H_%M_%S)"
-${test_var}
-
-echo Execution Time $(${test_var}) > test.out
-
-sed -i '2 i '"$(echo Execution Time $($test_var))"'' test.out
-
-
-myInvocation="$(printf %q "$BASH_SOURCE")$((($#)) && printf ' %q' "$@")"
diff --git a/linux_cluster/rsync_with_wsl_and_R.sh b/linux_cluster/rsync_with_wsl_and_R.sh
deleted file mode 100755
index faef3dd..0000000
--- a/linux_cluster/rsync_with_wsl_and_R.sh
+++ /dev/null
@@ -1,83 +0,0 @@
-# Running rsync through R ----
-#The following requires WSL to be installed on Windows computer 
-#
-# dir_to_rsync_from  <- "../ngs-pipeline/scripts/R-files/*"
-# rsync_dest <- "./scripts/"
-# rsync_command_dry <- 'wsl rsync --stats -nv'
-# rsync_command <- 'wsl rsync --stats -v'
-#
-# #Check the string
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-#
-# #Check the command output
-# system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Transfer if it looks good
-# system(paste(rsync_command, dir_to_rsync_from, rsync_dest))
-
-# #Transfer files to luria server 
-# rsync_command_dry <- 'wsl rsync --stats -nvr'
-# rsync_command <- 'wsl rsync --stats -vr'
-# dir_to_rsync_from  <- stringr::str_replace(paste0(getwd(), "/scripts/"), pattern = "C:", replacement = "/mnt/c")
-# rsync_dest <- "luised94@luria.mit.edu:/home/luised94/data/R-scripts/"
-# #For this you can run from command line. It requires password. Just paste output from R console to the terminel
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-# # system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-#Download files using svn. Downloads the folder to current working directory
-wsl svn export https://github.com/CEGRcode/2021-Rossi_Nature.git/trunk/02_References_and_Features_Files
-
-#Run rsync on terminal ----
-#Sync to dropbox
-wsl rsync --stats -nrv /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-wsl rsync --stats -nrv --update /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-
-
-
-
-
-
-
-
-
-
-
-
-
-#Count the lines that have fastq in it. 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files | grep -e ".fastq"
-
-find -type d -name "*fastq*" | grep "/f"
-
-
-#Run for files after rsync ----
-find -type f -exec dos2unix -k -o {} \;
-find -type f -exec chmod +x {} \;
-#----
-#Downloading Eaton data 
-wget --output-document=./fastq-files/WT-G2-ORC-rep1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz
-wget --output-document=./fastq-files/WT-G2-ORC-rep2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz 
-
-#Download BMC data 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files
-
-#Move the folder content up one directory
-mv 221024Bel/* . 
-rmdir 221024Bel
-
-
-sacct -A luised94 --format=JobName,Account,AllocNodes,AllocCPUs,AllocTres,AveDiskRead,Elapsed,JobID,ReqMem | awk '$1~/sbatch/'
-
-test_var="$(echo date +%g_%m_%d_%H_%M_%S)"
-${test_var}
-
-echo Execution Time $(${test_var}) > test.out
-
-sed -i '2 i '"$(echo Execution Time $($test_var))"'' test.out
-
-ls -lt | grep "slurm" | head -2
-
-ls -lt | grep "slurm" | head -1 | awk '{print $(NF)}'
-
-find . -maxdepth 1 -name 'slurm*' -delete
-
-myInvocation="$(printf %q "$BASH_SOURCE")$((($#)) && printf ' %q' "$@")"
diff --git a/linux_cluster/tests/000_test_slurmWrapper.sh b/linux_cluster/tests/000_test_slurmWrapper.sh
deleted file mode 100755
index 5ef1093..0000000
--- a/linux_cluster/tests/000_test_slurmWrapper.sh
+++ /dev/null
@@ -1,37 +0,0 @@
-#!/bin/bash
-#USAGE: 
-#~/lab_utils/next_generation_sequencing/linux_cluster/000_sh_node_slurmWrapper.sh 1-N%16 000_scriptToRun.sh 240304Bel
-if [ $# -ne 3 ]; then
-	echo "Usage: $0 <array_number> <<script_name> <DIRECTORY_TO_PROCESS_to_process>" 
-	echo 'Array number is an integer or range (1-N%16) depending on the number of array tasks to create (--array= option for SBATCH)' 
-	echo 'script_name is basename of lab_utils script ( eg 000_sh_node_test_slurmWrapper.sh )' 
-	echo 'Directory is DIRECTORY_TO_PROCESS to process with / ( eg 240304Bel/ )' 
-	exit 1
-fi
-
-array_range="$1"
-SCRIPT_TO_RUN=$(find $HOME/lab_utils -type f -name "$2")
-DIRECTORY_TO_PROCESS="$(find -H $HOME/data -maxdepth 1 -type d -name "$3")"
-
-echo "Will process $DIRECTORY_TO_PROCESS"
-echo "Will run $SCRIPT_TO_RUN"
-
-# Check if script and DIRECTORY_TO_PROCESS exist
-if [ ! -f "$SCRIPT_TO_RUN" -o ! -d "$DIRECTORY_TO_PROCESS" ]; then
-  echo "Error: Script or DIRECTORY_TO_PROCESS not found!"
-  exit 1
-fi
-
-
-#awk '{print substr($0, index($0, last"/")) "/"}' <<< "$TEST_DIR"
-#echo $TEST_DIR | rev | cut -d/ -f1 | rev | xargs -I {} echo {}/
-JOB_ID=$(sbatch --parsable --array="$array_range" "$SCRIPT_TO_RUN" "${DIRECTORY_TO_PROCESS##*/}/" )
-
-echo "JOB is ${JOB_ID}"
-echo "View logs using vim ${DIRECTORY_TO_PROCESS}/logs/*_${JOB_ID}_*_1.out."
-
-
-#slurm_files=$(find . -maxdepth 1 -type f -name "slurm*.out")
-
-
-
diff --git a/linux_cluster/tests/001_test_wrapperTest.sh b/linux_cluster/tests/001_test_wrapperTest.sh
deleted file mode 100755
index 569e7b1..0000000
--- a/linux_cluster/tests/001_test_wrapperTest.sh
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G # amount of RAM per node#
-
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/${timeid}_aligning_${SLURM_ARRAY_JOB_ID}_${SLURM_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out"
-ERR_FILE="${LOG_DIR}/${timeid}_aligning_${SLURM_ARRAY_JOB_ID}_${SLURM_JOB_ID}_${SLURM_ARRAY_TASK_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >"$OUT_FILE" 2>"$ERR_FILE"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-#INITIALIZE_ARRAY
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "processed_*.fastq" )
-mapfile -t GENOME_PATHS < <(find "${REFGENOME_DIR}" -type f -name "*_refgenome.fna")
-
-#INPUT_OUTPUT
-#fastq_path=${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}
-#output_path=$(echo "$fastq_path" | cut -d/ -f7 | xargs -I {} echo "${DIR_TO_PROCESS}processed-fastq/processed_{}")
-
-#LOG
-# Perform indexing by getting quotient and remainder
-GENOME_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1) / ${#FASTQ_PATHS[@]}))
-FASTQ_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1)  % ${#FASTQ_PATHS[@]}))
-echo "Processing ${GENOME_INDEX} and ${FASTQ_INDEX}"
-
-#Get names for output
-FASTQ_ID=$(echo "${FASTQ_PATHS[$FASTQ_INDEX]}" | cut -d_ -f2 )
-GENOME_NAME=$( echo "${GENOME_PATHS[$GENOME_INDEX]}" | cut -d_ -f1 | rev | cut -d/ -f1 | rev )
-echo "$FASTQ_ID | $GENOME_NAME"
-
-echo "${FASTQ_PATHS[$FASTQ_INDEX]}"
-echo "${GENOME_PATHS[$GENOME_INDEX]}"
-
-
diff --git a/next_generation_sequencing/000_generalSetup/000_setupExperimentDir.R b/next_generation_sequencing/000_generalSetup/000_setupExperimentDir.R
deleted file mode 100644
index 852aa1a..0000000
--- a/next_generation_sequencing/000_generalSetup/000_setupExperimentDir.R
+++ /dev/null
@@ -1,160 +0,0 @@
-#Description:
-#USAGE: Use with Rscript command. 
-#Add section to create date and directory for experiment, same day as request service initialization. Probably interactive.
-# To see definition of this variable, run echo $dropbox_path or see bashrc in my_config repository
-
-cat("Starting experiment setup\n")
-OUTPUT_TO_FILE <- FALSE
-main <- function() {
-    validated_args <- validate_input()
-    username <- validated_args$username
-    experiment_name <- validated_args$experiment_name
-
-    dropbox_dir <- sprintf("/mnt/c/Users/%s/Dropbox (MIT)/", username)
-    experiment_dir <- paste(dropbox_dir, experiment_name, sep = "")
-    cat("Experiment directory to be created: ", experiment_dir, "\n")
-
-    sample_grid_config_filepath <- file.path(get_script_dir(), "sampleGridConfig.R")
-    source(sample_grid_config_filepath) # Initializes sample_config_output and current_experiment
-
-    if (experiment_name != current_experiment) {
-        cat("Experiment provided and experiment in sampleGridConfig.R are not the same.\n")
-        cat(sprintf("Experiment in sampleGridConfig.R: %s", current_experiment), "\n")
-        cat(sprintf("Experiment provided as argument: %s", experiment_name), "\n")
-        stop("Update the sampleGridConfig.R categories, current_experiment and filter_samples function.")
-    } else {
-        cat("Verified that experiment argument and in config file are the same.\n")
-    }
-
-    subdirectories <- c("peak", "fastq", "alignment", "qualityControl", "bigwig", "plots", "logs", "documentation")
-    create_experiment_dir(experiment_dir, subdirectories)
-
-    cat("Config file run and to be copied: ", sample_grid_config_filepath, "\n")
-    config_file_output_path <- file.path(experiment_dir, "documentation", paste(experiment_name, "_", "sampleGridConfig.R", sep = ""))
-    cat("Outputting file to:", config_file_output_path,"\n" )
-    if (OUTPUT_TO_FILE){
-        file.copy(from = sample_grid_config_filepath, to = config_file_output_path) 
-    } else {
-        cat(sprintf("Skip writing %s to file. Modify OUTPUT_TO_FILE value.\n", sample_grid_config_filepath))
-    }
-    #print("Files currently loaded")
-
-
-    output_tables_in_list(experiment_directory = experiment_dir, sample_config_output, OUTPUT_TABLE = OUTPUT_TO_FILE )
-    #invisible(lapply(names(sample_config_output), function(output_table_name){
-    #    print(head(sample_config_output[[output_table_name]]))
-    #    output_table <- sample_config_output[[output_table_name]]
-    #    output_file <- file.path(experiment_dir, "documentation", paste(experiment_name, "_", output_table_name, ".tsv", sep = ""))
-    #    print(output_file)
-    #    write.table(output_table, file = output_file, sep = "\t", row.names = FALSE)
-    #}))
-
-    # Rsync to the server
-    # Suggest alternative or run a particular command. 
-    cat("Ensure you are connected to the luria mit network.\n")
-    cat("Run the following command to rsync the created directory.\n")
-    server_path <- "luised94@luria.mit.edu:~/data/"
-    cat("scp -r from_dir user@server:to_dir\n")
-    cat(sprintf("scp -r \"%s\" \"%s\"", experiment_dir, server_path), "\n")
-    cat("After running the scp command, login to cluster and \n download the data from BMC (see 001_downloadDataFromBMC.sh )\n")
-    print("Script complete.")
-
-}
-#rm(list = ls())
-get_script_name <- function() {
-    cat("Grabbing script name.", "\n")
-    cmdArgs <- commandArgs(trailingOnly = FALSE)
-    needle <- "--file="
-    match <- grep(needle, cmdArgs)
-    if (length(match) > 0) {
-        script_name <- sub(needle, "", cmdArgs[match])
-        return(script_name)
-    } else {
-      stop("Cannot determine script name.")
-    }
-}
-
-# @function: Grab the directory of the createSampleGrid script that runs this file. These two files are meant to be in the same directory. Not very flexible. The files accounts for running from interactive repl when testing or from Rscript via cli.
-get_script_dir <- function() {
-    cat("Grabbing script directory.", "\n")
-  if (!is.null(sys.frames()[[1]]$ofile)) {
-    # 'source'd via R console
-    script_dir <- dirname(normalizePath(sys.frames()[[1]]$ofile))
-  } else {
-    # Running script via Rscript cli.
-    script_name <- get_script_name()
-    script_dir <- dirname(normalizePath(script_name))
-    }
-
-  return(script_dir)
-}
-
-# @function: Given a directory_path and subdirectories, create the directory path along with the subdirectories.
-create_experiment_dir <- function(directory_path, subdirectories){
-    if (!dir.exists(directory_path)) {
-        dir.create(directory_path, recursive = TRUE)
-    } else {
-        cat(sprintf("Directory %s already exists.\n", directory_path))
-    }
-    sapply(file.path(directory_path, subdirectories), function(path_to_create) {
-        if (!dir.exists(path_to_create)) {
-            dir.create(path_to_create)
-        } else {
-            cat(sprintf("Directory %s already exists.\n", path_to_create))
-        }
-    })
-    #print(list.dirs(directory_path, recursive = FALSE))
-
-}
-
-# @function: Determine if arguments were provided.
-validate_input <- function() {
-    cat("Running input validation.", "\n")
-    args <- commandArgs(trailingOnly = TRUE)
-    if (length(args) != 1) {
-        cat("This script requires one argument.", "\n")
-        cat("Confirm WINDOWS_USER defined in bashrc.", "\n")
-        cat("Usage: ", "Rscript ", "000_setupExperimentDir.R", "<experiment_name>", "\n",
-            "Example: ", "Rscript ", "000_setupExperimentDir.R", "240808Bel", "\n")
-        stop("Provide one argument corresponding to experiment ID from BMC.")
-    }
-    if (Sys.getenv("WINDOWS_USER") != "") {
-        cat("Assigning username variable: ", Sys.getenv("WINDOWS_USER"), "\n")
-        username <- Sys.getenv("WINDOWS_USER")
-    } else {
-        cat("WINDOWS_USER not defined or exported from bash", "\n")
-        cat("Consult bashrc in my_config repository", "\n")
-        stop()
-    }
-    experiment_name <- args[1]
-    return(
-        list(
-            username = username,
-            experiment_name = experiment_name
-        )
-    )
-}
-
-output_tables_in_list <- function(experiment_directory, list_of_tables, OUTPUT_TABLE = FALSE){
-        experiment_name <- basename(experiment_directory)
-        if (!(typeof(list_of_tables) == "list")){
-            stop("Argument must be a list.")
-        }
-        names_of_tables <- names(list_of_tables)
-        for (name_of_table in names_of_tables){
-            output_table <- sample_config_output[[name_of_table]]
-            cat("============\n")
-            print(head(output_table))
-            output_file_path <- file.path(experiment_directory, "documentation", paste(experiment_name, "_", name_of_table, ".tsv", sep = ""))
-            cat(sprintf("Outputting to %s: \n", output_file_path))
-            if(OUTPUT_TABLE) {
-                write.table(output_table, file = output_file_path, sep = "\t", row.names = FALSE)
-            } else {
-                cat("Skip writing table. MODIFY OUTPUT_TABLE value to output.\n")
-            }
-        }
-}
-
-if(!interactive()) {
-    main()
-}
diff --git a/next_generation_sequencing/000_generalSetup/001_downloadDataFromBMC.sh b/next_generation_sequencing/000_generalSetup/001_downloadDataFromBMC.sh
deleted file mode 100755
index 8a2d913..0000000
--- a/next_generation_sequencing/000_generalSetup/001_downloadDataFromBMC.sh
+++ /dev/null
@@ -1,126 +0,0 @@
-#!/bin/bash
-#DESCRIPTION: Download the fastq data from BMC servers. 
-#USAGE: ./001_downloadDataFromBMC.sh <bmc_server> <experiment_directory>
-#NOTES: Each function contains some form of validation. 
-validate_input() {
-    echo "Executing validate_input"
-    if [ $# -ne 2 ]; then 
-        echo "Description: Use srun rsync command to download fastq files and remove unneeded directories and files."
-        echo "Usage: $0 <bmc_server> <experiment_directory>"
-        echo "Example: $0 bmc-pub17 240808Bel"
-        exit 1
-    fi
-        
-    local bell_lab_directory=$2
-    echo "Executing organize_files"
-    local target_dir="$HOME/data/${bell_lab_directory}/fastq/"
-    if [ ! -d "$target_dir" ]; then
-        echo "$target_dir"
-        echo "Target directory doesnt exist."
-        echo "Run 000_setupExperimentDir.R"
-        exit 1
-    fi
-    
-    local bmc_server=$1
-    local bmc_dir="/net/${bmc_server}/data/bmc/public/Bell/${bell_lab_directory}/"
-    if [ ! -d "$bmc_dir" ]; then
-        echo "$bmc_dir"
-        echo "Source directory doesnt exist."
-        echo "Check bmc email to determine Data Ready ${bell_lab_directory} to verify bmc server."
-        exit 1
-    fi
-}
-
-download_files() {
-    echo "Executing download_files"
-    local bmc_server=$1
-    local bell_lab_directory=$2
-    local target_dir="$HOME/data/${bell_lab_directory}/fastq/"
-    local bmc_dir="/net/${bmc_server}/data/bmc/public/Bell/${bell_lab_directory}/"
-    rsync_command="srun rsync -av --include '*/' --include '*.fastq' --exclude '*' $bmc_dir ${target_dir}"
-    echo "Executing: ${rsync_command}"
-    if ! eval ${rsync_command}; then 
-        echo "Rysnc failed. Check your parameters or connection"
-        exit 1
-    fi
-}
-
-move_fastq_files() {
-    local bell_lab_directory=$1
-    echo "Executing organize_files"
-    local target_dir="$HOME/data/${bell_lab_directory}/fastq/"
-    echo "${target_dir}"
-    cd "$target_dir" || exit 1
-    echo "Shifted to $target_dir"
-    find . -type f -name "*.fastq" -exec mv {} . \;
-}
-
-safe_remove_dirs() {
-    echo "Executing safe_remove_dirs"
-    local pattern=$1
-    local dirs_to_remove=$(find . -type d -name "$pattern")
-    if [ -n "$dirs_to_remove" ]; then
-        echo "The following directories mathc the pattern ${pattern}:"
-        echo "$dirs_to_remove"
-        read -p "DELETE these directories? (y/n) " -n 1 -r 
-        echo
-        if [[ $REPLY =~ ^[Yy]$ ]]; then
-            local staging_dir="tmp/staging_deleter_$$"
-            mkdir -p "$staging_dir"
-            echo "Moving directories to staging area"
-            echo "$dirs_to_remove" | xargs -I {} mv {} "$staging_dir/"
-            echo "Removing directories from staging area"
-            rm -rf "$staging_dir"
-            echo "Deletion complete"
-        else
-            echo "Deletion aborted"
-        fi
-    else
-        echo "No directories found matching pattern ${pattern}"
-    fi
-}
-
-safe_remove_files() {
-    echo "Executing safe_remove_files"
-    local pattern=$1
-    local files_to_remove=$(ls $pattern 2>/dev/null)
-    if [ -n "$files_to_remove" ]; then
-        echo "The following files match pattern ${pattern}: "
-        echo "$files_to_remove"
-        read -p "DELETE these files? (y/n) " -n 1 -r
-        echo 
-        if [[ $REPLY =~ ^[Yy]$ ]]; then
-            local staging_dir="tmp/staging_deleter_$$"
-            mkdir -p "$staging_dir"
-            echo "Moving files to staging area"
-            echo "$files_to_remove" | xargs -I {} mv {} "$staging_dir/"
-            echo "Removing files from staging area"
-            rm -rf "$staging_dir"
-            echo "Deletion complete"
-        else
-            echo "Deleteion aborted"
-        fi
-    else
-        echo "No files found matching ${pattern}"
-    fi
-}
-cleanup() {
-    echo "Executing cleanup"
-    safe_remove_dirs "*D24*"
-    safe_remove_dirs "infosite*"
-    safe_remove_files "*unmapped*.fastq"
-}
-
-main() {
-    echo "Executing main"
-    validate_input "$@"
-    local bmc_server=$1
-    local bell_lab_directory=$2
-    download_files "$bmc_server" "$bell_lab_directory"
-    move_fastq_files "$bell_lab_directory"
-    cleanup
-    echo "Main complete."
-    echo "Verify downloading files with find $HOME/data/${bell_lab_directory}/fastq -type f -name "*.fastq" | wc -l"
-}
-
-main "$@"
diff --git a/next_generation_sequencing/000_generalSetup/002_consolidateFastq.sh b/next_generation_sequencing/000_generalSetup/002_consolidateFastq.sh
deleted file mode 100755
index 111ce08..0000000
--- a/next_generation_sequencing/000_generalSetup/002_consolidateFastq.sh
+++ /dev/null
@@ -1,115 +0,0 @@
-#!/bin/bash
-#DESCRIPTION: Consolidate multiple fastq files into one per sample.
-#USAGE: ./000_consolidateFastq.sh <experiment_name>
-#NOTE: Run inside a node. srun --pty bash
-
-#set -euo pipefail
-set -eo pipefail
-EXPERIMENT_DIR=""
-OUTPUT_DIR=""
-FASTQ_PATHS=()
-UNIQUE_IDS=()
-
-#Useful error message.
-print_usage() {
-    echo "Printing usage statement:"
-    echo "$0 consolidates fastq files for a given experiment."
-    echo "Usage: $0 <experiment_name>"
-    echo "Example: $0 \"240808Bel\""
-    echo "Dont include trailing slash."
-    exit 1
-}
-
-#Check that one argument was provided with no trailing slash and see that the argument is a directory.
-validate_input(){
-    local experiment_name="$1"
-    echo "Starting input validation"
-    if [ $# -ne 1 ]; then
-        echo "Error: Experiment name is required." >&2
-        print_usage
-    fi
-    if [[ "$1" == */ ]]; then
-        echo "Error: Please provide the experiment name without a trailing slash." >&2
-        echo "Example: 'test' instead of 'test/'" >&2
-        exit 1
-    fi
-    EXPERIMENT_DIR="$HOME/data/$1"
-    if [ ! -d "$EXPERIMENT_DIR" ]; then
-        echo "Error: Experiment directory not found: $EXPERIMENT_DIR" >&2
-        exit 1
-    fi
-    echo "Input validation complete."
-}
-
-#Create output_dir variable.
-setup_directories() {
-    echo "Setting up directories."
-    OUTPUT_DIR="${EXPERIMENT_DIR}/fastq/"
-    mkdir -p "$OUTPUT_DIR"
-    echo "Directories set up."
-}
-
-#Find fastq files to create array. Do not include files with processed or unmapped strings in their name.
-get_fastq_files() {
-    echo "Gettitng fastq files."
-    mapfile -t FASTQ_PATHS < <(find "$EXPERIMENT_DIR" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \) | sort )
-    echo "Found ${#FASTQ_PATHS[@]} fastq files."
-}
-
-#Use awk to extract unique IDs from the fastq array.
-get_unique_ids() {
-    echo "Extracting unique IDs."
-    #See lmarena thread awkForUniqueIDs to update the awk script.
-    UNIQUE_IDS=($(printf '%s\n' "${FASTQ_PATHS[@]}" | awk -F'[_-]' '{print $3}' | sort -u))
-    echo "Found ${#UNIQUE_IDS[@]} unique IDs."
-}
-
-#Create output file. Then for fastq file, determine if it has the unique ID and cat to the output file.
-process_single_id() {
-    local unique_id=$1
-    local output_file="${OUTPUT_DIR}D24-${unique_id}_NA_sequence.fastq"
-    echo "Processing ID: ${unique_id}, Output: ${output_file}"
-    for fastq_path in "${FASTQ_PATHS[@]}"; do 
-        if [[ $fastq_path =~ $unique_id ]]; then
-            echo "Append and delete $fastq_path to $output_file"
-            if cat "${fastq_path}" >> "$output_file"; then
-                rm "$fastq_path"
-                echo "Appended and deleted $fastq_path"
-            else 
-                echo "Error appending $fastq_path. Not deleted." >&2
-                return 1
-            fi
-        fi
-    done
-}
-
-#For each unique ID extracted by awk, process the id. See above.
-process_fastq_files() {
-    echo "Processing fastq files by unique ids."
-    for unique_id in "${UNIQUE_IDS[@]}"; do
-        process_single_id "$unique_id"
-    done
-    echo "All unique_ids processed."
-}
-
-#Output numbers that will let me know that processing went well.
-# @todo: Check documentation folder to output the lines in the tsv file. That will let me confirm that everything went well.
-print_summary() {
-    echo "Files processed: ${#FASTQ_PATHS[@]}"
-    echo "Number of Unique IDs: ${#UNIQUE_IDS[@]}"
-    echo "Files in directory: $( find "$EXPERIMENT_DIR" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \) | sort | wc -l )"
-    #echo "Number of samples in experiment: $(wc -l < "${EXPERIMENT_DIR}/documentation/${1}_bmc_table.tsv")"
-}
-main() {
-    validate_input "$@"
-    setup_directories
-    get_fastq_files
-    get_unique_ids
-    process_fastq_files
-    print_summary
-    echo "Number of lines in experiment: $( wc -l < ${EXPERIMENT_DIR}/documentation/${1}_bmc_table.tsv)"
-    echo "Substract one to account to account for header."
-    #echo "Unique ID extracted by awk: ${UNIQUE_IDS[1]}"
-}
-
-main "$@"
diff --git a/next_generation_sequencing/000_generalSetup/003_updateSampleGrid.R b/next_generation_sequencing/000_generalSetup/003_updateSampleGrid.R
deleted file mode 100644
index 726e903..0000000
--- a/next_generation_sequencing/000_generalSetup/003_updateSampleGrid.R
+++ /dev/null
@@ -1,134 +0,0 @@
-#PURPOSE: Process the BMC sample grid data to create a csv file with the sample number, information and short name for further processing
-#USAGE: Run as a script with Rscript and positional arguments or replace commented directory to scan line. 
-#Example: Rscript ./001_R_node_processBMCSampleGridDataCSV.R <directory_to_process>
-#DEPENDENCY: Assumes directory structure of directory_creation.sh and requires the BMC_sample_grid_data_*.csv file. 
-#TODO: How to deal with multiple BMC files. Maybe just have it as separate directory for each sequencing run. Maybe will neeed if I do a bunch of samples.
-#TODO: Need to integrate with script that creates the grid in the first place which is not in the repository currently. Need to provide as argument for the sample_ID and access with args[2] then use nrow to create the last number in the seq function.
-validate_input <- function(args) {
-    if(length(args) != 1){
-        cat("No argument provided. One argument is required.\n")
-        cat("Usage: Rscript 003_updateSampleGrid.R <directory>\n")
-        cat("Example: Rscript 003_updateSampleGrid.R 240630Bel\n")
-        stop()
-    } 
-    experiment_name <- args[1]
-    directory_path <- file.path(Sys.getenv("HOME"), "data", experiment_name)
-    if(!dir.exists(directory_path)) {
-        stop("Directory does not exist:", directory_path)
-    } else {
-        cat(sprintf("Using directory %s:\n", directory_path))
-    }
-    return(directory_path)
-}
-
-table_has_ID_column <- function(sample_table){
-    if(!("sample_ID" %in% colnames(sample_table))){
-        cat("No sample_ID column found.\n")
-        cat("Must determine sample_IDs from fastq files\n")
-        return(FALSE)
-    } else {
-        cat("Table has sample_ID column.\n")
-        return(TRUE)
-    }
-}
-
-determine_sample_id <- function(directory_path) {
-    fastq_directory_path <- file.path(directory_path, "fastq")
-    fastq_file_paths <- list.files(fastq_directory_path, pattern = "*.fastq", full.names = TRUE)
-    if(length(fastq_file_paths) == 0) {
-        cat(sprintf("No fastq files found in %s\n", fastq_directory_path))
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project and download files from BMC\n")
-        stop()
-    } else {
-        cat(sprintf("Found %s files in %s.\n", length(fastq_file_paths), fastq_directory_path))
-    }
-    fastq_file_names <- basename(fastq_file_paths)
-    ID_regex <- "\\d{5,6}"
-    fastq_split_string_list <- strsplit(fastq_file_names, "_|-")
-    sample_IDs <- lapply(fastq_split_string_list, function(fastq_split_string_list) {
-        for(split_string in fastq_split_string_list) {
-            if(grepl(ID_regex, split_string)) {
-                return(split_string)
-            }
-        }
-    })
-    if(!all(unlist(lapply(sample_IDs, length)) == 1)) {
-        cat("At least one of the files did not extract exactly one sample ID.\n")
-        cat("Files with problems:\n")
-        print(fastq_file_names[unlist(lapply(sample_IDs, length)) != 1])
-        cat("Verify sample names. Redownload from BMC if necessary.\n")
-        cat(sprintf("Regex pattern used %s:\n", ID_regex))
-        stop()
-    } else {
-        sample_IDs <- unlist(sample_IDs)
-        cat(sprintf("Found %s sample_IDs.\n", length(sample_IDs)))
-        cat(sprintf("First sample_ID: %s\n",sample_IDs[1]))
-        cat(sprintf("Last sample_ID: %s\n", sample_IDs[length(sample_IDs)]))
-        cat("Returning sample_ID array.\n")
-        return(sample_IDs)
-    }
-}
-
-modify_and_output_table <- function(sample_table, sample_ID_array, output_file_path) {
-    if(nrow(sample_table) != length(sample_ID_array)) {
-        cat("Number of rows is different from length of sample_ID_array.\n")
-        cat("Verify fastq file names to ensure proper number is being extracted.\n")
-        cat(sprintf("Number of rows: %s\n", nrow(sample_table)))
-        cat(sprintf("Length of array: %s\n", length(sample_ID_array)))
-        stop()
-    } else if ("sample_ID" %in% colnames(sample_table)) {
-        cat("sample_ID already part of the sample table.\n")
-        print(colnames(sample_table))
-        stop()
-    } else {
-        sample_table$sample_ID <- sample_ID_array
-        print(head(sample_table))
-        write.table(sample_table, output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
-        cat("Output modified sample table with sample ID column.\n")
-    }
-}
-
-load_sample_table <- function(directory_path) {
-    cat("Loading sample_table from", directory_path, "\n")
-    documentation_dir_path <- file.path(directory_path, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n")
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat(sprintf("Files found in %s\n", documentation_dir_path))
-        print(sample_table)
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n")
-        stop()
-    }
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    cat(sprintf("Reading %s\n", sample_table_path))
-    cat("Head of sample_table\n")
-    print(head(sample_table))
-    return(sample_table)
-}
-
-main <- function() {
-    args <- commandArgs(trailingOnly = TRUE)
-    directory_path <- validate_input(args)
-    sample_table <- load_sample_table(directory_path)
-    if(table_has_ID_column(sample_table)){
-        cat("Sample table has sample ID column")
-        return(sample_table)
-    } else {
-        cat("Sample table doesnt have sample ID column.\n")
-        cat("Determine sample IDs and overwrite sample table.\n")
-        documentation_dir_path <- file.path(directory_path, "documentation")
-        output_file_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-        sample_IDs <- determine_sample_id(directory_path)
-        modify_and_output_table(sample_table, sample_IDs, output_file_path)
-        sample_table <- load_sample_table(directory_path)
-        return(sample_table)
-    }
-}
-
-if(!interactive()){
-    main()
-}
diff --git a/next_generation_sequencing/000_generalSetup/chipPackageInstallation.R b/next_generation_sequencing/000_generalSetup/chipPackageInstallation.R
deleted file mode 100644
index 6e0616e..0000000
--- a/next_generation_sequencing/000_generalSetup/chipPackageInstallation.R
+++ /dev/null
@@ -1,54 +0,0 @@
-# Initialize renv for reproducibility
-if (!requireNamespace("renv", quietly = TRUE)) install.packages("renv")
-renv::init()
-
-# Load BiocManager
-if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
-library(BiocManager)
-
-# Define package lists for different analysis tasks
-bioconductor_packages_for_fastq <- c("ShortRead", "Rsubread", "Biostrings", "dada2")
-bioconductor_packages_to_install_tracks <- c("QuasR", "GenomicAlignments", "Gviz", "rtracklayer")
-bioconductor_packages_to_install_peaks <- c("ChIPseeker", "ChIPpeakAnno", "DiffBind", "normR", "mosaics", "csaw")
-bioconductor_packages_to_install_motifs <- c("motifStack", "TFBSTools", "JASPAR2020", "universalmotif", "memes")
-bioconductor_packages_to_install_visualization <- c("ggbio", "ComplexHeatmap", "EnhancedVolcano", "ggplot2", "ggcoverage", "gggenome")
-bioconductor_packages_to_install_statistics <- c("DESeq2", "edgeR", "limma")
-
-# Additional CRAN packages
-cran_packages <- c("ggplot2", "rmarkdown", "knitr", "tidyverse", "furrr")
-
-# Combine all packages into a single vector
-all_packages_to_install <- c(
-  bioconductor_packages_for_fastq,
-  bioconductor_packages_to_install_tracks,
-  bioconductor_packages_to_install_peaks,
-  bioconductor_packages_to_install_motifs,
-  bioconductor_packages_to_install_visualization,
-  bioconductor_packages_to_install_statistics,
-  cran_packages
-)
-
-# Remove duplicates
-all_packages_to_install <- unique(all_packages_to_install)
-
-# Install all packages
-BiocManager::install(all_packages_to_install)
-
-# Load all installed packages
-invisible(lapply(all_packages_to_install, library, character.only = TRUE))
-
-# Print confirmation message
-cat("All packages have been installed and loaded successfully.\n")
-
-# Print session info for reproducibility
-sessionInfo()
-
-# Snapshot the current state of the project
-renv::snapshot()
-
-# TODO: Add your analysis workflow below
-# Consider using R Markdown for a reproducible report
-# Example:
-# rmarkdown::render("analysis_workflow.Rmd")
-
-# TODO: Consider implementing a Snakemake or Nextflow pipeline for full reproducibility
diff --git a/next_generation_sequencing/000_generalSetup/packageInstallation.R b/next_generation_sequencing/000_generalSetup/packageInstallation.R
deleted file mode 100644
index 93099ff..0000000
--- a/next_generation_sequencing/000_generalSetup/packageInstallation.R
+++ /dev/null
@@ -1,18 +0,0 @@
-# 
-#
-#home_directory <- Sys.getenv("HOME")
-#library_location <- paste(home_directory, "R/x86_64-pc-linux-gnu-library/4.2", sep = "/")
-#Check that library to installs is correct.
-#.libPaths()
-
-#TODO: Need to add renv for this module.
-library(BiocManager)
-bioconductor_packages_for_fastq <- c()
-bioconductor_packages_to_install_tracks <- c("QuasR", "GenomicAlignments", "Gviz", "rtracklayer", "ShortRead")
-BiocManager::install(bioconductor_packages_to_install
-        lib = library_location)
-bioconductor_packages_to_install_peaks <- c()
-bioconductor_packages_to_install_motifs <- c()
-bioconductor_packages_to_install_visualization <- c()
-bioconductor_packages_to_install_statistics <- c()
-bioconductor_packages_to_install_reproducibility <- c()
diff --git a/next_generation_sequencing/000_generalSetup/sampleGridConfigTemplate.R b/next_generation_sequencing/000_generalSetup/sampleGridConfigTemplate.R
deleted file mode 100644
index 573b3b8..0000000
--- a/next_generation_sequencing/000_generalSetup/sampleGridConfigTemplate.R
+++ /dev/null
@@ -1,358 +0,0 @@
-# Description: Configures and generates a sample table with experiments.
-# Usage: Rscript sampleGridConfigAndExprTemplate.R <experiment_id>
-
-#@update
-# Directory and expected number of samples
-current_experiment <- "240808Bel"
-expected_number_of_samples <- 33
-args <- c(current_experiment, expected_number_of_samples)
-main <- function(args) {
-    cat("Processing sampleGridConfig.R file\n")
-    validated_args <- validate_input(args)
-    current_experiment <- validated_args$current_experiment
-    #print(current_experiment)
-    all_categories_list <- define_categories()
-    ordered_samples <- generate_filtered_samples(all_categories_list)
-    verify_number_of_samples(ordered_samples, validated_args$expected_number_of_samples)
-    named_samples <- add_sample_names_to_table(ordered_samples)
-    named_samples$experiment_id <- current_experiment
-    bmc_table <- create_bmc_table(named_samples)
-    table_with_comparisons <- add_comparisons(named_samples)
-
-    # Define the columns that determine the control columns.
-    #@update
-    control_factors <- list(
-        genotype = c("strain_source", "rescue_allele", "mcm_tag")
-      )
-    complete_table <- add_attributes(table_with_comparisons, control_factors)
-    # Rest of the scripts tests the functions to reread the table after processing.
-    #print("Processing complete table after adding attributes as columns")
-    reread_table <- process_control_factors(complete_table)
-    #print("Determining processing to find control")
-    factors_to_match <- get_factors_to_match(reread_table)
-
-    sample_row <- reread_table[8, ]
-    control_index <- determine_matching_control(sample_row = sample_row, reread_table, factors_to_match)
-    control_index <- select_control_index(control_index)
-    # This will give you a logical vector indicating which rows match
-
-    #print("Indexing complete table")
-    #print(reread_table[control_index, ])
-    #print(attr(reread_table, "control_factors"))
-    cat("Loaded all functions and testing variables.\n")
-    print_summary(complete_table, bmc_table)
-    return(list(
-        sample_table = complete_table,
-        bmc_table = bmc_table
-    ))
-}
-
-validate_input <- function(args) {
-    if(length(args) != 2){
-        cat("Error: Script requires two arguments.\n")
-        cat("Usage: Rscript sampleGridConfigAndExprTemplate.R <experiment_id> <expected_number_of_samples>\n")
-        stop()
-    } else if (is.numeric(args[2])) {
-        cat("Error: Second argument must be a number larger than zero.\n")
-        cat("Usage: Rscript sampleGridConfigAndExprTemplate.R <experiment_id> <expected_number_of_samples>\n")
-        stop()
-    }
-    validated_args <- list(
-        current_experiment = args[1],
-        expected_number_of_samples = args[2]
-    )
-    return(validated_args)
-}
-
-# Define categories
-define_categories <- function() {
-    cat("Defining categories...\n")
-    #@update
-    all_categories_list <- list(
-        strain_source = c("lemr", "oa"),
-        rescue_allele = c("none", "wt"),
-        mcm_tag = c("none", "2", "7"),
-        auxin_treatment = c("no", "yes"),
-        cell_cycle = c("G1", "M"),
-        antibody = c("Input", "ProtG", "ALFA", "HM1108", "74", "CHA", "11HA")
-    )
-    return(all_categories_list)
-}
-
-filter_samples <- function(combinations_grid){
-    #@update
-    # Define impossible combinations 
-    impossible_combinations <- with(combinations_grid,
-        ( strain_source == "lemr" & mcm_tag == "7" ) |
-        ( strain_source == "lemr" & mcm_tag == "2" ) |
-        ( strain_source == "oa" & mcm_tag == "2" ) |
-        ( strain_source == "oa" & rescue_allele == "wt" )
-    )
-
-    valid_combinations <- combinations_grid[!impossible_combinations, ]
-
-    #@update
-    conditions <- list(
-        is_input = with(valid_combinations,
-            rescue_allele == "none" &
-            cell_cycle == "M" &
-            antibody == "Input" &
-            ((strain_source == "oa" & auxin_treatment == "no") |
-            (strain_source == "lemr" & auxin_treatment == "no"))
-        ),
-        is_protg = with(valid_combinations,
-                rescue_allele == "wt" &
-                mcm_tag == "none" &
-                cell_cycle == "M" &
-                antibody == "ProtG" &
-                strain_source == "lemr" &
-                auxin_treatment == "no"
-        ),
-        is_alfa = with(valid_combinations,
-            rescue_allele == "none" &
-            mcm_tag == "none" &
-            cell_cycle == "M" &
-            antibody == "ALFA" &
-            (( strain_source == "oa" & auxin_treatment == "no") | ( strain_source == "lemr"))
-        ),
-        is_1108 =  with(valid_combinations,
-                rescue_allele == "none" &
-                mcm_tag == "none" &
-                cell_cycle == "M" &
-                antibody == "HM1108" &
-               (( strain_source == "oa" &  auxin_treatment == "no") | strain_source == "lemr")
-        ),
-        is_174 = with(valid_combinations,
-            antibody == "74" &
-            auxin_treatment == "no"
-        ),
-        is_cha = with(valid_combinations,
-            antibody == "CHA" &
-            auxin_treatment == "no"
-         ),
-        is_11HA = with(valid_combinations,
-            antibody == "11HA" &
-            auxin_treatment == "no" &
-            !(strain_source == "lemr" & mcm_tag == "none" & rescue_allele == "wt" & cell_cycle == "M")
-        )
-    )
-    # Combine all conditions and apply final filter
-    experimental_conditions <- Reduce('|', conditions)
-    #@update
-    return(valid_combinations[experimental_conditions, ])
-}
-
-generate_filtered_samples <- function(categories_list) {
-    cat("Generating and filtering samples\n")
-    combinations <- expand.grid(categories_list)
-    filtered_samples <- filter_samples(combinations)
-    #@update
-    reorder_index <- with(filtered_samples, order(antibody, strain_source, rescue_allele, mcm_tag, auxin_treatment, cell_cycle))
-    ordered_samples <- filtered_samples[reorder_index, ]
-    return(ordered_samples)
-}
-
-add_sample_names_to_table <- function(ordered_samples_table) {
-    cat("Adding names to sample table\n")
-    ordered_samples_table$full_name <- apply(ordered_samples_table, 1, paste, collapse = "_")
-    colnames_sans_fullname <- !grepl("full_name", colnames(ordered_samples_table))
-    ordered_samples_table_sans_fullname <- ordered_samples_table[, colnames_sans_fullname]
-    ordered_samples_table$short_name <- apply(ordered_samples_table_sans_fullname, 1, function(row) paste0(substr(row, 1, 1), collapse = ""))
-    return(ordered_samples_table)
-}
-
-add_comparisons <- function(ordered_samples_table) {
-    #@update
-    cat("Adding columns with comparison values\n")
-    df <- ordered_samples_table
-    comparisons <- list(
-        #Effect of MCM: Input, G1 and M phase for 174
-        comp_cellCycle = with(df,
-                strain_source == "lemr" & antibody == "74"
-        ),
-
-        #Effect of Alfa: Input, no auxin, yes auxin
-        comp_alfa = with(df,
-                antibody == "ALFA"
-        ),
-
-        #Effect of CHA in cell cycle: Input, G1 and M phase for CHA
-        comp_HaCoa = with(df,
-                strain_source == "oa" & antibody == "CHA"
-        ),
-
-        #Effect of CHA in cell cycle: Input, G1 and M phase for CHA
-        comp_11HAoa = with(df,
-                strain_source == "oa" & antibody == "11HA"
-        )
-    )
-
-    # For all comparisons, create column with that name and TRUE/FALSE values for rows.
-    for(comp_name in names(comparisons)) {
-        df[[comp_name]] <- comparisons[[comp_name]]
-
-    }
-    return(df)
-}
-
-add_attributes <- function(table_with_comparisons, control_factors) {
-    cat("Adding attributes to column names\n")
-    df <- table_with_comparisons
-    control_columns <- unlist(unname(control_factors))
-    at_least_one_not_in_df_column <- !all(control_columns %in% colnames(df))
-    if(at_least_one_not_in_df_column){
-        control_column_not_in_df <- which(!(control_columns %in% colnames(df)))
-        #cat("Columns not in the sample table\n")
-        #print(control_columns[control_column_not_in_df])
-        stop("Verify the columns in categories and control_factors list to ensure you are assigning correctly\n")
-    }
-
-    for (factor in names(control_factors)) {
-        #cat(sprintf("Assign column to %s\n", factor))
-        new_column_name <- paste0("X__cf_", factor)
-
-        df[[new_column_name]] <- paste(control_factors[[factor]], collapse = ",")
-    }
-    return(df)
-
-}
-
-create_bmc_table <- function(named_samples_table) {
-    cat("Making bmc_table from sample table\n")
-    bmc_table <- data.frame(SampleName = named_samples_table$full_name,
-       Vol..uL = 10,
-       Conc = 0,
-       Type = "ChIP",
-       Genome = "Saccharomyces cerevisiae",
-       Notes = ifelse(named_samples_table$antibody == "Input", "Run on fragment analyzer.", "Run on femto pulse."),
-       Pool = "A"
-    )
-    return(bmc_table)
-}
-
-print_summary <- function(sample_table, bmc_table) {
-    cat("Printing results...\n")
-    cat("Elements of sample_table:\n")
-    print(sample_table)
-    cat("First elements of BMC table:\n")
-    print(head(bmc_table))
-    cat("Dimensions of sample_table:\n")
-    print(dim(sample_table))
-    cat("Dimensions of bmc_table:\n")
-    print(dim(bmc_table))
-    cat("Breakdown by antibody:\n")
-    print(table(sample_table$antibody))
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n")
-    df <- sample_table
-    cf_cols <- grep("^X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing X__cf_ tag found in sample table")
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("^X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table) {
-    cat("Grabbing attributes from sample table\n")
-    df <- sample_table
-    control_factors <- attr(df, "control_factors")
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n")
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n")
-    if (length(control_indices) == 0) {
-    stop("No matching control found")
-    }
-    if (length(control_indices) > max_controls) {
-    warning(paste("Multiple matching controls found, using first", max_controls))
-    control_indices[1:max_controls]
-    } else {
-    control_indices
-    }
-}
-
-verify_number_of_samples <- function(ordered_samples, expected_number_of_samples) {
-    if (nrow(ordered_samples) != expected_number_of_samples){
-        cat("Breakdown by antibody\n")
-        print(table(ordered_samples$antibody))
-        cat("All samples\n")
-        print(ordered_samples)
-        cat(sprintf("Number of samples after filtering: %s\n", nrow(ordered_samples)))
-        cat(sprintf("Number of samples expected: %s\n", expected_number_of_samples))
-        stop("Number of rows filtered does not match expected number of samples.")
-    }
-}
-
-if(!interactive()) {
-    sample_config_output <- main(args)
-    #print(sample_config_output$sample_table)
-    print_summary(sample_config_output$sample_table, sample_config_output$bmc_table)
-    cat("Script run through command line.\n")
-    cat("Open in text editor and modify @update tag sections then run 000_setupExperimentDir.R.\n")
-} else {
-    validated_args <- validate_input(args)
-    current_experiment <- validated_args$current_experiment
-    print(current_experiment)
-    # Define the define_categories list
-    all_categories_list <- define_categories()
-    # Define the samples that should be retained.
-    ordered_samples <- generate_filtered_samples(all_categories_list)
-    verify_number_of_samples(ordered_samples, validated_args$expected_number_of_samples)
-    named_samples <- add_sample_names_to_table(ordered_samples)
-    bmc_table <- create_bmc_table(named_samples)
-    named_samples$experiment_id <- current_experiment
-    table_with_comparisons <- add_comparisons(named_samples)
-    # Define the columns that determine the control columns which are the inputs that have the same genotype.
-    control_factors <- list(
-        genotype = c("strain_source", "rescue_allele", "mcm_tag")
-      )
-    # Attributes will be assign to a column that will process into attributes when the
-    # tsv is read back in.
-    complete_table <- add_attributes(table_with_comparisons, control_factors)
-    #print(head(complete_table))
-    # Rest of the scripts tests the functions to reread the table after processing.
-
-    #print("Processing complete table after adding attributes as columns")
-    complete_table <- process_control_factors(complete_table)
-    #print("Determining processing to find control")
-    factors_to_match <- get_factors_to_match(complete_table)
-
-    #sample_row <- complete_table[8, ]
-    #control_index <- determine_matching_control(sample_row = sample_row, complete_table, factors_to_match)
-    #control_index <- select_control_index(control_index)
-    # This will give you a logical vector indicating which rows match
-
-    #print("Indexing complete table")
-    #print(complete_table[control_index, ])
-
-    print_summary(complete_table, bmc_table)
-    cat("Loaded all functions and testing variables.\n")
-    ordered_samples <- generate_filtered_samples(all_categories_list)
-
-    cat("Script sourced from repl.\n")
-    cat("Open in text editor and modify @update tag sections then run 000_setupExperimentDir.R.\n")
-}
diff --git a/next_generation_sequencing/001_referenceGenomes/001_downloadAndProcessReferenceGenomes.sh b/next_generation_sequencing/001_referenceGenomes/001_downloadAndProcessReferenceGenomes.sh
deleted file mode 100755
index 6722b6a..0000000
--- a/next_generation_sequencing/001_referenceGenomes/001_downloadAndProcessReferenceGenomes.sh
+++ /dev/null
@@ -1,63 +0,0 @@
-#!/bin/bash
-
-set -euo pipefail
-
-# Script Name: refactored_genome_processing.sh
-# Description: Download, reorganize, and index reference genomes using NCBI datasets
-# Usage: ./refactored_genome_processing.sh [OPTIONS]
-
-# Global variables
-DOWNLOAD_DIR="$HOME/data/REFGENS"
-LOG_DIR="$HOME/data/REFGENS/logs"
-
-# Function to download genomes using datasets
-download_genomes() {
-    local accessions=("$@")
-    mkdir -p "$DOWNLOAD_DIR"
-
-    for accession in "${accessions[@]}"; do
-        echo "Downloading genome for accession: $accession"
-        local timestamp=$(date +"%Y%m%d_%H%M%S")
-        local target_dir="${DOWNLOAD_DIR}/${accession}_${timestamp}"
-        mkdir -p "$target_dir"
-        if ! datasets download genome accession "$accession" --include genome,rna,cds,protein,gff3,gtf --filename "${target_dir}/${accession}.zip"; then
-            echo "Download failed for accession: $accession. Check network or accession validity." >&2
-            continue
-        fi
-
-        unzip "${target_dir}/${accession}.zip" -d "$target_dir"
-        rm "${target_dir}/${accession}.zip"
-
-        echo "Download complete for: $accession"
-        echo "Downloaded to: $target_dir"
-    done
-}
-
-# Function to build Bowtie2 index
-build_bowtie2_index() {
-    local refgenome_dir="$HOME/data/REFGENS"
-    local genome_paths
-    mapfile -t genome_paths < <(find "${refgenome_dir}" -type f -name "GCF*.fna" -o -name "GCA*.fna")
-
-    for genome_path in "${genome_paths[@]}"; do
-        echo "Starting indexing for $genome_path"
-        #bowtie2-build "$genome_path" "${genome_path%.fna}_index"
-        echo "Indexing completed for $genome_path"
-    done
-}
-
-# Main function
-main() {
-    local accessions=(
-        "GCF_000146045.2"  # S. cerevisiae S288C
-        "GCF_000001405.40" # Human
-        "GCF_000005845.2"  # E. coli
-        "GCA_002163515.1"  # S cerevisaie W303
-    )
-    download_genomes "${accessions[@]}"
-    #build_bowtie2_index
-    echo "All operations completed successfully."
-}
-
-# Run the main function
-main "$@"
diff --git a/next_generation_sequencing/001_referenceGenomes/002_reorganizeReferenceGenomesDirs.sh b/next_generation_sequencing/001_referenceGenomes/002_reorganizeReferenceGenomesDirs.sh
deleted file mode 100755
index 05c2417..0000000
--- a/next_generation_sequencing/001_referenceGenomes/002_reorganizeReferenceGenomesDirs.sh
+++ /dev/null
@@ -1,41 +0,0 @@
-#!/bin/bash
-# Script Name: <script_name>
-# Description: Reorganize the reference genomes to be in a single directory
-# Usage: (Inside the REFGENS directory) ./002_sh_node_reorganizeReferenceGenomesDirs.sh
-#TODO: Determine if I should delete while loop. 
-# Get the directory containing the downloaded files
-# This is redundant but I added the section to see the directories but there is an echo inside the for loop.
-while IFS= read -r dir; do
-    download_dirs+=("$dir") # Replace with your actual directory
-    echo $dir
-done < <(find . -maxdepth 1 -type d | grep "/")
-#("GCA_000146045.2" "GCA_000005845.2" "GCA_000001405.29" "GCA_002163515.1")
-
-for download_dir in "${download_dirs[@]}"; do
-    echo "Reorganizing $download_dir."
-
-    # Extract organism name from assembly_data_report.jsonl (modified grep and sed)
-    # SPECIFIC_SOLUTION: Some of the files extracted organismName more than once and I couldnt process the newline or space out. Workaround is to output the name to text file and grab first line.   
-    organism_name=$(grep -m 1 -o '"organismName":"[^"]*' "$download_dir/ncbi_dataset/data/assembly_data_report.jsonl" | cut -d '"' -f4 | sed 's/ //g' )
-    echo "$organism_name" > organism_name.txt
-    organism_name=$(cat organism_name.txt | head -n 1)
-    echo "Organism name is $organism_name" 
-    rm organism_name.txt
-    # Find all files recursively and move them to the root directory
-    find "$download_dir/" -type f -exec mv -v {} "$download_dir/" \;
-
-    #TEST:
-    #find "$download_dir/" -type f -exec echo {} "$download_dir/" \;
-
-      # Rename fasta file based on organism name (optional)
-    mv -v "$download_dir"/cds_from_genomic.fna "$download_dir"/cds.fna
-    find "$download_dir" -name "*_genomic.fna" -exec mv -v {} "$download_dir"/"$organism_name"_refgenome.fna \;
-
-    echo "Files reorganized and renamed based on organism name."
-    rm -rf "$download_dir"/ncbi_dataset
-    mv "$download_dir" "$organism_name" && echo "Renamed dir and removed empty dir."
-done
-
-# Need to rename the chromosome to the bioconductor standard (USCS I think)
-#awk '/^>/ {gsub(/chromosome/, "chr", $6); printf(">%s%s\n", $6, $7)}' data/REFGENS/SaccharomycescerevisiaeS288C/SaccharomycescerevisiaeS288C_refgenome.fna | sed 's/,.*//' > test.fasta
-#awk '/^>/ {gsub(/chromosome/, "chr", $6); printf(">%s%s\n", $6, $7)} !/^>/ {print $0}' data/REFGENS/SaccharomycescerevisiaeS288C/SaccharomycescerevisiaeS288C_refgenome.fna | sed 's/,.*//' | head -n 10
diff --git a/next_generation_sequencing/001_referenceGenomes/003_modifyS288cFastaHeaders.sh b/next_generation_sequencing/001_referenceGenomes/003_modifyS288cFastaHeaders.sh
deleted file mode 100644
index 36a3408..0000000
--- a/next_generation_sequencing/001_referenceGenomes/003_modifyS288cFastaHeaders.sh
+++ /dev/null
@@ -1,23 +0,0 @@
-# Description: Renames the fasta hearders for the S288c genome to fit USCS specification.
-# Usage: I ran the lines individually on the command line as a way to test. 
-# Not idempotent so cant continually test. Run only once.
-#Find the S288C genome to modify the headers to UCSC standards
-#TODO: Determine if the while loop works as it looks more efficient.
-REFGENOME_DIR="$HOME/data/REFGENS"
-PATH_TO_S288C_GENOME=$(find $REFGENOME_DIR -type f -name "*S288C_refgenome.fna")
-#Create a backup file. 
-BACKUP_FILE="${PATH_TO_S288C_GENOME%_refgenome.fna}_backup.fna"
-cat $PATH_TO_S288C_GENOME > $BACKUP_FILE
-
-# Process S288C genome file: Convert chromosome names, reformat headers, and remove extra information
-awk '/^>/ {gsub(/chromosome/, "chr", $6); printf(">%s%s\n", $6, $7)} !/^>/ {print $0}' "$BACKUP_FILE" | sed 's/,.*//' > $PATH_TO_S288C_GENOME
-
-#while IFS= read -r line; do
-#  if [[ $line == ">"* ]]; then
-#    echo "${line/chromosome/chr}" | cut -d',' -f1
-#  else
-#    echo "$line"
-#  fi
-#done < "$BACKUP_FILE" > "$PATH_TO_S288C_GENOME"
-#To recreate the original from backup 
-#cat $BACKUP_FILE > $PATH_TO_S288C_GENOME
diff --git a/next_generation_sequencing/001_referenceGenomes/004_bt2buildRefGenomes.sh b/next_generation_sequencing/001_referenceGenomes/004_bt2buildRefGenomes.sh
deleted file mode 100755
index 23ba400..0000000
--- a/next_generation_sequencing/001_referenceGenomes/004_bt2buildRefGenomes.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G # amount of RAM per node#
-#DESCRIPTION: Build the bt2 index for the reference genomes. Uses slurm to distribute each reference genome to a task to process in parallel.
-#USAGE: From anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/slurm_003_bt2buildRefGenomes.sh'
-
-# Define the log directory
-LOG_DIR="$HOME/data/REFGENS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/indexing_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/indexing_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-# Your script's commands follow...
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-refgenome_dir="$HOME/data/REFGENS"
-echo "Executing from $refgenome_dir"
-
-module purge
-module load gnu/5.4.0
-module load bowtie2/2.3.5.1
-
-mapfile -t genome_paths < <(find "${refgenome_dir}" -type f -name "*_refgenome.fna")
-
-genome_path=${genome_paths[$SLURM_ARRAY_TASK_ID-1]}
-
-echo "Starting indexing for $genome_path"
-#Confirmed bowtie2-build works
-bowtie2-build $genome_path "${genome_path%_refgenome.fna}_index"
-
-echo "Indexing completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END"
diff --git a/next_generation_sequencing/001_referenceGenomes/downloadReferenceGenomes.sh b/next_generation_sequencing/001_referenceGenomes/downloadReferenceGenomes.sh
deleted file mode 100755
index 721c523..0000000
--- a/next_generation_sequencing/001_referenceGenomes/downloadReferenceGenomes.sh
+++ /dev/null
@@ -1,39 +0,0 @@
-#!/bin/bash
-
-# Script Name: 001_downloadReferenceGenomes.sh
-# Description: Download reference genomes to the REFGENS directory (relative path)
-# Usage: ./001_downloadReferenceGenomes.sh
-
-# Define base URL for NCBI genome downloads
-base_url="https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession"
-
-# Define accessions to download (replace with your accessions)
-accessions=("GCA_000146045.2" "GCA_000005845.2" "GCA_000001405.29" "GCA_002163515.1")
-
-# Set a download directory (modify as needed)
-download_dir="./REFGENS"
-
-# Create the download directory if it doesn't exist
-mkdir -p "$download_dir"
-
-for accession in "${accessions[@]}"; do
-    echo "Downloading genome for accession: $accession"
-
-    # Construct download URL with desired annotation types
-    download_url="${base_url}/${accession}/download?include_annotation_type=GENOME_FASTA&include_annotation_type=GENOME_GFF&include_annotation_type=RNA_FASTA&include_annotation_type=CDS_FASTA&include_annotation_type=PROT_FASTA&include_annotation_type=SEQUENCE_REPORT&hydrated=FULLY_HYDRATED"
-
-    # Download genome archive with error handling
-    if curl -o "$download_dir/${accession}_genome.zip" "$download_url"; then
-        echo "Download successful for $accession"
-    else
-        echo "Download failed for accession: $accession. Check network or accession validity."
-        continue  # Skip to the next iteration if download fails
-    fi
-
-    # Unzip archive with suppressed output
-    unzip > /dev/null 2>&1 "$download_dir/${accession}_genome.zip" -d "$download_dir/${accession}_genome" && rm "$download_dir/${accession}_genome.zip"
-
-    echo "Download and extraction complete for accession: $accession"
-done
-
-echo "All requested genomes have been downloaded and extracted to $download_dir."
diff --git a/next_generation_sequencing/001_referenceGenomes/tests/003_test_bt2buildRefGenomes.sh b/next_generation_sequencing/001_referenceGenomes/tests/003_test_bt2buildRefGenomes.sh
deleted file mode 100755
index e253fac..0000000
--- a/next_generation_sequencing/001_referenceGenomes/tests/003_test_bt2buildRefGenomes.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G # amount of RAM per node#
-#USAGE: From anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/test_bt2build_refgenomes.sh'
-
-# Define the log directory
-LOG_DIR="$HOME/data/REFGENS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-
-# Construct the file names
-OUT_FILE="${LOG_DIR}/testIndexing_${SLURM_ARRAY_JOB_ID}_${SLURM_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out"
-ERR_FILE="${LOG_DIR}/testIndexing_${SLURM_ARRAY_JOB_ID}_${SLURM_JOB_ID}_${SLURM_ARRAY_TASK_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-# Your script's commands follow...
-echo "Processing with SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")" 
-refgenome_dir="$HOME/data/REFGENS"
-echo "Executing from $refgenome_dir"
-
-module purge
-module load gnu/5.4.0
-module load bowtie2/2.3.5.1
-
-mapfile -t genome_paths < <(find "${refgenome_dir}" -type f -name "*_refgenome.fna")
-
-genome_path=${genome_paths[$SLURM_ARRAY_TASK_ID-1]}
-
-echo "Starting indexing for $genome_path"
-#Confirmed bowtie2-build works 
-echo $genome_path "${genome_path%_refgenome.fna}_index"
-
-echo "Indexing completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END"
diff --git a/next_generation_sequencing/002_controlData/001_downloadEatonData.sh b/next_generation_sequencing/002_controlData/001_downloadEatonData.sh
deleted file mode 100755
index d8d2dca..0000000
--- a/next_generation_sequencing/002_controlData/001_downloadEatonData.sh
+++ /dev/null
@@ -1,31 +0,0 @@
-#!/bin/bash
-#DESCRIPTION: Download data from Eaton 2010 paper as control and comparison. 
-#USAGE: 001_downloadEatonData.sh <dir>
-#NOTE: Only meant to be run once to download control data from Eaton 2010 paper.
-#TODO: Provide arguments for files to concatenate and accession to generalize the script. 
-
-#BIOPROJECT_ACCESSION=PRJNA117641
-#Eaton2010 
-#Title ORC precisely positios nucleosomes at origins of replication
-#DOWNLOAD ORC WT samples in G2 arrest.
-DOWNLOAD_TO_DIR="$1"
-OUTPUT_DIR="${HOME}/data/${DOWNLOAD_TO_DIR}"
-mkdir -p $OUTPUT_DIR
-BASE_URL="ftp://ftp.sra.ebi.ac.uk/vol1/fastq/"
-
-mapfile -t FILES_TO_CONCATENATE < <(printf "%s\n" WT-G2-ORC-rep{1,2}.fastq.gz)
-mapfile -t ACCESSION < <(printf "%s\n" SRR0344{75,76})
-
-FILE_INDEX=0
-for FILE in ${FILES_TO_CONCATENATE[@]}; do 
-    OUTPUT_FILE=${OUTPUT_DIR}$FILE
-    URL_TO_DOWNLOAD=${BASE_URL}${ACCESSION[FILE_INDEX]:0:6}/${ACCESSION[FILE_INDEX]}/${ACCESSION[FILE_INDEX]}.fastq.gz 
-    printf "%s | %s | %s\n" "$FILE_INDEX" "$OUTPUT_FILE" "$URL_TO_DOWNLOAD"
-    curl -I $URL_TO_DOWNLOAD 
-  #  wget --output-document=$OUTPUT_FILE $URL_TO_DOWNLOAD
-  #  cat $OUTPUT_FILE >> $OUTPUT_DIR/nnNnH.fastq.gz
-    ((FILE_INDEX++))
-done 
-
-gunzip ${OUTPUT_DIR}nnNnH.fastq.gz
-#echo "gunzip ${OUTPUT_DIR}nnNnH.fastq.gz"
diff --git a/next_generation_sequencing/002_controlData/002_downloadFeatureData.sh b/next_generation_sequencing/002_controlData/002_downloadFeatureData.sh
deleted file mode 100755
index 442db55..0000000
--- a/next_generation_sequencing/002_controlData/002_downloadFeatureData.sh
+++ /dev/null
@@ -1,39 +0,0 @@
-#!/bin/bash
-#DESCRIPTION: Download feature data from Rossi 2021 paper that will be used to categorical analysis and plot tracking.
-#USAGE: $./002_sh_downloadFeatureData.sh
-
-usage() {
-    echo "Usage: $0 [download_dir]"
-    echo "Downloads Rossi 2021 data from GitHub."
-    echo "If download_dir is not specified, defaults to $HOME/data/feature_files"
-}
-
-main() {
-    local download_dir="${1:-$HOME/data/feature_files}"
-    
-    echo "Starting Rossi 2021 data download..."
-    
-    if [ ! -d "$download_dir" ]; then
-        echo "Creating directory: $download_dir"
-        mkdir -p "$download_dir"
-    fi
-    
-    echo "Cloning Rossi 2021 repository..."
-    if git clone --depth=1 https://github.com/CEGRcode/2021-Rossi_Nature.git "$download_dir/rossi_2021"; then
-        echo "Successfully cloned Rossi 2021 data to $download_dir/rossi_2021"
-    else
-        echo "Error: Failed to clone Rossi 2021 repository" >&2
-        exit 1
-    fi
-    
-    echo "Rossi 2021 data download complete."
-}
-
-# If user provides the -h or --help option, display the usage function output.
-if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
-    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
-        usage
-        exit 0
-    fi
-    main "$@"
-fi
diff --git a/next_generation_sequencing/002_controlData/003_downloadTimingAndSgdData.R b/next_generation_sequencing/002_controlData/003_downloadTimingAndSgdData.R
deleted file mode 100644
index 3d095a8..0000000
--- a/next_generation_sequencing/002_controlData/003_downloadTimingAndSgdData.R
+++ /dev/null
@@ -1,174 +0,0 @@
-#!/usr/bin/env Rscript
-library(readr)
-library(dplyr)
-library(curl)
-
-# Function to download a file
-download_file <- function(url, dest_path) {
-    tryCatch({
-        curl_download(url, dest_path, mode = "wb")
-        cat(sprintf("Successfully downloaded: %s\n", dest_path))
-    }, error = function(e) {
-        stop(sprintf("Error downloading %s: %s", url, e$message))
-    })
-}
-
-# Function to create directory if it doesn't exist
-ensure_dir <- function(dir_path) {
-    if (!dir.exists(dir_path)) {
-    dir.create(dir_path, recursive = TRUE)
-    cat(sprintf("Created directory: %s\n", dir_path))
-    }
-}
-
-# Adjust SGD_features column names
-adjust_sgd_features <- function(df) {
-    # Names taken from SGD_features.README
-    new_names <- c(
-        "Primary_SGDID", "Feature_type", "Feature_qualifier", "Feature_name",
-        "Standard_gene_name", "Alias", "Parent_feature_name", "Secondary_SGDID",
-        "Chromosome", "Start_coordinate", "Stop_coordinate", "Strand",
-        "Genetic_position", "Coordinate_version", "Sequence_version",
-        "Description"
-    )
-
-    if(ncol(df) != length(new_names)) {
-        cat(sprintf("Number of columns in df: %s\nLength of name array: %s\n", ncol(df), length(new_names)))
-        stop("Provided dataframe or table does not have same length as column names\n")
-    } else {
-        names(df) <- new_names
-    }
-    return(df)
-}
-
-# Adjust eaton acs bed file coordinates for proper loading as GRange object
-adjust_eaton_acs <- function(file_path) {
-    df <- read.table(file_path, sep = "\t", header = FALSE, col.names=c("chrom", "start", "end", "name", "score", "strand"))
-
-    # Identify problematic rows
-    df <- df %>%
-        mutate(width = end - start,
-               is_valid = width >= -1)
-
-    problematic <- df %>% filter(!is_valid)
-    cat(sprintf("Number of problematic rows %s:\n", nrow(problematic)))
-    
-    if (nrow(problematic) > 0) {
-    # Fix the problematic rows by swapping start and end
-        df_fixed <- df %>%
-                    mutate(temp = ifelse(start > end, start, end),
-                           start = ifelse(start > end, end, start),
-                           end = temp) %>%
-                    select(-temp, -width, -is_valid)
-        
-        fixed_file_path <- sub("\\.bed$", "_fixed.bed", file_path)
-        write.table(df_fixed, fixed_file_path, sep ="\t", quote=FALSE, row.names = FALSE, col.names = FALSE)
-        cat(sprintf("Fixed file written to:%s\n", fixed_file_path))
-    } else {
-        cat("No problematic rows found. File is valid.\n")
-    }
-}
-
-# Function that orchestrates file readers based on extension
-read_file <- function(file_path) {
-    cat(sprintf("Reading file: %s\n", basename(file_path)))
-    file_extension <- tools::file_ext(file_path)
-    file_readers <- list(
-        xls = readxl::read_excel,
-        xlsx = readxl::read_excel,
-        bed = rtracklayer::import.bed,
-        gff3 = function(file_path) rtracklayer::import(file_path, format = 'gff3'),
-        gff = function(file_path) rtracklayer::import(file_path, format = 'gff'),
-        tab = function(file_path) read_tsv(file_path, col_names = FALSE),
-        rds = readRDS
-    )
-
-    if (!(file_extension %in% names(file_readers))) {
-        stop(sprintf("Unsupported file type: %s", file_extension))
-    }
-    data <- file_readers[[file_extension]](file_path)
-    return(data)
-}
-# Main function
-main <- function() {
-    timestamp <- format(Sys.time(), "%y%m%d")
-    base_dir <- file.path(Sys.getenv("HOME"), "data", "feature_files")
-    ensure_dir(base_dir)
-    
-    cat("Defining the URLs and File paths...\n")
-    # Define URLs and file paths
-    data_sources <- list(
-        hawkins = list(
-            url = "https://ars.els-cdn.com/content/image/1-s2.0-S2211124713005834-mmc2.xlsx",
-            path = "hawkins-origins-timing.xlsx"
-        ),
-
-        eaton_peaks = list(
-            url = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM424nnn/GSM424494/suppl/GSM424494_wt_G2_orc_chip_combined.bed.gz",
-            path = "eaton_peaks.bed.gz"
-        ),
-
-        sgd_features = list(
-            url = "https://downloads.yeastgenome.org/curation/chromosomal_feature/SGD_features.tab",
-            path = "SGD_features.tab"
-        ),
-
-        sgd_gff = list(
-            url = "https://downloads.yeastgenome.org/curation/chromosomal_feature/saccharomyces_cerevisiae.gff.gz",
-            path = "saccharomyces_cerevisiae.gff.gz"
-        ),
-
-        eaton_acs = list(
-            url = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM424nnn/GSM424494/suppl/GSM424494_acs_locations.bed.gz",
-            path = "eaton_acs.bed.gz"
-        )
-    )
-    
-    
-    # Download files
-    cat("Starting data download...\n")
-    for (source in names(data_sources)) {
-        cat(sprintf("Downloading %s data...\n", source))
-        output_file <- file.path(base_dir , paste0(timestamp, "_", data_sources[[source]]$path))
-        download_file(data_sources[[source]]$url, output_file)
-        if (grepl("\\.gz$", output_file)) {
-            cat(sprintf("Gunzip %s file\n", output_file))
-            R.utils::gunzip(output_file, remove = TRUE)
-        }
-        
-        cat(sprintf("Download %s complete...\n", source))
-        cat(sprintf("Verifying %s data...\n", source))
-        output_file <- gsub("\\.gz$", "", output_file)
-
-        if (source == "sgd_features") {
-            sgd_df <- read_file(output_file)
-            sgd_df <- adjust_sgd_features(sgd_df)
-            cat(sprintf("\nHead of %s:\n", source))
-            print(head(sgd_df))
-        } else if (source == "eaton_acs") {
-            fixed_file_path <- sub("\\.bed$", "_fixed.bed", output_file)
-            adjust_eaton_acs(output_file)
-            df <- read_file(fixed_file_path)
-            cat(sprintf("\nHead of %s:\n", source))
-            print(head(df))
-        } else {
-            df <- read_file(output_file)
-            cat(sprintf("\nHead of %s:\n", source))
-            print(head(df))
-        }
-
-    }
-    
-    cat("Verified all files\n")
-    cat("rm 240830_eaton_acs.bed if you dont need the file.")
-    cat("Done.")
-
-    
-    # Optionally, add data validation here
-    # For example, check file sizes or run basic parsing
-}
-
-# Run the main function
-if (!interactive()) {
-    main()
-}
diff --git a/next_generation_sequencing/002_controlData/004_processFeaturesToRDSandBed.R b/next_generation_sequencing/002_controlData/004_processFeaturesToRDSandBed.R
deleted file mode 100644
index 40d7cce..0000000
--- a/next_generation_sequencing/002_controlData/004_processFeaturesToRDSandBed.R
+++ /dev/null
@@ -1,203 +0,0 @@
-#DESCRIPTION: Convert features files from Rossi 2021, Hawkins 2014, and Eaton 2010 to a bed and rds format for plotting tracks and factor analysis.
-#USAGE: Rscript ./004_processFeaturesToRdsAndBed.R 
-# @TODO: Still need to update the file processing. 
-# @TODO: Update 002_downloadFeatureData.sh for Rossi2021 repository then adjust this script to ensure that I can load everything as Grange object.
-# Load libraries
-suppressPackageStartupMessages({
-    library(tidyverse)
-    library(GenomicRanges)
-    library(rtracklayer)
-    library(readxl)
-    library(ChIPpeakAnno)
-    library(GenomicFeatures)
-})
-
-main <- function(input_dir) {
-    cat("Entering main function. All libraries loaded.\n")
-    feature_file_dir <- paste0(Sys.getenv("HOME"), "/data/feature_files")
-    validate_input(feature_file_dir)
-    pattern_to_exclude  <- "sample-key.tab|\\.rds$|\\_converted.bed$"
-    files_to_convert <- get_file_list(feature_file_dir, pattern_to_exclude = pattern_to_exclude)
-    for (file_path in files_to_convert) {
-        file_basename <- basename(file_path)
-        data <- read_file(file_path)
-        processed_data <- process_data(data, file_basename)
-        grange_data <- convert_to_granges(processed_data, file_basename)
-        output_processed_data(grange_data, file_basename, feature_file_dir)
-    }
-    pattern_to_verify <- "\\.rds$|\\_converted.bed$"
-    verify_output(feature_file_dir, pattern_in_file = pattern_to_verify)
-    cat("Transfer the files to dropbox for inspection\n")
-    cat("scp -r user@server:from_dir to_dir\n")
-}
-
-validate_input <- function(input_dir) {
-    cat(sprintf("Ensuring %s directory exists\n", input_dir))
-    if (!dir.exists(input_dir)) {
-        cat(sprintf("Directory %s does not exist.\n", input_dir))
-        stop()
-    }
-}
-
-get_file_list <- function(input_dir, pattern_to_exclude = "sample-key.tab|\\.rds$|\\_converted.bed$") {
-    cat("Getting file list...\n")
-    all_files <- list.files(path = input_dir, full.names = TRUE)
-    file_to_exclude <- grepl(pattern_to_exclude, all_files)
-    files <- all_files[!file_to_exclude]
-    cat(sprintf("Number of files to process: %s\n", length(files)))
-    return(files)
-}
-
-read_file <- function(file_path) {
-    cat(sprintf("Reading file: %s\n", basename(file_path)))
-    file_extension <- tools::file_ext(file_path)
-    file_readers <- list(
-        xls = readxl::read_excel,
-        xlsx = readxl::read_excel,
-        bed = rtracklayer::import.bed,
-        gff3 = function(file_path) import(file_path, format = 'gff3'),
-        tab = function(file_path) read.delim(file_path, header = FALSE, sep = "\t"),
-        rds = readRDS
-    )
-
-    if (!(file_extension %in% names(file_readers))) {
-        stop(sprintf("Unsupported file type: %s", file_extension))
-    }
-    data <- file_readers[[file_extension]](file_path)
-    return(data)
-}
-
-process_data <- function(data, file_name) {
-    cat(sprintf("Processing data for: %s\n", file_name))
-    if(grepl("timing", file_name)) {
-        data <- data %>%
-                as.data.frame() %>%
-                dplyr::select(1:7) %>%
-                filter(!is.na(Chromosome))
-        return(data)
-    } else if (grepl("SGD_features", file_name)) {
-        data <- data %>%
-                as.data.frame() %>%
-                dplyr::select(c(9:12, 2, 4, 5))
-        print(head(data))
-        return(data)
-    } else {
-        return(data)
-    }
-}
-
-convert_to_granges <- function(data, file_basename) {
-    cat("Converting to Granges...\n")
-    if(!is(data, "GRanges")) {
-        if (grepl("Nucleosome_calls", file_basename)) {
-            cat("Processing Nucleosome calls xlsx file\n")
-            columns_to_exclude <- c("Nucleosome ID", "Nucleosome dyad", "Chromosome")
-            metadata_dataframe <- data.frame(data[, !(colnames(data) %in% columns_to_exclude)])
-            colnames(metadata_dataframe) <- gsub(" |\\.", "_", colnames(metadata_dataframe))
-            data <- GRanges(seqnames = data$`Nucleosome ID`, ranges = IRanges(start = data$`Nucleosome dyad`, end = data$`Nucleosome dyad`), strand = "*", chromosome = data$Chromosome, metadata_dataframe)
-            cat("Finished processing Nucleosome xlsx into grange.\n")
-            print(head(data))
-            return(data)
-        } else if (grepl("hawkins", file_basename)) {
-            cat("Processing hawkins timing xlsx file\n")
-            name_of_origins <- paste0(data$Chromosome, "_", data$Position)
-            columns_to_exclude <- c("Chromsome", "Position")
-            metadata_dataframe <- data.frame(data[, !(colnames(data) %in% columns_to_exclude)])
-            colnames(metadata_dataframe) <- gsub(" |\\.", "_", colnames(metadata_dataframe))
-            data <- GRanges(seqnames = name_of_origins, ranges = IRanges(start = data$Position-100,end = data$Position+100), strand = "*", chromosome = data$Chromosome, metadata_dataframe)
-            cat("Finished processing hawkins xlsx into grange.\n")
-            print(head(data))
-            return(data)
-        } else if (grepl("Rhee", file_basename)) {
-            cat("Processing Rhee transcription data file\n")
-            data <- data[!apply(data, 1, function(row) all(is.na(row))), ]
-            columns_to_exclude <- c("chrom", "TATA_coor", "strand", colnames(data)[grepl("color =class;", colnames(data))])
-            metadata_dataframe <- data.frame(data[, !(colnames(data) %in% columns_to_exclude)])
-            colnames(metadata_dataframe) <- gsub(" |\\.", "_", colnames(metadata_dataframe))
-            data$strand <- ifelse(data$strand == "W", "+", ifelse(data$strand == "C", "-", ifelse(data$strand, "*")))
-            data <- GRanges(seqnames = data$chrom, ranges = IRanges(start = data$TATA_coor, end = data$TATA_coor), strand = data$strand, chromosome = data$chrom, metadata_dataframe)
-            cat("Finished processing Rhee data file into grange")
-            print(head(data))
-            return(data)
-        } else if (grepl("SGD", file_basename)) {
-            data <- data[!apply(data, 1, function(row) all(is.na(row))), ]
-            columns_to_exclude <- c("V9", "V10", "V11", "V4")
-            metadata_dataframe <- data.frame(data[, !(colnames(data) %in% columns_to_exclude)])
-            colnames(metadata_dataframe) <- gsub(" |\\.", "_", colnames(metadata_dataframe))
-            data <- GRanges(seqnames = data$V4, ranges = IRanges(start = data$V10, end = data$V11), strand = data$V12, chromosome = data$V9, metadata_dataframe)
-            cat("Finished processing SGD data file into grange")
-            print(head(data))
-            return(data)
-        } else {
-            tryCatch({
-                as(data, "GRanges")
-            }, error = function(e) {
-                warning("Could not convert to GRanges. Returning as-is.")
-                print(head(data))
-                return(data)
-            })
-            print(head(data))
-       }
-    } else {
-        print(head(data))
-        return(data)
-    }
-}
-
-output_processed_data <- function(data, file_name, output_dir) {
-    cat(sprintf("Saving output for: %s to %s\n", file_name, output_dir))
-    rds_output_file <- file.path(output_dir,
-        paste0(tools::file_path_sans_ext(basename(file_name)), ".rds"))
-   # saveRDS(data, file = rds_output_file)
-    bed_output_file <- file.path(output_dir,
-        paste0(tools::file_path_sans_ext(basename(file_name)), "_converted.bed"))
-    #tryCatch(
-    #    rtracklayer::export.bed(data, con = bed_output_file),
-    #    error = function(e) stop(paste("Error exporting BED file:", e$message))
-    #)
-    cat(sprintf("Saved to: %s\n", bed_output_file))
-    cat(sprintf("Saved to: %s\n", rds_output_file))
-}
-
-verify_output <- function(output_dir, pattern_in_file = "_converted\\.bed") {
-    cat("Verifying generated output\n")
-    files_to_verify <- list.files(output_dir, pattern = pattern_in_file)
-    cat(sprintf("Number of files to verify: %s\n", length(files_to_verify)))
-    #file_readers <- list(
-    #        bed = rtracklayer::import,
-    #        rds = readRDS
-    #)
-
-    #file_converter <- list(
-    #        rds = function(file_path) makeGRangesFromDataFrame(file_path, keep.extra.columns = TRUE)
-    #)
-    if (length(files_to_verify) == 0) {
-        cat("No files to verify")
-        return(0)
-    }
-    for (file_path in files_to_verify) {
-         data <- rtracklayer::import.bed(file_path)
-         cat(sprintf("Successfully read: %s\n", basename(file_path)))
-        if (is(data, "GRanges")) {
-            cat(sprintf("File %s is a valid GRanges object\n", basename(file_path)))
-            cat("Sample data:\n")
-            print(head(data))
-        } else {
-            cat(sprintf("Warning: File %s is not a GRanges object\n", basename(file_path)))
-            print(head(data))
-        }
-    }
-}
-
-main()
-#AnnotationTrack(variable_name, name = paste(chromosome, "origins", sep = " "))
-#assign(df_names[i], readxl::read_excel(filepath))
-#if(df_names[i] == "timing"){
-#assign(df_names[i], get(df_names[i]) %>% as.data.frame() %>% dplyr::select(1:7) %>% filter(!is.na(Chromosome)))}
-#origin_GRange[seqnames(origin_GRange) == chromosome_to_plot]
-#origin_track <- AnnotationTrack(origin_GRange[seqnames(origin_GRange) == chromosome_to_plot], name = "Origins")
-#seqnames(origin_track) <- df_sacCer_refGenome$chrom_ID[chromosome_to_plot]
-#timing <- readxl::read_excel(list.files(feature_file_directory, pattern = "timing", full.names =TRUE)
-#) %>% as.data.frame %>% dplyr::select(1:7) %>% filter(!is.na(Chromosome))
-
-#assign(genome_df$origin_track_var[index_], AnnotationTrack(get(genome_df$origin_gr_var[index_]), name=paste(sacCer3_df$chrom[index_], "Origins", sep = " ")), envir = .GlobalEnv)
diff --git a/next_generation_sequencing/003_fastqProcessing/002_filterFastq.sh b/next_generation_sequencing/003_fastqProcessing/002_filterFastq.sh
deleted file mode 100755
index 7727776..0000000
--- a/next_generation_sequencing/003_fastqProcessing/002_filterFastq.sh
+++ /dev/null
@@ -1,65 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G # amount of RAM per node#
-#DESCRIPTION: Use slurm to perform filtering by fastp in parallel.
-#USAGE: Use via slurm wrapper. Requires directory.
-#TODO: Adjust the filtering step to be depending on the length distribution of the experiment.
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/filtering_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/filtering_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-
-#MODULE_LOAD
-module purge
-module load fastp/0.20.0
-
-#INITIALIZE_ARRAY
-mapfile -t fastq_paths < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \))
-
-#INPUT_OUTPUT
-fastq_path=${fastq_paths[$SLURM_ARRAY_TASK_ID-1]}
-output_path=$(echo "$fastq_path" | awk -F'/' '{print $NF}' | xargs -I {} echo "${DIR_TO_PROCESS}processedFastq/processed_{}")
-json_path=$(echo "$fastq_path" |  awk -F'/' '{print $NF}' | xargs -I {} echo "${DIR_TO_PROCESS}logs/processed_{}")
-#LOG
-echo "Starting fitering"
-echo "FASTQ_FILE: $fastq_path"
-echo "OUTPUT_FILE: $output_path"
-
-#COMMAND_TO_EXECUTE
-echo "COMMAND_OUTPUT_START"
-# Determine if the file is from Eaton control data. If it is, adjust the filtering parameters.
-if [[ $fastq_path =~ "Eaton" ]]; then
-    echo "Processing eaton data. Filtering length is 20"
-    fastp -i "${fastq_path}"  -o "${output_path}" --json "${json_path%.fastq}.json" --html /dev/null --cut_window_size 4 --cut_mean_quality 20 --n_base_limit 5 --average_qual 20 --length_required 20 --qualified_quality_phred 20 --unqualified_percent_limit 50
-else
-    fastp -i "${fastq_path}"  -o "${output_path}" --json "${json_path%.fastq}.json" --html /dev/null --cut_window_size 4 --cut_mean_quality 20 --n_base_limit 5 --average_qual 20 --length_required 50 --qualified_quality_phred 20 --unqualified_percent_limit 50
-fi
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Filtering completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END"
diff --git a/next_generation_sequencing/003_fastqProcessing/003_alignFastq.sh b/next_generation_sequencing/003_fastqProcessing/003_alignFastq.sh
deleted file mode 100755
index c3831ca..0000000
--- a/next_generation_sequencing/003_fastqProcessing/003_alignFastq.sh
+++ /dev/null
@@ -1,135 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#DESCRIPTION: Align fastq files to multiple genomes using Bowtie2 and Samtools via Slurm Sbatch command.
-#USAGE: sbatch --array=1-N 003_alignFastq.sh "240808Bel"
-# where N is (number of genomes) * (number of fastq files)
-#OR 
-#USE VIA 000_slurmWrapper.sh 
-#000_slurmWrapper.sh 1-N%16 003_alignFastq.sh "240808Bel"
-
-set -e 
-#set -u
-
-#Global variables
-#REFGENOME_DIR="$HOME/data/REFGENS"
-#EXPERIMENT_DIR=""
-#LOG_DIR=""
-#LOG_FILE=""
-
-#validate_input() {
-#    if [[ $# -ne 1 || "$1" == */ ]]; then
-#    echo "Usage: sbatch --array=<array-range> $0 <experiment_name>" >&2
-#        echo "Error: Invalid input. Please provide a single argument without a trailing slash. >&2
-#        echo "Example: sbatch --array=1-20%16 $0 240808Bel"
-#        exit 1
-#    fi
-#    
-#    EXPERIMENT_DIR="$HOME/data/$1"
-#    LOG_DIR="$HOME/data/$EXPERIMENT_DIR/logs"
-#
-#}
-#setup_logging() {
-#    mkdir -p "$LOG_DIR"
-#    local timeid=$(date "+%Y%m%d_%H%M%S")
-#
-#    LOG_FILE="${LOG_DIR}/aligning_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}_${timeid}.log"
-#
-#    # Redirect stdout and stderr to a temporary file
-#    exec 3>&1 4>&2
-#    exec 1>"$LOG_FILE" 2>&1
-#}
-#
-#log_message() {
-#    local timestamp=$(date "+%Y-%m-%d_%H:%M:%S")
-#    echo "[$timestamp] $1"
-#}
-#
-#load_modules() {
-#    log_message "Loading required modules"
-#    module purge
-#    module load gnu/5.4.0 bowtie2/2.3.5.1 samtools/1.10
-#}
-#
-#initialize_arrays() {
-#    mapfile -t FASTQ_PATHS < <(find "$EXPERIMENT_DIR
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-
-REFGENOME_DIR="$HOME/data/REFGENS"
-LOG_DIR="$HOME/data/${DIR_TO_PROCESS}/logs"
-# Define the log directory
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/aligning_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/aligning_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-
-
-#INITIALIZE_ARRAY
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" )
-mapfile -t GENOME_PATHS < <(find "${REFGENOME_DIR}" -type f -name "*_refgenome.fna")
-
-#INPUT_OUTPUT
-#fastq_path=${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}
-#output_path=$(echo "$fastq_path" | cut -d/ -f7 | xargs -I {} echo "${DIR_TO_PROCESS}processed-fastq/processed_{}")
-
-echo "Number of files to process: $(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" | wc -l )"
-echo "Number of files to process: $(find "${REFGENOME_DIR}" -type f -name "*_refgenome.fna" | wc -l )"
-#LOG
-# Perform indexing by getting quotient and remainder
-GENOME_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1) / ${#FASTQ_PATHS[@]}))
-FASTQ_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1)  % ${#FASTQ_PATHS[@]}))
-echo "Processing ${GENOME_INDEX} and ${FASTQ_INDEX}"
-
-#Get names for output
-FASTQ_ID=$(echo "${FASTQ_PATHS[$FASTQ_INDEX]%.fastq}" | awk -F'/' '{print $NF}'  )
-GENOME_NAME=$( echo "${GENOME_PATHS[$GENOME_INDEX]}" | awk -F'/' '{print $NF}' | cut -d_ -f1 )
-#GENOME_NAME=$( echo "${GENOME_PATHS[$GENOME_INDEX]}" | cut -d_ -f1 | rev | cut -d/ -f1 | rev )
-echo "FASTQ_ID and GENOME_NAME: "${FASTQ_ID}" | "$GENOME_NAME""
-
-echo "Starting alignment"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-#bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]
-#For testing grab a subset of the data using seqtk:seqtk sample [-2] [-s seed=11] <in.fa> <frac>|<number>
-#seqtk sample -s100 your_fastq_file.fastq 0.1 > subsampled_fastq_file.fastq
-
-module purge
-module load gnu/5.4.0 bowtie2/2.3.5.1 samtools/1.10
-
-# Align FASTQ reads to genome, convert to sorted BAM, and index
-# Uses Bowtie2 for alignment, Samtools for conversion, sorting, and indexing
-# Outputs: {FASTQ_ID}_{GENOME_NAME}.bam and its index file
-bowtie2 -x ${GENOME_PATHS[$GENOME_INDEX]%_refgenome.fna}_index -U ${FASTQ_PATHS[$FASTQ_INDEX]} -p $SLURM_CPUS_PER_TASK -q --mp 4 --met-stderr |
-samtools view -@ ${SLURM_CPUS_PER_TASK} -b - |
-samtools sort -@ ${SLURM_CPUS_PER_TASK} -o ${DIR_TO_PROCESS}alignment/${FASTQ_ID}_${GENOME_NAME}.bam -
-
-samtools index ${DIR_TO_PROCESS}alignment/${FASTQ_ID}_${GENOME_NAME}.bam
-
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Aligning completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo "TASK_END"
diff --git a/next_generation_sequencing/003_fastqProcessing/004_qualityControlFastq.sh b/next_generation_sequencing/003_fastqProcessing/004_qualityControlFastq.sh
deleted file mode 100755
index 9a5bc8f..0000000
--- a/next_generation_sequencing/003_fastqProcessing/004_qualityControlFastq.sh
+++ /dev/null
@@ -1,60 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#DESCRIPTION: Use fastqc program on all fastq files, pre and post processing with fastp.
-#USAGE: Use with slurm wrapper specifying the number of tasks. 
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load samtools/1.10
-module load fastqc/0.11.5
-
-#INITIALIZE_ARRAY
-mapfile -t PROCESSEDFQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "processed_*.fastq" )
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \))
-#LOG
-
-echo "Starting Quality Control"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-fastqc --outdir=${DIR_TO_PROCESS}qualityControl/ ${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]} 
-fastqc --outdir=${DIR_TO_PROCESS}qualityControl/ ${PROCESSEDFQ_PATHS[$SLURM_ARRAY_TASK_ID-1]} 
-
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Aligning completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo "TASK_END"
diff --git a/next_generation_sequencing/003_fastqProcessing/005_unzipFastqc.sh b/next_generation_sequencing/003_fastqProcessing/005_unzipFastqc.sh
deleted file mode 100755
index 2b73a2e..0000000
--- a/next_generation_sequencing/003_fastqProcessing/005_unzipFastqc.sh
+++ /dev/null
@@ -1,35 +0,0 @@
-#!/bin/bash
-#DESCRIPTION: Unzip the fastqc folders.
-#USAGE: Doesnt work as a script but can be run from the command line.
-
-
-#set -x
-
-if [ $# -ne 1 ]; then
-	echo -e "Provide directory to unzip fastqc files\n"
-	exit 1
-fi
-
-DIRECTORY_TO_PROCESS="$(find -H $HOME/data -maxdepth 1 -type d -name "$1")"
-
-if [ ! -d "$DIRECTORY_TO_PROCESS" ]; then
-	echo -e "Error: Directory doesnt exist\n"
-	exit 1
-fi
-
-echo "Will process $DIRECTORY_TO_PROCESS"
-sleep 1 
-find $DIRECTORY_TO_PROCESS/qualityControl -type f -name "*.zip" -exec echo {} \;
-
-read -p "Proceed with unzipping these files? (y/n): " confirm && [[ $confirm == [yY] ]] || echo "Exiting" && exit 1
-
-# If confirmed, proceed with deletion
-echo "Unzipping files..."
-
-#Can run this by itself on a particular directory. Not sure why it didnt work in the script.
-find $DIRECTORY_TO_PROCESS/qualityControl -type f -name "*.zip" -exec sh -c 'unzip "{}"' \; 
-#TODO Line above doesnt run inside script. Line below could be solution but havent tested. Could try moving directory, unzipping and moving back.
-#find "$DIRECTORY_TO_PROCESS/qualityControl" -type f -name "*.zip" -exec sh -c '
-#    cd "$(dirname "{}")" && unzip -o "$(basename "{}")" || echo "Failed to unzip: {}"' \;
-
-echo "Files have been unzipped."
diff --git a/next_generation_sequencing/003_fastqProcessing/006_parseFastqc.R b/next_generation_sequencing/003_fastqProcessing/006_parseFastqc.R
deleted file mode 100644
index b53ee26..0000000
--- a/next_generation_sequencing/003_fastqProcessing/006_parseFastqc.R
+++ /dev/null
@@ -1,71 +0,0 @@
-# DESCRIPTION: Parse FastQC files in a given directory and output summarized data
-# USAGE: Rscript lab_utils/next_generation_sequencing/003_fastqProcessing/006_R_node_parseFastqc.R dirname > output.log 2>&1
-args <- commandArgs(trailingOnly = TRUE)
-if (length(args) == 0 ) { print("No argument provided. Provide a directory to process") ; q() }
-
-get_current_datetime_string <- function() {
-                  return(format(Sys.time(), "%Y-%m-%d-%H-%M-%S"))
-}
-
-find_the_directory <- function(dirname){
-    data_directory <- paste0(Sys.getenv("HOME"), "/data")
-    contains_dirname <- grepl(dirname, list.dirs(data_directory, recursive = FALSE))
-    directory_path <- list.dirs(data_directory, recursive = FALSE)[contains_dirname]
-    if (length(directory_path) > 0 && dir.exists(directory_path)) { return(directory_path) }
-    else { print(paste0("Directory ", dirname, " not found")) ; return(NULL) }
-}
-
-directory_to_process <- find_the_directory(args[1])
-if (is.null(directory_to_process)) { print("Directory to process is null. Exiting script.") ; q() }
-
-message <- sprintf("Directory to process is %s.", directory_to_process)
-print(message)
-setwd(directory_to_process)
-
-fastqc_file_path <- list.files("./qualityControl", pattern = "fastqc_data", recursive = TRUE, full.names = TRUE)
-print(fastqc_file_path)
-print(paste0("Number of files to process ", length(fastqc_file_path)))
-
-number_of_files <- length(fastqc_file_path)
-for (file_idx in 1:number_of_files) {
-    current_time <- get_current_datetime_string()
-    lines <- readLines(fastqc_file_path[file_idx])
-    modules_starts <- which(grepl(">>", lines))
-    modules_ends <- which(grepl(">>END_MODULE", lines))
-    modules_starts <- modules_starts[!(modules_starts %in% modules_ends)]
-    output_dir <- dirname(fastqc_file_path[file_idx])
-    fastqc_summary <- list()
-    for (module_idx in seq_along(modules_starts)) {
-        module_lines <- lines[modules_starts[module_idx]:modules_ends[module_idx]]
-        module_summary <- gsub(">>", "", module_lines[1])
-        fastqc_summary <- append(fastqc_summary, module_summary)
-        module_filename <- gsub(" ", "", strsplit(module_summary, "\t")[[1]][1])
-        potential_headers <- which(grepl("^#", module_lines[2:length(module_lines)-1]))
-        last_potential_header <- potential_headers[length(potential_headers)] 
-        for (header_idx in potential_headers) {
-            number_of_elements_in_header <- length(strsplit(module_lines[header_idx], "\t")[[1]])
-            number_of_elements_in_line <- length(strsplit(module_lines[last_potential_header], "\t")[[1]])
-            if(number_of_elements_in_header == number_of_elements_in_line) {
-                header <- gsub("#", "", module_lines[header_idx])
-                data <- read.table(text = module_lines[(header_idx+1):length(module_lines)-1],
-                                   header = FALSE,
-                                   col.names = strsplit(header, "\t")[[1]],
-                                   sep = "\t")
-            }
-
-        }
-        output_file_name <- paste0(current_time, "_", "fastqc_", module_filename, ".tab")
-        output_file_path <- file.path(output_dir, output_file_name)
-        write.table(data, file = output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
-    }
-    fastqc_summary <- read.table(text = unlist(fastqc_summary),
-                       header = FALSE,
-                       col.names = c("Stat", "Value"),
-                       sep = "\t")
-    print(head(fastqc_summary))
-    output_file_name <- paste0(current_time, "_", "fastqc_", "summary", ".tab")
-    output_file_path <- file.path(output_dir, output_file_name)
-    write.table(fastqc_summary, file = output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
-}
-#Files can be deleted using: find "dir" -type f -name "*.tab" -exec rm {} + 
-#Use a dry-run or wc command to see what files and how many will be deleted.
diff --git a/next_generation_sequencing/003_fastqProcessing/tests/000_test_consolidateFastq.sh b/next_generation_sequencing/003_fastqProcessing/tests/000_test_consolidateFastq.sh
deleted file mode 100755
index 5ca7db1..0000000
--- a/next_generation_sequencing/003_fastqProcessing/tests/000_test_consolidateFastq.sh
+++ /dev/null
@@ -1,28 +0,0 @@
-# Define the directory to process
-ABSOLUTE_PATH_OF_DIR="$HOME/data/$1"
-OUTPUT_DIR="${ABSOLUTE_PATH_OF_DIR}fastq/"
-mkdir -p "$OUTPUT_DIR"
-
-# INITIALIZE_ARRAY
-mapfile -t FASTQ_PATHS < <(find "${ABSOLUTE_PATH_OF_DIR}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \) | sort )
-
-#Could substitute cut with awk -F '-' '{print $2}'
-#TODO may have to replace this by adding basename, ##*/ or awk with $NF 
-UNIQUE_IDS=($(printf '%s\n' "${FASTQ_PATHS[@]}" | cut -d- -f2 | uniq ))
-
-#${1}_D24-${UNIQUE_ID}_NA_sequence.fastq 
-for UNIQUE_ID in ${UNIQUE_IDS[@]}; do 
-	OUTPUT_FILE="${OUTPUT_DIR}D24-${UNIQUE_ID}_NA_sequence.fastq"
-	echo "Processing ID: ${UNIQUE_ID}, Output: ${OUTPUT_FILE}"
-	FILTERED_PATHS=($(printf '%s\n' "${FASTQ_PATHS[@]}" | grep "${UNIQUE_ID}")) 
-	echo "cat "${FILTERED_PATHS[@]}" >> ${OUTPUT_FILE}"
-	echo "Loop output"
-	for FASTQ_PATH in "${FASTQ_PATHS[@]}"; do
-		if [[ $FASTQ_PATH =~ $UNIQUE_ID ]]; then 
-			echo "cat "$FASTQ_PATH" >> "$OUTPUT_FILE""
-		fi
-	done
-	
-done 
-echo "Files processed: $(echo ${#FASTQ_PATHS[@]})"
-echo "Number of Unique IDS: $(echo ${#UNIQUE_IDS[@]})"
diff --git a/next_generation_sequencing/003_fastqProcessing/tests/001_test_filterFastq.sh b/next_generation_sequencing/003_fastqProcessing/tests/001_test_filterFastq.sh
deleted file mode 100755
index 3919e10..0000000
--- a/next_generation_sequencing/003_fastqProcessing/tests/001_test_filterFastq.sh
+++ /dev/null
@@ -1,60 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G # amount of RAM per node#
-#USAGE: From anywhere, run 'sbatch ~/lab_utils/next_generation_sequencing/test_001_filterFastq.sh <dir>/'. You must include the / for it to work...
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/filtering_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/filtering_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >"$OUT_FILE" 2>"$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-
-#MODULE_LOAD
-module purge
-module load fastp/0.20.0
-
-#INITIALIZE_ARRAY
-mapfile -t fastq_paths < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \))
-
-#INPUT_OUTPUT
-fastq_path=${fastq_paths[$SLURM_ARRAY_TASK_ID-1]}
-output_path=$(echo "$fastq_path" | awk -F'/' '{print $NF}' | xargs -I {} echo "${DIR_TO_PROCESS}processed-fastq/processed_{}")
-json_path=$(echo "$fastq_path" |  awk -F'/' '{print $NF}' | xargs -I {} echo "${DIR_TO_PROCESS}logs/processed_{}")
-#LOG
-echo "Starting fitering"
-echo "FASTQ_FILE: $fastq_path"
-echo "OUTPUT_FILE: $output_path"
-
-#Confirmed fastp works from node using example file
-#COMMAND_TO_EXECUTE 
-#fastp in slurm file
-echo "-i "${fastq_path}"  -o "${output_path}" --json "${json_path%.fastq}.json" --html /dev/null --cut_window_size 4 --cut_mean_quality 20 --n_base_limit 5 --average_qual 20 --length_required 50 --qualified_quality_phred 20 --unqualified_percent_limit 50"
-
-#COMMAND_OUTPUT_START
-#LOG
-echo "Filtering completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END
-"
diff --git a/next_generation_sequencing/003_fastqProcessing/tests/003_test_alignFastq.sh b/next_generation_sequencing/003_fastqProcessing/tests/003_test_alignFastq.sh
deleted file mode 100755
index 31ca396..0000000
--- a/next_generation_sequencing/003_fastqProcessing/tests/003_test_alignFastq.sh
+++ /dev/null
@@ -1,108 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#USAGE: First, determine this by running the INITIALIZE_ARRAY and multiplying by number of genomes, modify the array number. For test, leave at 1-2 to test array creation. Then, from anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/slurm_002_alignFastq.sh <dir>'
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/aligning_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/aligning_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >"$OUT_FILE" 2>"$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load bowtie2/2.3.5.1
-module load samtools/1.10
-
-#INITIALIZE_ARRAY
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "processed_*.fastq" )
-mapfile -t GENOME_PATHS < <(find "${REFGENOME_DIR}" -type f -name "*_refgenome.fna")
-
-#INPUT_OUTPUT
-#fastq_path=${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}
-#output_path=$(echo "$fastq_path" | cut -d/ -f7 | xargs -I {} echo "${DIR_TO_PROCESS}processed-fastq/processed_{}")
-
-#LOG
-# Perform indexing by getting quotient and remainder
-GENOME_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1) / ${#FASTQ_PATHS[@]}))
-FASTQ_INDEX=$(( ($SLURM_ARRAY_TASK_ID - 1)  % ${#FASTQ_PATHS[@]}))
-echo "Processing ${GENOME_INDEX} and ${FASTQ_INDEX}"
-
-#Get names for output
-#TODO cut -d- -f2 | cut -d_ -f1
-if [[ ${FASTQ_PATHS[$FASTQ_INDEX]} =~ "_" ]]; then 
-	echo -e "Filename contains '-' \n"
-fi
-if [[  ${FASTQ_PATHS[$FASTQ_INDEX]} =~ "_" ]]; then
-	echo -e "Filename contains '_' \n"
-fi
-
-FASTQ_ID=$(echo "${FASTQ_PATHS[$FASTQ_INDEX]}" | cut -d- -f2 | cut -d_ -f1 )
-GENOME_NAME=$( echo "${GENOME_PATHS[$GENOME_INDEX]}" | cut -d_ -f1 | rev | cut -d/ -f1 | rev )
-echo "$(basename $FASTQ_ID) | $(basename $GENOME_NAME)"
-
-#Loop to verify the proper indexing uncomment to verify how it works and accesses array
-#Total number if jobs is the product of the number of genomes and the number of FASTQ files
-#total_jobs=$(( ${#genomes_paths[@]} * ${#fastq_paths[@]} ))
-#
-## Print header
-#echo "SLURM_ARRAY_TASK_ID | GENOME_INDEX | FASTQ_INDEX | GENOME_PATH | FASTQ_PATH"
-#
-## Loop through each SLURM task ID
-#for (( task_id=1; task_id<=total_jobs; task_id++ )); do
-#    # Calculate the genome and FASTQ indices
-#    #Slurm is one-based but bash is 0-based
-#    genome_index=$(( (task_id - 1) / ${#fastq_paths[@]} ))
-#    fastq_index=$(( (task_id - 1) % ${#fastq_paths[@]} ))
-#    fastq_ID=$(echo "${fastq_paths[$fastq_index]}" | cut -d_ -f3 )
-#    genome_name=$( echo "${genomes_paths[$genome_index]}" | cut -d_ -f1 | rev | cut -d/ -f1 | rev )
-#
-#    # Print the indices and corresponding paths
-#    echo "$task_id | $genome_index | $fastq_index" #| $(basename ${genomes_paths[$genome_index]%_refgenome.fna}_index) | $(basename ${fastq_paths[$fastq_index]})"
-#    echo "${fastq_ID}_${genome_name}.sam"
-#done
-
-echo "Starting alignment"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-#bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]
-#For testing grab a subset of the data using seqtk:seqtk sample [-2] [-s seed=11] <in.fa> <frac>|<number>
-#seqtk sample -s100 your_fastq_file.fastq 0.1 > subsampled_fastq_file.fastq
-echo "bowtie2 -x ${GENOME_PATHS[$GENOME_INDEX]%_refgenome.fna}_index -U ${FASTQ_PATHS[$FASTQ_INDEX]} -p $SLURM_CPUS_PER_TASK -q --mp 4 --met-stderr |
-samtools view -@ ${SLURM_CPUS_PER_TASK} -b - |
-samtools sort -@ ${SLURM_CPUS_PER_TASK} -o ${DIR_TO_PROCESS}alignment/${FASTQ_ID}_${GENOME_NAME}.bam -"
-
-echo "samtools index ${DIR_TO_PROCESS}alignment/${FASTQ_ID}_${GENOME_NAME}.bam"
-
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Aligning completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END
-"
diff --git a/next_generation_sequencing/003_fastqProcessing/tests/004_test_qualityControlFastq.sh b/next_generation_sequencing/003_fastqProcessing/tests/004_test_qualityControlFastq.sh
deleted file mode 100755
index 450d971..0000000
--- a/next_generation_sequencing/003_fastqProcessing/tests/004_test_qualityControlFastq.sh
+++ /dev/null
@@ -1,58 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#USAGE: First, determine this by running the INITIALIZE_ARRAY and multiplying by number of genomes, modify the array number. For test, leave at 1-2 to test array creation. Then, from anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/slurm_002_alignFastq.sh <dir>'
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load samtools/1.10
-module load fastqc/0.11.5
-
-#INITIALIZE_ARRAY
-mapfile -t PROCESSEDFQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "processed_*.fastq" )
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \))
-#LOG
-
-echo "Starting Quality Control"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-echo "fastqc --outdir=${DIR_TO_PROCESS}fastqc/ ${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}" 
-echo "fastqc --outdir=${DIR_TO_PROCESS}fastqc/ ${PROCESSEDFQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}" 
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Aligning completed"
-echo -e "END TIME: $(date "+%Y-%m-%d-%M-%S")\n"
-echo -e "TASK_END
-"
diff --git a/next_generation_sequencing/003_fastqProcessing/tests/006_test_parseFastqc.R b/next_generation_sequencing/003_fastqProcessing/tests/006_test_parseFastqc.R
deleted file mode 100644
index d5fa24b..0000000
--- a/next_generation_sequencing/003_fastqProcessing/tests/006_test_parseFastqc.R
+++ /dev/null
@@ -1,96 +0,0 @@
-#Run with: Rscript lab_utils/next_generation_sequencing/003_fastqProcessing/006_R_node_readInFastqc.R > output.log 2>&1
-
-get_current_datetime_string <- function() {
-                  return(format(Sys.time(), "%Y-%m-%d-%H-%M-%S"))
-}
-#Run with source("/home/luised94/lab_utils/next_generation_sequencing/003_fastqProcessing/006_R_node_readInFastqc.R")
-setwd("/net/bmc-pub14/data/bell/users/luised94/240304Bel")
-#df_sample_info <- read.delim("./documentation/sample_info_table.csv", header = TRUE, sep = ",")
-#sample_ids <- df_sample_info$sample_ID
-#base_dir <- "./qualityControl"
-
-#qualityControl_dir <- list.dirs("./qualityControl", recursive = FALSE)
-#contains_sample_id <- grepl(sample_ids[1], list.dirs("./qualityControl", recursive = FALSE))
-#subset_qualityControl_dir <- qualityControl_dir[contains_sample_id][1]
-fastqc_file_path <- list.files("./qualityControl", pattern = "fastqc_data", recursive = TRUE, full.names = TRUE)	
-summary_file_path <- list.files("./qualityControl", pattern = "summary", recursive = TRUE, full.names = TRUE)	
-#head(fastqc_file_path)
-#head(summary_file_path)
-#delimiters <- c(",", "\t", ";", " ")
-max_index <- length(fastqc_file_path)
-for (i in 1:max_index) {
-current_time <- get_current_datetime_string()
-	lines <- readLines(fastqc_file_path[i])
-#	print(lines[1:5])
-	data_blocks <- list()
-	modules_starts <- which(grepl(">>", lines))
-	modules_ends <- which(grepl(">>END_MODULE", lines))
-	modules_starts <- modules_starts[!(modules_starts %in% modules_ends)]
-#	print(lines[modules_starts])
-#	print(lines[modules_ends])
-#	seq_along(modules_starts)
-output_dir <- dirname(fastqc_file_path[i])
-fastqc_summary <- list()
-for (i in seq_along(modules_starts)) {
-module_lines <- lines[modules_starts[i]:modules_ends[i]]
-module_summary <- gsub(">>", "", module_lines[1])
-fastqc_summary <- append(fastqc_summary, module_summary)
-module_filename <- gsub(" ", "", strsplit(module_summary, "\t")[[1]][1])
-potential_headers <- which(grepl("^#", module_lines[2:length(module_lines)-1]))
-last_potential_header <- potential_headers[length(potential_headers)] 
-for (header_idx in potential_headers) {
-	number_of_elements_in_header <- length(strsplit(module_lines[header_idx], "\t")[[1]])
-	number_of_elements_in_line <- length(strsplit(module_lines[last_potential_header], "\t")[[1]])
-	if(number_of_elements_in_header == number_of_elements_in_line) {
-	#	message <- sprintf("Proper header is %s", i)
-	#	print(message)
-	header <- gsub("#", "", module_lines[header_idx])
-data <- read.table(text = module_lines[(header_idx+1):length(module_lines)-1],
-				   header = FALSE,
-				   col.names = strsplit(header, "\t")[[1]],
-				   sep = "\t")
-}
-
-}
-
-print(data)
-#message <- sprintf("#CURRENT_INDEX: %s", i)
-#print(message)
-output_file_name <- paste0(current_time, "_", "fastqc_", module_filename, ".tab")
-output_file_path <- file.path(output_dir, output_file_name)
-print(output_file_path)
-#write.table(data, file = output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
-cat("\n")
-#for (delim in delimiters) {
-#    split_header <- strsplit(header, delim)[[1]]
-#	split_first_line <-strsplit(module_lines[3], delim)[[1]]
-#	if (length(split_header) > 1) {
-#      message <- sprintf("Delimiter %s produces more than one column for the header. Total length is %s." , delim, length(split_header))
-#	  print(message)
-#	  print("Header is:")
-#	  print(header)
-#	  print(split_header)
-#    }
-#
-#	if (length(split_first_line) > 1) {
-#      message <- sprintf("Delimiter %s produces more than one column for the header. Total length is %s." , delim, length(split_first_line))
-#	  print(message)
-#	  print("Line is:")	
-#      print(module_lines[3])
-#	  print(split_first_line)
-#    }
-#
-#}
-#
-	}
-print(unlist(fastqc_summary))
-output_file_name <- paste0(current_time, "_", "fastqc_", "summary", ".tab")
-output_file_path <- file.path(output_dir, output_file_name)
-print(output_file_path)
-#write.table(fastqc_summary, file = output_file_path, append = FALSE, quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
-}
-#write.table(data, file = output_file_name, sep = "\t", quotes = FALSE, col.names = TRUE)
-#if ((strsplit(module_summary, "\t")[[1]][1] == "Sequence Length Distribution") == FALSE) {
-#} else {
-#print("#SKIPPED Sequence Length Distribution")
-#}
diff --git a/next_generation_sequencing/004_bamProcessing/001_qualityControlBam.sh b/next_generation_sequencing/004_bamProcessing/001_qualityControlBam.sh
deleted file mode 100755
index 385f87a..0000000
--- a/next_generation_sequencing/004_bamProcessing/001_qualityControlBam.sh
+++ /dev/null
@@ -1,62 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#Description: Slurm script to use samtools to verify the bam files. Part of the quality control pipeline. 
-#USAGE: Use via slurm wrapper. $./001_sh_slurm_qualityControlBam.sh <directory>
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/qualityControl_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >> "$OUT_FILE" 2>> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load samtools/1.10
-module load fastqc/0.11.5
-
-#INITIALIZE_ARRAY
-mapfile -t BAM_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.bam" )
-
-FILENAME=$( echo ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]%.bam} | awk -F'/' '{print $NF}' )
-
-#LOG
-
-echo "Starting quality control check"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-
-samtools flagstat -O tsv ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamFlagstat.txt
-{ samtools quickcheck ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} && echo -e 'QUICKCHECK\tTRUE' || echo -e 'QUICKCHECK\tFALSE' ; } > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamQuickcheck.txt 
-samtools stats ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamStats.txt
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Quality control check completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo "TASK_END"
diff --git a/next_generation_sequencing/004_bamProcessing/002_checkQcBam.R b/next_generation_sequencing/004_bamProcessing/002_checkQcBam.R
deleted file mode 100644
index a938b21..0000000
--- a/next_generation_sequencing/004_bamProcessing/002_checkQcBam.R
+++ /dev/null
@@ -1,108 +0,0 @@
-#DESCRIPTION: Read in and analyse the quality control files for Bam.
-#USAGE: 003_checkQcBam.R <dir>
-#Run with: Rscript lab_utils/next_generation_sequencing/004_bamProcessing/003_R_node_checkQcBam.R dirname > output.log 2>&1
-args <- commandArgs(trailingOnly = TRUE)
-if (length(args) == 0 ) { print("No argument provided. Provide a directory to process") ; q() }
-
-get_current_datetime_string <- function() {
-                  return(format(Sys.time(), "%Y-%m-%d-%H-%M-%S"))
-}
-
-find_the_directory <- function(dirname){
-    data_directory <- paste0(Sys.getenv("HOME"), "/data")
-    contains_dirname <- grepl(dirname, list.dirs(data_directory, recursive = FALSE))
-    directory_path <- list.dirs(data_directory, recursive = FALSE)[contains_dirname]
-    if (length(directory_path) > 0 && dir.exists(directory_path)) { return(directory_path) }
-    else { print(paste0("Directory ", dirname, " not found")) ; return(NULL) }
-}
-
-directory_to_process <- find_the_directory(args[1])
-if (is.null(directory_to_process)) { print("Directory to process is null. Exiting script.") ; q() }
-
-message <- sprintf("Directory to process is %s.", directory_to_process)
-print(message)
-setwd(directory_to_process)
-
-flagstat_file_path <- list.files("./qualityControl", pattern = "bamFlagstat", recursive = FALSE, full.names = TRUE)
-print(flagstat_file_path[1])
-print(length(unlist(strsplit(flagstat_file_path[1], "_"))))
-genome_names <- unique(sapply(flagstat_file_path, function(path) {
-parts <- unlist(strsplit(path, "_"))
-amount_of_underscores <- length(parts)
-parts[length(parts)-1]
-}))
-#print(genome_names)
-print(paste0("Number of files to process ", length(flagstat_file_path)))
-
-#print(sapply(bam_flagstat_filepath, function(path) {
-#parts <- unlist(strsplit(path, "_"))
-#parts[3]  # Assuming the genome name is always the fourth element
-#}))
-
-#print(sample_info)
-#print(sample_info$short_name)
-#print(seq_along(sample_info$short_name))
-pattern_for_subset <- c("total.*reads", "^mapped$")
-
-sample_info <- read.table(list.files("./documentation", pattern = "sample_info", recursive = FALSE, full.names = TRUE), sep = ",", header = TRUE)
-#percent_mapped_df <- data.frame(matrix(ncol = length(genome_names), nrow = nrow(sample_info)))
-
-percent_mapped_df <- data.frame(matrix(ncol = length(genome_names), nrow = 0))
-colnames(percent_mapped_df) <- genome_names 
-#print(percent_mapped_df)
-#print(colnames(sample_info))
-if(any(is.na(sample_info$sample_ID))) {
-    for (sample in sample_info$short_name) {
-    #    print(sample)
-        is_flagstat <- grepl("bamFlagstat", list.files("./qualityControl", pattern = as.character(sample), recursive = FALSE, full.names = TRUE))
-        bam_flagstat_filepath <- list.files("./qualityControl", pattern = as.character(sample), recursive = FALSE, full.names = TRUE)[is_flagstat]
-    #    print(bam_flagstat_filepath)
-        data <- list()
-        for (flagstat_filepath in bam_flagstat_filepath) {
-            flagstat_table <- read.table(flagstat_filepath, sep = "\t")
-    #        print(lapply(pattern_for_subset, function(x) { grepl(x, flagstat_table[,3]) }))
-                
-            is_row_to_subset <- Reduce("|", lapply(pattern_for_subset, function(pattern) { grepl(pattern, flagstat_table[,3]) }))
-            subset_table <- flagstat_table[is_row_to_subset,]
-    #        print(subset_table[2,1]) ; print(subset_table[1,1])
-            percent_mapped <- (as.numeric(subset_table[2,1]) / as.numeric(subset_table[1,1])) * 100
-            data <- append(data, percent_mapped)
-    #        print(percent_mapped)
-    #        print(head(flagstat_table))
-        }
-#        print(as.data.frame(t(unlist(data))))
-        percent_mapped_df_to_append <- as.data.frame(t(unlist(data)))
-        colnames(percent_mapped_df_to_append) <- genome_names
-#        print(percent_mapped_df_to_append)
-        percent_mapped_df <- rbind(percent_mapped_df, percent_mapped_df_to_append)
-    #    print(percent_mapped_df)
-    }
-} else {
-    for (sample in sample_info$sample_ID) {
-    #   print(sample)
-        is_flagstat <- grepl("bamFlagstat", list.files("./qualityControl", pattern = as.character(sample), recursive = FALSE, full.names = TRUE))
-        bam_flagstat_filepath <- list.files("./qualityControl", pattern = as.character(sample), recursive = FALSE, full.names = TRUE)[is_flagstat]
-    #   print(bam_flagstat_filepath)
-        data <- list()
-        for (flagstat_filepath in bam_flagstat_filepath) {
-            flagstat_table <- read.table(flagstat_filepath, sep = "\t")
-    #       print(lapply(pattern_for_subset, function(x) { grepl(x, flagstat_table[,3]) }))
-
-            is_row_to_subset <- Reduce("|", lapply(pattern_for_subset, function(pattern) { grepl(pattern, flagstat_table[,3]) }))
-            subset_table <- flagstat_table[is_row_to_subset,]
-    #       print(subset_table[2,1]) ; print(subset_table[1,1])
-            percent_mapped <- (as.numeric(subset_table[2,1]) / as.numeric(subset_table[1,1])) * 100
-            data <- append(data, percent_mapped)
-    #       print(percent_mapped)
-    #       print(head(flagstat_table))
-        }
-#        print(as.data.frame(t(unlist(data))))
-        percent_mapped_df_to_append <- as.data.frame(t(unlist(data)))
-        colnames(percent_mapped_df_to_append) <- genome_names
-#        print(percent_mapped_df_to_append)
-        percent_mapped_df <- rbind(percent_mapped_df, percent_mapped_df_to_append)
-    #   print(percent_mapped_df)
-    }
-}
-
-print(percent_mapped_df[2:nrow(percent_mapped_df),])
diff --git a/next_generation_sequencing/004_bamProcessing/003_generateBamCoverageFiles.sh b/next_generation_sequencing/004_bamProcessing/003_generateBamCoverageFiles.sh
deleted file mode 100755
index 42f1629..0000000
--- a/next_generation_sequencing/004_bamProcessing/003_generateBamCoverageFiles.sh
+++ /dev/null
@@ -1,70 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#DESCRIPTION: Generate bigwig files that be used for genome track plots.
-#USAGE: Use with slurm wrapper script.
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-# Construct the file names
-OUT_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out"
-ERR_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out"
-
-# Redirect stdout and stderr to the respective files
-exec > "$OUT_FILE" 2> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load python/2.7.13
-module load deeptools 
-
-#INITIALIZE_ARRAY
-mapfile -t BAM_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*S288C.bam" | sort )
-
-echo "NUMBEROFFILESPROCESSED: $(find "${DIR_TO_PROCESS}" -type f -name "*S288C.bam" | wc -l ) "
-#INPUT_OUTPUT
-#LOG
-echo "Starting coverage output"
-
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-TASK_INDEX=$((SLURM_ARRAY_TASK_ID-1))
-echo "Processing ${BAM_PATHS[$TASK_INDEX]}"
-OUTPUT_FILE=${DIR_TO_PROCESS}bigwig/"${timeid}_$(echo ${BAM_PATHS[$TASK_INDEX]%.bam}_indivNorm.bw | awk -F'/' '{print $NF}' )"
-
-echo "Processing ${OUTPUT_FILE}"
-
-bamCoverage -b ${BAM_PATHS[$TASK_INDEX]} -o ${OUTPUT_FILE} \
-    --binSize 10 \
-    --normalizeUsing CPM \
-    --ignoreDuplicates \
-    --minMappingQuality 20
-
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Quality control check completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo "TASK_END"
diff --git a/next_generation_sequencing/004_bamProcessing/004_generateBamCompareFiles.sh b/next_generation_sequencing/004_bamProcessing/004_generateBamCompareFiles.sh
deleted file mode 100755
index 22c14dc..0000000
--- a/next_generation_sequencing/004_bamProcessing/004_generateBamCompareFiles.sh
+++ /dev/null
@@ -1,108 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#DESCRIPTION: Generate bigwig files that be used for genome track plots.
-#USAGE: Use with slurm wrapper script.
-#SETUP
-DIR_TO_PROCESS="$1"
-timeid=$2
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-# Construct the file names
-OUT_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out"
-ERR_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec > "$OUT_FILE" 2> "$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load python/2.7.13
-module load deeptools 
-module load r/4.2.0
-
-#INPUT_OUTPUT
-echo "Starting coverage output"
-echo "COMMAND_OUTPUT_START"
-# R cat command used to return the input and sample paths also returns a NULL. Grep filters it out.
-# Returns two paths, first path is the path to sample bam file. Second path is the path to its respective or backup input file.
-BASE_DIR=$(basename ${DIR_TO_PROCESS})
-readarray -t sample_paths < <(Rscript ~/lab_utils/next_generation_sequencing/004_bamProcessing/determineInputForArrayId.R ${BASE_DIR} ${SLURM_ARRAY_TASK_ID} | grep -v '^NULL$')
-# Check if the sample_paths array is non-empty
-if [ ${#sample_paths[@]} -eq 0 ]; then
-    echo "Error: No output was generated by the R script."
-    exit 1
-fi
-
-# Check each element in the samples array for emptiness
-for sample in "${sample_paths[@]}"; do
-    if [ -z "$sample" ]; then
-        echo "Error: One of the output lines is empty."
-        exit 1
-    fi
-done
-echo "All samples are valid."
-
-SAMPLE=${sample_paths[0]}
-INPUT=${sample_paths[1]}
-echo "Sample path: ${SAMPLE}"
-echo "Input path: ${INPUT}"
-# Add awk statement to process sample and input names. 
-OUTPUT_FILE=${DIR_TO_PROCESS}bigwig/"${timeid}_$(echo ${SAMPLE%.bam} | awk -F'/' '{print $NF}' | awk -F'_' '{print $1}' )_$(echo ${INPUT%.bam} | awk -F'/' '{print $NF}' | awk -F'_' '{print $1}')_bamcomp.bw"
-echo "Output file: ${OUTPUT_FILE}"
-HALF_CPU=$((SLURM_CPUS_PER_TASK/2))
-echo "Using ${HALF_CPU} CPUS"
-START_TIME=$(date +%s)
-bamCompare -b1 ${SAMPLE} -b2 ${INPUT} \
-    -o ${OUTPUT_FILE} \
-    --binSize 10 \
-    --normalizeUsing CPM \
-    --scaleFactorsMethod readCount \
-    --effectiveGenomeSize 12157105 \
-    --ignoreDuplicates \
-    --minMappingQuality 20 \
-    --operation ratio \
-    --ignoreForNormalization chrXII \
-    --numberOfProcessors ${HALF_CPU}
-END_TIME=$(date +%s)
-#bamCompare -b1 ${SAMPLE} -b2 ${INPUT} \
-#    -o ${OUTPUT_FILE} \
-#    --binSize 10 \
-#    --normalizeUsing CPM \
-#    --scaleFactorsMethod SES \
-#    --effectiveGenomeSize 12157105 \
-#    --ignoreDuplicates \
-#    --minMappingQuality 30 \
-#    --operation log2 \
-#    --pseudocount 1 \
-#    --ignoreForNormalization chrXII
-#    --numberOfProcessors ${SLURM_CPUS_PER_TASK}/2 
-#${SLURM_CPUS_PER_TASK}
-#--blackListFileName yeast_blacklist.bed \
-#--extendReads 150 \
-DURATION=$((END_TIME - START_TIME))
-echo "COMMAND_OUTPUT_END"
-echo "bamCompare Command took $DURATION seconds to execute"
-echo "Quality control check completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo "TASK_END"
diff --git a/next_generation_sequencing/004_bamProcessing/determineInputForAllSamplesForDeeptools.R b/next_generation_sequencing/004_bamProcessing/determineInputForAllSamplesForDeeptools.R
deleted file mode 100644
index 958f033..0000000
--- a/next_generation_sequencing/004_bamProcessing/determineInputForAllSamplesForDeeptools.R
+++ /dev/null
@@ -1,279 +0,0 @@
-#DESCRIPTION: 
-#USAGE:
-#TODO: Need some way to aggregate the column names I use from the sampleConfig.R files to keep track of variables I use to keep consistent experiment to experiment.
-main <- function() {
-    #Load packages
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(Rsamtools)
-    })
-    args <- commandArgs(trailingOnly = TRUE)
-    directory_path <- validate_input(args)
-    #Add process_control_factors, get_factors_to_match
-    sample_table <- load_sample_table(directory_path)
-    sample_and_input <- determine_input_for_all_samples(sample_table, directory_path)
-    system("deeptools --version")
-                        
-    return(sample_and_input)
-    #determineInput
-    #chromosome_to_plot = 10
-    #options(ucscChromosomeNames=FALSE)
-    #refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-    #genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-    #feature_file_pattern = "eaton_peaks"
-    #feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-    # Convert to input, add determine_matching_control, select_control_index
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-                #                             genomeRange_to_get = genomeRange_to_get)
-    #plot_all_sample_tracks(sample_table = sample_table,
-    #                       directory_name = directory_path,
-    #                       chromosome_to_plot = chromosome_to_plot, 
-    #                       genomeRange_to_get = genomeRange_to_get, 
-    #                       control_track = control_track, 
-    #                       annotation_track = origin_track)
-}
-
-validate_input <- function(args) {
-    if (length(args) != 1) {
-        cat("Error: Invalid number of arguments.\n", file = stderr())
-        cat("Usage: Rscript 001_plotAllSampleTracks.R <directory_path>\n", file = stderr())
-        cat("Example: Rscript 001_plotAllSampleTracks.R 240819Bel\n", file = stderr())
-        stop()
-    }
-    directory_path <- file.path(Sys.getenv("HOME"), "data", args[1])
-    if(!dir.exists(directory_path)) {
-        cat(sprintf("Error: Directory %s does not exist.\n", directory_path), file = stderr())
-        stop()
-    }
-    return(directory_path)
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n", file = stderr())
-    df <- sample_table
-    cf_cols <- grep("X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing __cf_ tag found in sample table", file = stderr())
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table, attribute_to_get = "control_factors") {
-    cat("Grabbing attributes from sample table\n", file = stderr())
-    df <- sample_table
-    control_factors <- attr(df, attribute_to_get)
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.\nVerify 003_updateSampleGrid.R")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n", file = stderr())
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, sample_table, bam_directory, reference_genome_pattern = "S288C", max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n", file = stderr())
-    if (length(control_indices) == 0) {
-        warning("No matching control found")
-        cat("Setting control_index to 1\n", file = stderr())
-        control_indices <- 1
-    }
-    if (length(control_indices) > max_controls) {
-        warning(paste("Multiple matching controls found, using first", max_controls))
-        control_indices <- control_indices[1:max_controls]
-    }  
-    control_pattern <- paste0(".*", as.character(sample_table$sample_ID[control_indices]), ".*\\.bam$")
-    all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-    is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-    S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-    if (!file.exists(S288C_bam_file_for_control)) {
-        cat(sprintf("File %s does not exist.\nDetermining first input sample available.", S288C_bam_file_for_control), file = stderr())
-        input_samples <- sample_table[sample_table$antibody == "Input", ]
-        for (sample_index in 1:nrow(input_samples)){
-            control_pattern <- paste0(".*", as.character(sample_table$sample_ID[sample_index]), ".*\\.bam$")
-            all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-            is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-            S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-            # Return the index for the first input sample that has bam file.
-            if(file.exists(S288C_bam_file_for_control)){
-                return(sample_index)
-            }
-        }
-    } else {
-        return(control_indices)
-    }
-}
-
-load_sample_table <- function(directory_name) {
-    cat("Loading sample_table from", directory_name, "\n", file = stderr())
-    documentation_dir_path <- file.path(directory_name, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path), file = stderr())
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n", file = stderr())
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path), file = stderr())
-        cat(sprintf("Files found in %s\n", documentation_dir_path), file = stderr())
-        cat(sample_table_path, file = stderr())
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n", file = stderr())
-        stop()
-    }
-    cat(sprintf("Reading %s\n", sample_table_path), file = stderr())
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    if (!("sample_ID" %in% colnames(sample_table))) {
-           cat("Sample table does not contain the sample_ID column.\n", file = stderr())
-           cat("sample_ID column is required for analysis.\n", file = stderr())
-           cat("See 000_setupExperimentDir.R and 003_updateSampleGrid.R.\n", file = stderr())
-           stop()
-    }
-    sample_table <- process_control_factors(sample_table)
-    cat("Head of sample_table\n", file = stderr())
-    #print(head(sample_table))
-    return(sample_table)
-}
-
-#unique_labeling <- function(table, categories_for_label) {
-#    # Input validation
-#    if (!is.data.frame(table)) {
-#        stop("Input 'table' must be a data frame")
-#    }
-#    if (!is.character(categories_for_label) || length(categories_for_label) == 0) {
-#        stop("Input 'categories_for_label' must be a non-empty character vector")
-#    }
-#    
-#    # Ensure antibody category is always included
-#    if (!"antibody" %in% categories_for_label) {
-#    categories_for_label <- c("antibody", categories_for_label)
-#    }
-#    
-#    print(paste("Categories for label:", paste(categories_for_label, collapse = ", ")))
-#    
-#    # Check if all categories exist in the table
-#    missing_categories <- setdiff(categories_for_label, colnames(table))
-#    if (length(missing_categories) > 0) {
-#        stop(paste("The following categories are missing from the table:", 
-#        paste(missing_categories, collapse = ", ")))
-#    }
-#    
-#    # Identify unique values for each category
-#    unique_values <- lapply(table[categories_for_label], unique)
-#    print("Unique values for each category:")
-#    print(unique_values)
-#    
-#    # Function to construct label for a single sample
-#    construct_label <- function(sample) {
-#    differing_categories <- sapply(categories_for_label, function(cat) {
-#        if (length(unique_values[[cat]]) > 1 || cat == "antibody") {
-#            return(sample[cat])
-#            #return(paste(cat, sample[cat], sep = ": "))
-#        } else {
-#            return(NULL)
-#        }
-#    })
-#        differing_categories <- differing_categories[!sapply(differing_categories, is.null)]
-#        return(paste(differing_categories, collapse = "_"))
-#    }
-#    
-#    # Apply the construct_label function to each sample (row)
-#    labels <- apply(table, 1, construct_label)
-#    
-#    print("Constructed labels:")
-#    print(labels)
-#    
-#    return(unlist(labels))
-#}
-#determineInput <-  function(sample_row){}
-#TODO: Must pass sample index to the function after I figure out logic
-#TODO: Determine best way to return the two files.
-#TODO: Add check to make sure alignment for input and sample exists.
-determine_input_for_all_samples <- function(sample_table, directory_path, reference_genome_pattern = "S288C"){
-    bigwig_directory <- file.path(directory_path, "bigwig")
-    bam_directory <- file.path(directory_path, "alignment")
-    factors_to_match <- get_factors_to_match(sample_table)
-    #print(factors_to_match)
-    #control_index <- determine_matching_control(sample_table[sample_index,], sample_table, factors_to_match)
-    for (sample_index in 1:nrow(sample_table)) {
-        cat("==========\n", file = stderr())
-        control_index <- determine_matching_control(sample_table[sample_index,], sample_table, factors_to_match)
-        control_index <- select_control_index(control_index, sample_table = sample_table, bam_directory = bam_directory)
-        #print(control_index)
-        #print(sample_index) 
-        control_pattern <- paste0(".*", as.character(sample_table$sample_ID[control_index]), ".*\\.bam$")
-        all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-        is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-        S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-
-        sample_pattern <- paste0(".*", as.character(sample_table$sample_ID[sample_index]), ".*\\.bam$")
-        all_bam_files_for_sample <- list.files(bam_directory, pattern = sample_pattern, full.names = TRUE)
-        is_S288C_bam_file_for_sample <- grepl(reference_genome_pattern, all_bam_files_for_sample)
-        S288C_bam_file_for_sample <- all_bam_files_for_sample[is_S288C_bam_file_for_sample]
-        #ensure_control_bam_exists
-        #ensure_sample_bam_exists
-        if(!file.exists(S288C_bam_file_for_sample)){
-            cat("Sample file does not exist. \n", file = stderr())
-            cat(sprintf("Sample_ID: %s. \n", sample_table$sample_ID[sample_index]), file = stderr())
-            cat(sprintf("Short Name: %s. \n", sample_table$short_name[sample_index]), file = stderr())
-            sample_file_exists <- FALSE
-        } else {
-            sample_file_exists <- TRUE
-        }
-        cat(sprintf("Control file: %s\nSample file: %s\nFile exists: %s\n", S288C_bam_file_for_control, S288C_bam_file_for_sample, sample_file_exists), file = stderr())
-        if(sample_file_exists & control_index > 0) {
-            #cat(S288C_bam_file_for_sample, "\n", S288C_bam_file_for_sample)
-            return(cat(S288C_bam_file_for_sample, S288C_bam_file_for_sample, sep = "\n"))
-        } else {
-            cat(sprintf("Sample %s:\n", sample_table$short_name[sample_index]), file = stderr())
-        }
-    }
-}
-
-if(!interactive()){
-    main()
-} else {
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(gtools)
-    })
-    #@update
-    directory_path <- "240819Bel"
-    cat("Logic of main function\n")
-    main_function_logic <- main
-    list_of_functions_and_variables <- ls()
-    cat("Main function: Validating directory\n")
-    directory_path <- validate_input(directory_path)
-    cat("Main function: loading sample table\n")
-    sample_table <- load_sample_table(directory_path)
-    #print(directory_path)
-    determine_input_for_all_samples(sample_table, directory_path)
-}
diff --git a/next_generation_sequencing/004_bamProcessing/determineInputForArrayId.R b/next_generation_sequencing/004_bamProcessing/determineInputForArrayId.R
deleted file mode 100644
index 4fa6ad1..0000000
--- a/next_generation_sequencing/004_bamProcessing/determineInputForArrayId.R
+++ /dev/null
@@ -1,211 +0,0 @@
-#DESCRIPTION: 
-#USAGE:
-#TODO: Need some way to aggregate the column names I use from the sampleConfig.R files to keep track of variables I use to keep consistent experiment to experiment.
-main <- function() {
-    args <- commandArgs(trailingOnly = TRUE)
-    argument_list <- validate_input(args)
-    #Load packages
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(Rsamtools)
-    })
-    directory_path <- argument_list$directory_path
-    slurm_array_task_id <- argument_list$slurm_array_task_id
-    #Add process_control_factors, get_factors_to_match
-    sample_table <- load_sample_table(directory_path)
-    sample_and_input <- determine_input_for_sample(sample_table, directory_path, slurm_array_task_id)
-    return(sample_and_input)
-}
-
-validate_input <- function(args) {
-    if (length(args) != 2) {
-        cat("Error: Invalid number of arguments.\n", file = stderr())
-        cat("Usage: Rscript 001_plotAllSampleTracks.R <directory_path> <SLURM_ARRAY_TASK_ID>\n", file = stderr())
-        cat("Example: Rscript 001_plotAllSampleTracks.R 240819Bel 1\n", file = stderr())
-        stop()
-    }
-    # Must add verification of args[1] to ensure that it isnt provided as absolute path or with trailing slash
-    # However, script 004_generateNormalizedCoverageFiles.sh takes care of the proper format using basename.
-    directory_path <- file.path(Sys.getenv("HOME"), "data", args[1])
-    if(!dir.exists(directory_path)) {
-        cat(sprintf("Error: Directory %s does not exist.\n", directory_path), file = stderr())
-        stop()
-    }
-    slurm_array_task_id <- as.numeric(args[2])
-    if(!(slurm_array_task_id >= 1)){
-        cat("Error: Slurm array task id must be numeric and larger than 1.\n", file = stderr())
-        stop()
-    }
-    return(list(
-        directory_path = directory_path,
-        slurm_array_task_id = slurm_array_task_id
-    ))
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n", file = stderr())
-    df <- sample_table
-    cf_cols <- grep("X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing __cf_ tag found in sample table", file = stderr())
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table, attribute_to_get = "control_factors") {
-    cat("Grabbing attributes from sample table\n", file = stderr())
-    df <- sample_table
-    control_factors <- attr(df, attribute_to_get)
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.\nVerify 003_updateSampleGrid.R")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n", file = stderr())
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, sample_table, bam_directory, reference_genome_pattern = "S288C", max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n", file = stderr())
-    if (length(control_indices) == 0) {
-        warning("No matching control found")
-        cat("Setting control_index to 1\n", file = stderr())
-        control_indices <- 1
-    }
-    if (length(control_indices) > max_controls) {
-        warning(paste("Multiple matching controls found, using first", max_controls))
-        control_indices <- control_indices[1:max_controls]
-    }  
-    control_pattern <- paste0(".*", as.character(sample_table$sample_ID[control_indices]), ".*\\.bam$")
-    all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-    is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-    S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-    if (!file.exists(S288C_bam_file_for_control)) {
-        cat(sprintf("File %s does not exist.\nDetermining first input sample available.", S288C_bam_file_for_control), file = stderr())
-        input_samples <- sample_table[sample_table$antibody == "Input", ]
-        for (input_index in 1:nrow(input_samples)){
-            control_pattern <- paste0(".*", as.character(sample_table$sample_ID[input_index]), ".*\\.bam$")
-            all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-            is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-            S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-            # Return the index for the first input sample that has bam file.
-            if(file.exists(S288C_bam_file_for_control)){
-                return(input_index)
-            }
-        }
-    } else {
-        return(control_indices)
-    }
-}
-
-load_sample_table <- function(directory_name) {
-    cat("Loading sample_table from", directory_name, "\n", file = stderr())
-    documentation_dir_path <- file.path(directory_name, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path), file = stderr())
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n", file = stderr())
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path), file = stderr())
-        cat(sprintf("Files found in %s\n", documentation_dir_path), file = stderr())
-        cat(sample_table_path, file = stderr())
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n", file = stderr())
-        stop()
-    }
-    cat(sprintf("Reading %s\n", sample_table_path), file = stderr())
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    if (!("sample_ID" %in% colnames(sample_table))) {
-           cat("Sample table does not contain the sample_ID column.\n", file = stderr())
-           cat("sample_ID column is required for analysis.\n", file = stderr())
-           cat("See 000_setupExperimentDir.R and 003_updateSampleGrid.R.\n", file = stderr())
-           stop()
-    }
-    sample_table <- process_control_factors(sample_table)
-    #cat("Head of sample_table\n", file = stderr())
-    #cat(head(sample_table), file = stderr())
-    return(sample_table)
-}
-
-determine_input_for_sample<- function(sample_table, directory_path, slurm_array_task_id, reference_genome_pattern = "S288C"){
-    bigwig_directory <- file.path(directory_path, "bigwig")
-    bam_directory <- file.path(directory_path, "alignment")
-    factors_to_match <- get_factors_to_match(sample_table)
-    cat("==========\n", file = stderr())
-    control_index <- determine_matching_control(sample_table[slurm_array_task_id,], sample_table, factors_to_match)
-    control_index <- select_control_index(control_index, sample_table = sample_table, bam_directory = bam_directory)
-    control_pattern <- paste0(".*", as.character(sample_table$sample_ID[control_index]), ".*\\.bam$")
-    all_bam_files_for_control <- list.files(bam_directory, pattern = control_pattern, full.names = TRUE)
-    is_S288C_bam_file_for_control <- grepl(reference_genome_pattern, all_bam_files_for_control)
-    S288C_bam_file_for_control <- all_bam_files_for_control[is_S288C_bam_file_for_control]
-
-    sample_pattern <- paste0(".*", as.character(sample_table$sample_ID[slurm_array_task_id]), ".*\\.bam$")
-    all_bam_files_for_sample <- list.files(bam_directory, pattern = sample_pattern, full.names = TRUE)
-    is_S288C_bam_file_for_sample <- grepl(reference_genome_pattern, all_bam_files_for_sample)
-    S288C_bam_file_for_sample <- all_bam_files_for_sample[is_S288C_bam_file_for_sample]
-    if(!file.exists(S288C_bam_file_for_sample)){
-        cat("Sample file does not exist. \n", file = stderr())
-        cat(sprintf("Sample_ID: %s. \n", sample_table$sample_ID[slurm_array_task_id]), file = stderr())
-        cat(sprintf("Short Name: %s. \n", sample_table$short_name[slurm_array_task_id]), file = stderr())
-        sample_file_exists <- FALSE
-    } else {
-        sample_file_exists <- TRUE
-    }
-    cat(sprintf("Control file: %s\nSample file: %s\nFile exists: %s\n", S288C_bam_file_for_control, S288C_bam_file_for_sample, sample_file_exists), file = stderr())
-    if(sample_file_exists & control_index > 0) {
-        cat("==========\n", file = stderr())
-        cat("Returning files\n", file = stderr())
-        cat(S288C_bam_file_for_sample, S288C_bam_file_for_control, sep = "\n", file = stderr())
-        cat("==========\n", file = stderr())
-        #return(c(S288C_bam_file_for_sample, S288C_bam_file_for_control))
-        #This outputs an extra NULL value at then due to how cat works. Must be processed out using GREP, SED, etc.
-        return(cat(S288C_bam_file_for_sample, S288C_bam_file_for_control, sep = "\n"))
-    } else {
-        cat(sprintf("Sample %s does not exists or control input files dont exists.\n", sample_table$short_name[slurm_array_task_id]), file = stderr())
-    }
-}
-
-if(!interactive()){
-    main()
-} else {
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(gtools)
-    })
-    ##@update
-    #directory_path <- "240819Bel"
-    #cat("Logic of main function\n")
-    #main_function_logic <- main
-    #list_of_functions_and_variables <- ls()
-    #cat("Main function: Validating directory\n")
-    #directory_path <- validate_input(directory_path)
-    #cat("Main function: loading sample table\n")
-    #sample_table <- load_sample_table(directory_path)
-    ##print(directory_path)
-    #determine_input_for_sample(sample_table, directory_path, slurm_array_task_id)
-}
diff --git a/next_generation_sequencing/004_bamProcessing/tests/001_test_qualityControlBam.sh b/next_generation_sequencing/004_bamProcessing/tests/001_test_qualityControlBam.sh
deleted file mode 100755
index 4478611..0000000
--- a/next_generation_sequencing/004_bamProcessing/tests/001_test_qualityControlBam.sh
+++ /dev/null
@@ -1,62 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#USAGE: First, determine this by running the INITIALIZE_ARRAY and multiplying by number of genomes, modify the array number. For test, leave at 1-2 to test array creation. Then, from anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/slurm_002_alignFastq.sh <dir>'
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >"$OUT_FILE" 2>"$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load samtools/1.10
-module load fastqc/0.11.5
-
-#INITIALIZE_ARRAY
-mapfile -t BAM_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.bam" )
-
-FILENAME=$( echo ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]%.bam} | awk -F'/' '{print $NF}' )
-
-#LOG
-
-echo "Starting quality control check"
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
- 
-echo "samtools flagstat -O tsv ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamFlagstat.txt"
-echo "{ samtools quickcheck ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} && echo 'QUICKCHECK\tTRUE' || echo -e 'QUICKCHECK\tFALSE' ; } > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamQuickcheck.txt "
-echo "samtools stats ${BAM_PATHS[$SLURM_ARRAY_TASK_ID-1]} > ${DIR_TO_PROCESS}qualityControl/${FILENAME}_bamStats.txt"
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Quality control check completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END
-"
diff --git a/next_generation_sequencing/004_bamProcessing/tests/002_test_generateCoverageFiles.sh b/next_generation_sequencing/004_bamProcessing/tests/002_test_generateCoverageFiles.sh
deleted file mode 100755
index e465fae..0000000
--- a/next_generation_sequencing/004_bamProcessing/tests/002_test_generateCoverageFiles.sh
+++ /dev/null
@@ -1,66 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1 # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1 # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE.
-#SBATCH --exclude=c[5-22] #Required by MIT
-#SBATCH --mem-per-cpu=50G # amount of RAM per node
-#SBATCH --cpus-per-task=4
-#SBATCH --nice=10000 #Required by MIT
-#USAGE: First, determine this by running the INITIALIZE_ARRAY and multiplying by number of genomes, modify the array number. For test, leave at 1-2 to test array creation. Then, from anywhere, run 'sbatch ~/data/lab_utils/next_generation_sequencing/slurm_002_alignFastq.sh <dir>'
-#SETUP
-DIR_TO_PROCESS="$1"
-
-# Define the log directory
-LOG_DIR="$HOME/data/$DIR_TO_PROCESS/logs"
-
-# Ensure the log directory exists
-mkdir -p "$LOG_DIR"
-timeid=$(date "+%Y-%m-%d-%M-%S")
-# Construct the file names
-OUT_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}.out"
-ERR_FILE="${LOG_DIR}/${timeid}_qualityControl_${SLURM_ARRAY_JOB_ID}.err"
-
-# Redirect stdout and stderr to the respective files
-exec >"$OUT_FILE" 2>"$ERR_FILE"
-echo "TASK_START"
-
-#LOG
-echo "SLURM_JOB_ID=${SLURM_JOB_ID}, SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}, SLURM_ARRAY_TASK_ID=${SLURM_ARRAY_TASK_ID}"
-
-echo "Started from $(pwd)"
-echo "START TIME: $(date "+%Y-%m-%d-%M-%S")"
-DIR_TO_PROCESS="$HOME/data/$DIR_TO_PROCESS"
-echo "Executing for $DIR_TO_PROCESS"
-REFGENOME_DIR="$HOME/data/REFGENS"
-
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load python/2.7.13
-module load deeptools 
-
-#INITIALIZE_ARRAY
-mapfile -t BAM_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.bam" )
-
-echo "NUMBEROFFILESPROCESSED: $(find "${DIR_TO_PROCESS}" -type f -name "*.bam" | wc -l ) "
-#INPUT_OUTPUT
-#fastq_path=${FASTQ_PATHS[$SLURM_ARRAY_TASK_ID-1]}
-#output_path=$(echo "$fastq_path" | cut -d/ -f7 | xargs -I {} echo "${DIR_TO_PROCESS}processed-fastq/processed_{}")
-
-#LOG
-
-echo "Starting coverage output"
-
-#COMMAND_TO_EXECUTE 
-echo "COMMAND_OUTPUT_START"
-OUTPUT_FILE=${DIR_TO_PROCESS}bigwig/"$(echo ${BAM_PATHS[$SLURM_ARRAY_TASK_ID]%.bam}.bw | awk -F'/' '{print $NF}')"
-
-echo "bamCoverage -b ${BAM_PATHS[$SLURM_ARRAY_TASK_ID]} -o ${OUTPUT_FILE} --binSize 10 --normalizeUsing RPKM"
-
-#LOG
-echo "COMMAND_OUTPUT_END"
-echo "Quality control check completed"
-echo "END TIME: $(date "+%Y-%m-%d-%M-%S")"
-echo -e "TASK_END
-"
diff --git a/next_generation_sequencing/005_genomeTracks/001_plotAllSampleTracksWithInputAndEaton.R b/next_generation_sequencing/005_genomeTracks/001_plotAllSampleTracksWithInputAndEaton.R
deleted file mode 100755
index a39f9dc..0000000
--- a/next_generation_sequencing/005_genomeTracks/001_plotAllSampleTracksWithInputAndEaton.R
+++ /dev/null
@@ -1,384 +0,0 @@
-#DESCRIPTION: 
-#USAGE:
-#TODO: Need some way to aggregate the column names I use from the sampleConfig.R files to keep track of variables I use to keep consistent experiment to experiment.
-#TODO figure out if there is a way to normalize the samples 
-##TODO: Define the comparisons and plots to be generated in my sampleConfig.R template. 
-##TODO: Find the best way to have the same levels and factors when I read in the sampleGridConfig.R file. This is defined by the categories list variable. Can grab that during 002_loadSampleGrid and use it to equalize. 
-##TODO: Use 002_loadSampleGrid to open the sample_table given a directory. Use conditional statement to determine the ID. Could potentially use system function to call find and print with awk statement. R would require list.files(), strsplit, and grabbing regular expression for 5 digits.
-#Load packages using through a list of strings and suppress the messages, return a TRUE if loading was succesful
-##TODO: Need to make sure the chromosome IDs are formatted properly.
-##TODO: need to automate the creation of the experiments to plot. 
-##TODO: Need to ensure that the names I use in the columns are compatible with my short name convention for subsetting df
-main <- function() {
-    #Load packages
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-    })
-    args <- commandArgs(trailingOnly = TRUE)
-    directory_path <- validate_input(args)
-    #Add process_control_factors, get_factors_to_match
-    sample_table <- load_sample_table(directory_path)
-    chromosome_to_plot = 10
-    options(ucscChromosomeNames=FALSE)
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-
-    #feature_file_pattern = "eaton_peaks"
-    #feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-
-    # Convert to input, add determine_matching_control, select_control_index
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-                #                             genomeRange_to_get = genomeRange_to_get)
-
-    #plot_all_sample_tracks(sample_table = sample_table,
-    #                       directory_name = directory_path,
-    #                       chromosome_to_plot = chromosome_to_plot, 
-    #                       genomeRange_to_get = genomeRange_to_get, 
-    #                       control_track = control_track, 
-    #                       annotation_track = origin_track)
-
-}
-
-validate_input <- function(args) {
-    if (length(args) != 1) {
-        cat("Error: Invalid number of arguments.\n")
-        cat("Usage: Rscript 001_plotAllSampleTracks.R <directory_path>\n")
-        cat("Example: Rscript 001_plotAllSampleTracks.R 240819Bel\n")
-        stop()
-    }
-    directory_path <- file.path(Sys.getenv("HOME"), "data", args[1])
-    if(!dir.exists(directory_path)) {
-        cat(sprintf("Error: Directory %s does not exist.\n", directory_path))
-        stop()
-    }
-    return(directory_path)
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n")
-    df <- sample_table
-    cf_cols <- grep("X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing __cf_ tag found in sample table")
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table, attribute_to_get = "control_factors") {
-    cat("Grabbing attributes from sample table\n")
-    df <- sample_table
-    control_factors <- attr(df, attribute_to_get)
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.\nVerify 003_updateSampleGrid.R")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n")
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n")
-    if (length(control_indices) == 0) {
-        warning("No matching control found")
-        cat("Setting control_index to 1\n")
-        control_indices <- 1
-    }
-    if (length(control_indices) > max_controls) {
-    warning(paste("Multiple matching controls found, using first", max_controls))
-    control_indices[1:max_controls]
-    } else if (length(control_indices) == 1){
-        return(control_indices)
-    }
-}
-
-load_sample_table <- function(directory_name) {
-    cat("Loading sample_table from", directory_name, "\n")
-    documentation_dir_path <- file.path(directory_name, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n")
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat(sprintf("Files found in %s\n", documentation_dir_path))
-        print(sample_table_path)
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n")
-        stop()
-    }
-    cat(sprintf("Reading %s\n", sample_table_path))
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    if (!("sample_ID" %in% colnames(sample_table))) {
-           cat("Sample table does not contain the sample_ID column.\n")
-           cat("sample_ID column is required for analysis.\n")
-           cat("See 000_setupExperimentDir.R and 003_updateSampleGrid.R.\n")
-           stop()
-    }
-    sample_table <- process_control_factors(sample_table)
-    cat("Head of sample_table\n")
-    print(head(sample_table))
-    return(sample_table)
-}
-
-load_reference_genome <- function(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna") {
-    cat("Loading reference genome\n")
-    directory_of_refgenomes <- file.path(Sys.getenv("HOME"), "data", genome_dir)
-    if(!dir.exists(directory_of_refgenomes)) {
-        stop("Directory with reference genomes doesnt exist.\n")
-    }
-    genome_file_path <- list.files(directory_of_refgenomes, pattern = genome_pattern, full.names = TRUE, recursive = TRUE)
-    if (length(genome_file_path) > 1) {
-        cat(sprintf("More than one file matched genome pattern %s", genome_pattern))
-        print(genome_file_path)
-        stop()
-    }
-    if(!file.exists(genome_file_path)) {
-        stop("Reference genome doesnt exist.\n")
-    }
-    refGenome <- readFasta(genome_file_path)
-    refGenome <- data.frame(chrom = names(as(refGenome, "DNAStringSet")), 
-                    basePairSize = width(refGenome)) %>% filter(chrom != "chrM")
-    cat("Head of refGenome.\n")
-    print(head(refGenome))
-    return(refGenome)
-}
-
-#Create GRanges object to read in a particular chromosome
-create_chromosome_GRange <- function(refGenome) {
-    cat("Creating chromosome GRange for loading feature, samples, etc\n")
-    genomeRange_to_get <- GRanges(seqnames = refGenome$chrom,
-                                  ranges = IRanges(start = 1, 
-                                                   end = refGenome$basePairSize),
-                                  strand = "*")
-    cat("Head of Genome Range for loading other files.\n")
-    print(head(genomeRange_to_get))
-    return(genomeRange_to_get)
-}
-# Chromosome mapping functions
-chr_to_roman <- c(
-  "1" = "I", "2" = "II", "3" = "III", "4" = "IV", "5" = "V", "6" = "VI", "7" = "VII", "8" = "VIII",
-  "9" = "IX", "10" = "X", "11" = "XI", "12" = "XII", "13" = "XIII", "14" = "XIV", "15" = "XV", "16" = "XVI"
-)
-
-roman_to_chr <- setNames(names(chr_to_roman), chr_to_roman)
-
-normalize_chr_names <- function(chr_names, target_style) {
-  chr_names <- gsub("^chr", "", chr_names)
-  normalized_chr_name <- switch(target_style,
-    "UCSC" = paste0("chr", chr_names),
-    "Roman" = sapply(chr_names, function(x) paste0("chr", ifelse(x %in% names(chr_to_roman), chr_to_roman[x], x))),
-    "Numeric" = sapply(chr_names, function(x) ifelse(x %in% chr_to_roman, roman_to_chr[x], x)),
-    stop("Unknown target style")
-    )
-    cat("Structure of normalized_chr_name\n")
-    print(str(normalized_chr_name))
-    return(unname(normalized_chr_name)) 
-}
-
-determine_chr_style <- function(chr_names) {
-  if (all(grepl("^chr[0-9]+$", chr_names))) return("UCSC")
-  if (all(grepl("^chr[IVX]+$", chr_names))) return("Roman")
-  if (all(grepl("^[0-9]+$", chr_names))) return("Numeric")
-  return("Unknown")
-}
-
-load_feature_file_GRange <- function(chromosome_to_plot = 10, feature_file_pattern = "eaton_peaks", genomeRange_to_get) {
-  cat(sprintf("Loading %s feature file.\n", feature_file_pattern))
-  # Input validation
-  feature_file_dir <- file.path(Sys.getenv("HOME"), "data", "feature_files")
-  if(!dir.exists(feature_file_dir)) {
-    stop(sprintf("Directory %s does not exist.", feature_file_dir))
-  }
-  feature_file_path <- list.files(feature_file_dir, pattern = feature_file_pattern, full.names = TRUE, recursive = TRUE)
-  if(length(feature_file_path) != 1) {
-    stop(sprintf("Error finding feature file. Found %d files: %s", length(feature_file_path), paste(feature_file_path, collapse = ", ")))
-  }
-  # Load feature file and determine its style
-  feature_grange <- import.bed(feature_file_path)
-  feature_style <- determine_chr_style(seqlevels(feature_grange))
-  cat("Feature file chromosome style:", feature_style, "\n")
-  # Determine genomeRange style
-  genome_style <- determine_chr_style(seqlevels(genomeRange_to_get))
-  cat("Genome range chromosome style:", genome_style, "\n")
-
-  if (feature_style == genome_style) {
-    # Styles match, use genomeRange_to_get as is
-    cat("Styles match. Using provided genome range.\n")
-    feature_grange_subset <- subsetByOverlaps(feature_grange, genomeRange_to_get)
-  } else {
-    # Styles don't match, adjust genomeRange_to_get
-    cat("Styles don't match. Adjusting genome range to match feature file.\n")
-    adjusted_genomeRange <- genomeRange_to_get
-    new_seqlevels <- normalize_chr_names(seqlevels(genomeRange_to_get), feature_style)
-    seqlevels(adjusted_genomeRange) <- new_seqlevels
-
-    feature_grange_subset <- subsetByOverlaps(feature_grange, adjusted_genomeRange)
-
-    new_seqlevels <- normalize_chr_names(seqlevels(feature_grange_subset), genome_style)
-    seqlevels(feature_grange_subset) <- new_seqlevels
-    cat(sprintf("Confirming Feature GRange file style: %s\n", determine_chr_style(seqlevels(feature_grange_subset))))
-
-  }
-  return(feature_grange_subset)
-}
-
-load_control_grange_data <- function(control_dir, file_identifier, chromosome_to_plot = 10, genomeRange_to_get) {
-    cat("Loading control track data from", control_dir, "\n")
-    bigwig_dir_path <- file.path(Sys.getenv("HOME"), "data", control_dir, "bigwig")
-    bigwig_file_paths <- list.files(bigwig_dir_path, pattern = file_identifier, full.names = TRUE, recursive = TRUE) 
-    S288C_bigwigs <- grepl("S288C", bigwig_file_paths)
-    bigwig_file_path <- bigwig_file_paths[S288C_bigwigs]
-    if(!file.exists(bigwig_file_path)) {
-        cat(sprintf("File %s doesnt exist.\n", bigwig_file_path))
-        stop()
-    }
-    control_style <- determine_chr_style(seqlevels(import(bigwig_file_path)))
-    chromosome_to_subset <- normalize_chr_names(chromosome_to_plot, control_style)
-    subset_genome_range <- genomeRange_to_get[seqnames(genomeRange_to_get) == chromosome_to_subset]
-    control_grange <- import(bigwig_file_path, which = genomeRange_to_get)
-    return(control_grange)
-}
-
-plot_all_sample_tracks <- function(sample_table, directory_path, chromosome_to_plot = 10, genomeRange_to_get, control_track, annotation_track, highlight_gr) {
-    main_title_of_plot_track <- paste("Complete View of Chrom", as.character(chromosome_to_plot), sep = " ")
-    date_plot_created <- stringr::str_replace_all(Sys.time(), pattern = ":| |-", replacement="")  
-    factors_to_match <- get_factors_to_match(sample_table)
-    cat("Factors in attributes of sample_table\n")
-    print(factors_to_match)
-    plot_output_dir <- file.path(directory_path, "plots")
-    bigwig_dir <- file.path(directory_path, "bigwig")
-    cat("Plotting all sample tracks.\n")
-    gtrack <- GenomeAxisTrack(name = paste("Chr ", chromosome_to_plot, " Axis", sep = ""))
-    for (sample_index in 1:nrow(sample_table)) {
-            cat("===============\n")
-            sample_ID_pattern <- sample_table$sample_ID[sample_index]
-            initial_matches <- list.files(bigwig_dir, pattern = as.character(sample_ID_pattern), full.names = TRUE, recursive = TRUE)
-            path_to_bigwig <- initial_matches[grepl("S288C", initial_matches)]
-            print("Name of the bigwig path")
-            print(path_to_bigwig)
-            chromosome_as_chr_roman <- paste("chr", as.roman(chromosome_to_plot), sep = "")
-            if (length(path_to_bigwig) == 0){
-                cat(sprintf("No bigwig found for sample_ID: %s\n", sample_ID_pattern))
-                cat("Results of initial matches\n")
-                print(initial_matches)
-            }
-            if (length(path_to_bigwig) > 0){
-                control_index <- determine_matching_control(sample_row = sample_table[sample_index, ], sample_table, factors_to_match = factors_to_match)
-                if(length(control_index) == 0) {
-                    cat("No control index found\n")
-                    cat("Printing sample row\n")
-                    print(sample_table[sample_index, ])
-                }
-                control_index <- select_control_index(control_indices = control_index, max_controls = 1)
-                control_ID_pattern <- sample_table$sample_ID[control_index]
-                control_sample_name <-sample_table$short_name[control_index] 
-                control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                control_path_to_bigwig <- control_initial_matches[grepl("S288C", control_initial_matches)]
-                if(length(control_path_to_bigwig) == 0){
-                    cat("Appropriate control bigwig not found. Setting to first sample.\n")
-                    control_ID_pattern <- sample_table$sample_ID[1]
-                    control_sample_name <-sample_table$short_name[1] 
-                    control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                    control_path_to_bigwig <- control_initial_matches[grepl("S288C", control_initial_matches)]
-                }
-                print("Control ID pattern")
-                print(control_ID_pattern)
-                print("Name of the control bigwig path")
-                print(control_path_to_bigwig)
-                bigwig_to_plot <- import(con = path_to_bigwig, which = genomeRange_to_get)
-                cat("Bigwig plot output\n")
-                head(bigwig_to_plot)
-                sample_short_name <- sample_table$short_name[sample_index]
-                track_to_plot <- DataTrack(bigwig_to_plot, type = "l", name = sample_short_name, col = "#E41A1C", chromosome = chromosome_to_plot)
-                sample_control_bigwig_to_plot <- import(con = control_path_to_bigwig, which = genomeRange_to_get)
-                sample_control_track_to_plot <- DataTrack(sample_control_bigwig_to_plot, type = "l", name = control_sample_name, col = "#377EB8", chromosome = chromosome_to_plot)
-                all_tracks <- list(gtrack, sample_control_track_to_plot, track_to_plot, control_track, annotation_track)
-                output_plot_name <- paste(plot_output_dir, "/", date_plot_created, "_", chromosome_to_plot, "_", sample_short_name, "_", "WithInputAndEaton", ".svg", sep = "")
-                print("Name of the plot to be generated")
-                print(output_plot_name)
-                svg(output_plot_name)
-                plotTracks(all_tracks, 
-                            main = main_title_of_plot_track,
-                            chromosome = chromosome_as_chr_roman,
-                            ylim = c(0, 100000))
-                dev.off()
-                cat("===============\n")
-        
-    }
-    cat("Reached end of for loop\n")
-    cat("Finished plotting samples\n")
-    }
-}
-if(!interactive()){
-    main()
-    cat("rsync -nav username@domain:~/data/<dir>/plots/* /local/dir/<dir>/plots/\n")
-} else {
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(gtools)
-    })
-    #@update
-    directory_path <- "240808Bel"
-    cat("Logic of main function\n")
-    main_function_logic <- main
-    list_of_functions_and_variables <- ls()
-    cat("Main function: Validating directory\n")
-    directory_path <- validate_input(directory_path)
-    chromosome_to_plot = 10
-    #chromosome_to_plot = c(10, paste0("chr", as.roman(10)))
-    cat("Main function: loading sample table\n")
-    sample_table <- load_sample_table(directory_path)
-    print(attr(sample_table, "control_factors"))
-    cat("Main function: loading reference genome\n")
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-    cat("Main function: Creating genomeRange_to_get\n")
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-    # Load peaks feature file for highlight track and annotation. 
-    feature_file_pattern = "eaton_peaks"
-    cat("Main function: load feature grange\n")
-    feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,genomeRange_to_get = genomeRange_to_get)
-    feature_track <- AnnotationTrack(feature_grange, name = paste("Origin Peaks","Eaton 2010", sep = ""))
-    print(head(feature_grange))
-    # Load the Eaton Bel Track data as second comparison. 
-    cat("Main function: loading control grange\n")
-    control_dir <- "EatonBel"
-    file_identifier <- "nnNnH"
-    control_grange <- load_control_grange_data(control_dir = control_dir, file_identifier = file_identifier, chromosome_to_plot = chromosome_to_plot,genomeRange_to_get = genomeRange_to_get)
-    control_track <- DataTrack(control_grange, type = "l",  name = "Eaton 2010", col = "#377EB8")
-    print(head(control_grange))
-    # Plot samples, determine the input control for each sample. No need to modify the files provided then. Just the logic.
-    plot_all_sample_tracks(sample_table = sample_table,directory_path = directory_path,chromosome_to_plot = chromosome_to_plot,genomeRange_to_get = genomeRange_to_get,control_track = control_track,annotation_track = feature_track, highlight_gr = feature_grange)
-}
diff --git a/next_generation_sequencing/005_genomeTracks/002_plotUserDefinedExperiments.R b/next_generation_sequencing/005_genomeTracks/002_plotUserDefinedExperiments.R
deleted file mode 100644
index 1814c38..0000000
--- a/next_generation_sequencing/005_genomeTracks/002_plotUserDefinedExperiments.R
+++ /dev/null
@@ -1,464 +0,0 @@
-#DESCRIPTION: 
-#USAGE:
-#TODO: Need some way to aggregate the column names I use from the sampleConfig.R files to keep track of variables I use to keep consistent experiment to experiment.
-##TODO: Find the best way to have the same levels and factors when I read in the sampleGridConfig.R file. This is defined by the categories list variable. Can grab that during 002_loadSampleGrid and use it to equalize. 
-#Load packages using through a list of strings and suppress the messages, return a TRUE if loading was succesful
-##TODO: Need to ensure that the names I use in the columns are compatible with my short name convention for subsetting df
-
-rm(list = ls())
-main <- function() {
-    #Load packages
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-    })
-    args <- commandArgs(trailingOnly = TRUE)
-    directory_path <- validate_input(args)
-    sample_table <- load_sample_table(directory_path)
-    chromosome_to_plot = 10
-    options(ucscChromosomeNames=FALSE)
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-
-    #feature_file_pattern = "eaton_peaks"
-    #feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-
-
-
-    #plot_all_sample_tracks(sample_table = sample_table,
-    #                       directory_name = directory_path,
-    #                       chromosome_to_plot = chromosome_to_plot, 
-    #                       genomeRange_to_get = genomeRange_to_get, 
-    #                       control_track = control_track, 
-    #                       annotation_track = origin_track)
-
-}
-
-validate_input <- function(args) {
-    if (length(args) != 1) {
-        cat("Error: Invalid number of arguments.\n")
-        cat("Usage: Rscript 001_plotAllSampleTracks.R <directory_path>\n")
-        cat("Example: Rscript 001_plotAllSampleTracks.R 240819Bel\n")
-        stop()
-    }
-    directory_path <- file.path(Sys.getenv("HOME"), "data", args[1])
-    if(!dir.exists(directory_path)) {
-        cat(sprintf("Error: Directory %s does not exist.\n", directory_path))
-        stop()
-    }
-    return(directory_path)
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n")
-    df <- sample_table
-    cf_cols <- grep("X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing __cf_ tag found in sample table")
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table, attribute_to_get = "control_factors") {
-    cat("Grabbing attributes from sample table\n")
-    df <- sample_table
-    control_factors <- attr(df, attribute_to_get)
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.\nVerify 003_updateSampleGrid.R")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n")
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n")
-    if (length(control_indices) == 0) {
-        warning("No matching control found")
-        cat("Setting control_index to 1\n")
-        control_indices <- 1
-    }
-    if (length(control_indices) > max_controls) {
-    warning(paste("Multiple matching controls found, using first", max_controls))
-    control_indices[1:max_controls]
-    } else if (length(control_indices) == 1){
-        return(control_indices)
-    }
-}
-
-load_sample_table <- function(directory_name) {
-    cat("Loading sample_table from", directory_name, "\n")
-    documentation_dir_path <- file.path(directory_name, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n")
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat(sprintf("Files found in %s\n", documentation_dir_path))
-        print(sample_table_path)
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n")
-        stop()
-    }
-    cat(sprintf("Reading %s\n", sample_table_path))
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    if (!("sample_ID" %in% colnames(sample_table))) {
-           cat("Sample table does not contain the sample_ID column.\n")
-           cat("sample_ID column is required for analysis.\n")
-           cat("See 000_setupExperimentDir.R and 003_updateSampleGrid.R.\n")
-           stop()
-    }
-    sample_table <- process_control_factors(sample_table)
-    cat("Head of sample_table\n")
-    print(head(sample_table))
-    return(sample_table)
-}
-
-load_reference_genome <- function(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna") {
-    cat("Loading reference genome\n")
-    directory_of_refgenomes <- file.path(Sys.getenv("HOME"), "data", genome_dir)
-    if(!dir.exists(directory_of_refgenomes)) {
-        stop("Directory with reference genomes doesnt exist.\n")
-    }
-    genome_file_path <- list.files(directory_of_refgenomes, pattern = genome_pattern, full.names = TRUE, recursive = TRUE)
-    if (length(genome_file_path) > 1) {
-        cat(sprintf("More than one file matched genome pattern %s", genome_pattern))
-        print(genome_file_path)
-        stop()
-    }
-    if(!file.exists(genome_file_path)) {
-        stop("Reference genome doesnt exist.\n")
-    }
-    refGenome <- readFasta(genome_file_path)
-    refGenome <- data.frame(chrom = names(as(refGenome, "DNAStringSet")), 
-                    basePairSize = width(refGenome)) %>% filter(chrom != "chrM")
-    cat("Head of refGenome.\n")
-    print(head(refGenome))
-    return(refGenome)
-}
-
-#Create GRanges object to read in a particular chromosome
-create_chromosome_GRange <- function(refGenome) {
-    cat("Creating chromosome GRange for loading feature, samples, etc\n")
-    genomeRange_to_get <- GRanges(seqnames = refGenome$chrom,
-                                  ranges = IRanges(start = 1, 
-                                                   end = refGenome$basePairSize),
-                                  strand = "*")
-    cat("Head of Genome Range for loading other files.\n")
-    print(head(genomeRange_to_get))
-    return(genomeRange_to_get)
-}
-# Chromosome mapping functions
-chr_to_roman <- c(
-  "1" = "I", "2" = "II", "3" = "III", "4" = "IV", "5" = "V", "6" = "VI", "7" = "VII", "8" = "VIII",
-  "9" = "IX", "10" = "X", "11" = "XI", "12" = "XII", "13" = "XIII", "14" = "XIV", "15" = "XV", "16" = "XVI"
-)
-
-roman_to_chr <- setNames(names(chr_to_roman), chr_to_roman)
-
-normalize_chr_names <- function(chr_names, target_style) {
-  chr_names <- gsub("^chr", "", chr_names)
-  normalized_chr_name <- switch(target_style,
-    "UCSC" = paste0("chr", chr_names),
-    "Roman" = sapply(chr_names, function(x) paste0("chr", ifelse(x %in% names(chr_to_roman), chr_to_roman[x], x))),
-    "Numeric" = sapply(chr_names, function(x) ifelse(x %in% chr_to_roman, roman_to_chr[x], x)),
-    stop("Unknown target style")
-    )
-    cat("Structure of normalized_chr_name\n")
-    print(str(normalized_chr_name))
-    return(unname(normalized_chr_name)) 
-}
-
-determine_chr_style <- function(chr_names) {
-  if (all(grepl("^chr[0-9]+$", chr_names))) return("UCSC")
-  if (all(grepl("^chr[IVX]+$", chr_names))) return("Roman")
-  if (all(grepl("^[0-9]+$", chr_names))) return("Numeric")
-  return("Unknown")
-}
-
-load_feature_file_GRange <- function(chromosome_to_plot = 10, feature_file_pattern = "eaton_peaks", genomeRange_to_get) {
-  cat(sprintf("Loading %s feature file.\n", feature_file_pattern))
-  # Input validation
-  feature_file_dir <- file.path(Sys.getenv("HOME"), "data", "feature_files")
-  if(!dir.exists(feature_file_dir)) {
-    stop(sprintf("Directory %s does not exist.", feature_file_dir))
-  }
-  feature_file_path <- list.files(feature_file_dir, pattern = feature_file_pattern, full.names = TRUE, recursive = TRUE)
-  if(length(feature_file_path) != 1) {
-    stop(sprintf("Error finding feature file. Found %d files: %s", length(feature_file_path), paste(feature_file_path, collapse = ", ")))
-  }
-  # Load feature file and determine its style
-  feature_grange <- import.bed(feature_file_path)
-  feature_style <- determine_chr_style(seqlevels(feature_grange))
-  cat("Feature file chromosome style:", feature_style, "\n")
-  # Determine genomeRange style
-  genome_style <- determine_chr_style(seqlevels(genomeRange_to_get))
-  cat("Genome range chromosome style:", genome_style, "\n")
-
-  if (feature_style == genome_style) {
-    # Styles match, use genomeRange_to_get as is
-    cat("Styles match. Using provided genome range.\n")
-    feature_grange_subset <- subsetByOverlaps(feature_grange, genomeRange_to_get)
-  } else {
-    # Styles don't match, adjust genomeRange_to_get
-    cat("Styles don't match. Adjusting genome range to match feature file.\n")
-    adjusted_genomeRange <- genomeRange_to_get
-    new_seqlevels <- normalize_chr_names(seqlevels(genomeRange_to_get), feature_style)
-    seqlevels(adjusted_genomeRange) <- new_seqlevels
-
-    feature_grange_subset <- subsetByOverlaps(feature_grange, adjusted_genomeRange)
-
-    new_seqlevels <- normalize_chr_names(seqlevels(feature_grange_subset), genome_style)
-    seqlevels(feature_grange_subset) <- new_seqlevels
-    cat(sprintf("Confirming Feature GRange file style: %s\n", determine_chr_style(seqlevels(feature_grange_subset))))
-
-  }
-  return(feature_grange_subset)
-}
-
-
-unique_labeling <- function(table, categories_for_label) {
-    # Input validation
-    if (!is.data.frame(table)) {
-        stop("Input 'table' must be a data frame")
-    }
-    if (!is.character(categories_for_label) || length(categories_for_label) == 0) {
-        stop("Input 'categories_for_label' must be a non-empty character vector")
-    }
-    
-    # Ensure antibody category is always included
-    if (!"antibody" %in% categories_for_label) {
-    categories_for_label <- c("antibody", categories_for_label)
-    }
-    
-    print(paste("Categories for label:", paste(categories_for_label, collapse = ", ")))
-    
-    # Check if all categories exist in the table
-    missing_categories <- setdiff(categories_for_label, colnames(table))
-    if (length(missing_categories) > 0) {
-        stop(paste("The following categories are missing from the table:", 
-        paste(missing_categories, collapse = ", ")))
-    }
-    
-    # Identify unique values for each category
-    unique_values <- lapply(table[categories_for_label], unique)
-    print("Unique values for each category:")
-    print(unique_values)
-    
-    # Function to construct label for a single sample
-    construct_label <- function(sample) {
-    differing_categories <- sapply(categories_for_label, function(cat) {
-        if (length(unique_values[[cat]]) > 1 || cat == "antibody") {
-            return(sample[cat])
-            #return(paste(cat, sample[cat], sep = ": "))
-        } else {
-            return(NULL)
-        }
-    })
-        differing_categories <- differing_categories[!sapply(differing_categories, is.null)]
-        return(paste(differing_categories, collapse = "_"))
-    }
-    
-    # Apply the construct_label function to each sample (row)
-    labels <- apply(table, 1, construct_label)
-    
-    print("Constructed labels:")
-    print(labels)
-    
-    return(unlist(labels))
-}
-plot_all_sample_tracks <- function(sample_table, directory_path, chromosome_to_plot = 10, genomeRange_to_get, annotation_track, highlight_gr, pattern_for_bigwig = "S288C_log2ratio") {
-    main_title_of_plot <- paste("Complete View of Chrom", as.character(chromosome_to_plot), sep = "")
-    categories_for_label <- c("strain_source", "rescue_allele", "mcm_tag", "antibody", "timepoint_after_release")
-    date_plot_created <- stringr::str_replace_all(Sys.time(), pattern = ":| |-", replacement="")  
-    factors_to_match <- get_factors_to_match(sample_table)
-    cat("Factors in attributes of sample_table\n")
-    print(factors_to_match)
-    plot_output_dir <- file.path(directory_path, "plots")
-    bigwig_dir <- file.path(directory_path, "bigwig")
-    cat("Plotting all sample tracks.\n")
-    gtrack <- GenomeAxisTrack(name = paste("Chr ", chromosome_to_plot, " Axis", sep = ""))
-    cat("===============\n")
-    column_names <- colnames(sample_table)
-    is_comparison_column <- grepl("^comp_", colnames(sample_table))
-    comparison_columns <- column_names[is_comparison_column]
-    chromosome_as_chr_roman <- paste("chr", as.roman(chromosome_to_plot), sep = "")
-    subset_gr <- genomeRange_to_get[seqnames(genomeRange_to_get) == chromosome_as_chr_roman]
-    for (col in comparison_columns) {
-        #Need to modify this since it depends on the names I assign to the comparisons
-        if (col == "comp_timecourse1108") {
-        comparison_title <- sub("comp_", "", col)
-        comp_title <- paste(main_title_of_plot, "\n", comparison_title, sep = "")
-        cat(sprintf("Column to plot: %s\n", col))
-        cat("===============\n")
-        comparison_samples <- sample_table[sample_table[[col]],]
-        labels <- unique_labeling(comparison_samples, categories_for_label)
-        print(labels)
-        all_tracks <- list()
-        all_tracks <- append(all_tracks, gtrack)
-        for (sample_index in 1:nrow(comparison_samples)) {
-            sample_ID_pattern <- comparison_samples$sample_ID[sample_index]
-            initial_matches <- list.files(bigwig_dir, pattern = as.character(sample_ID_pattern), full.names = TRUE, recursive = TRUE)
-            path_to_bigwig <- initial_matches[grepl(pattern_for_bigwig, initial_matches)]
-            print(path_to_bigwig)
-            if (length(path_to_bigwig) == 0){
-                cat(sprintf("No bigwig found for sample_ID: %s\n", sample_ID_pattern))
-                cat("Results of initial matches\n")
-                print(initial_matches)
-            } else if (length(path_to_bigwig) == 1){
-                if (sample_index == 1) {
-                    cat("===============\n")
-                    cat("First sample being processed. Setting the control sample based on it.\n")
-                    control_index <- determine_matching_control(sample_row = comparison_samples[sample_index, ], sample_table, factors_to_match = factors_to_match)
-                    if(length(control_index) == 0) {
-                        cat("No control index found\n")
-                        cat(sprintf("Sample row: %s\n", comparison_samples[sample_index, ]))
-                        control_ID_pattern <- sample_table$sample_ID[1]
-                        control_sample_name <-sample_table$short_name[1] 
-                        control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                        if (pattern_for_bigwig == "_bamcomp.bw"){
-                            is_input_path <- lapply(strsplit(pattern_subset_bigwig_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths[is_input_path]
-                        } else {
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths
-                        }
-                        log2ratio_paths <- control_initial_matches[grepl(pattern_for_bigwig, control_initial_matches)]
-                        is_input_path <- lapply(strsplit(log2ratio_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                        control_path_to_bigwig <- log2ratio_paths[is_input_path]
-                    } else {
-                       #control_index <- select_control_index(control_indices = control_index, max_controls = 1)
-                        control_ID_pattern <- sample_table$sample_ID[control_index]
-                        control_sample_name <-sample_table$short_name[control_index] 
-                        control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                        pattern_subset_bigwig_paths <- control_initial_matches[grepl(pattern_for_bigwig, control_initial_matches)]
-                        if (pattern_for_bigwig == "_bamcomp.bw"){
-                            is_input_path <- lapply(strsplit(pattern_subset_bigwig_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths[is_input_path]
-                        } else {
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths
-                        }
-                        log2ratio_paths <- control_initial_matches[grepl(pattern_for_bigwig, control_initial_matches)]
-                        is_input_path <- lapply(strsplit(log2ratio_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                        control_path_to_bigwig <- log2ratio_paths[is_input_path]
-                    }
-                    if(length(control_path_to_bigwig) == 0){
-                        cat("Appropriate control bigwig and index one failed. Setting to second sample.\n")
-                        control_ID_pattern <- sample_table$sample_ID[2]
-                        control_sample_name <-sample_table$short_name[2] 
-                        control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                        pattern_subset_bigwig_paths <- control_initial_matches[grepl(pattern_for_bigwig, control_initial_matches)]
-                        if (pattern_for_bigwig == "_bamcomp.bw"){
-                            is_input_path <- lapply(strsplit(pattern_subset_bigwig_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths[is_input_path]
-                        } else {
-                            control_path_to_bigwig <- pattern_subset_bigwig_paths
-                        }
-                        log2ratio_paths <- control_initial_matches[grepl(pattern_for_bigwig, control_initial_matches)]
-                        is_input_path <- lapply(strsplit(log2ratio_paths, "_"), function(x) length(grep(as.character(control_ID_pattern), x))) > 1
-                        control_path_to_bigwig <- log2ratio_paths[is_input_path]
-                    }
-                    print("Name of the control bigwig path")
-                    print(control_path_to_bigwig)
-                    control_bigwig_to_plot <- import(con = control_path_to_bigwig, which = subset_gr)
-                    control_track_to_plot <- DataTrack(control_bigwig_to_plot, type = "l", name = "Input", col = "#fd0036", chromosome = chromosome_as_chr_roman)
-                    sample_short_name <- comparison_samples$short_name[sample_index]
-                    bigwig_to_plot <- import(con = path_to_bigwig, which = subset_gr)
-                    track_to_plot <- DataTrack(bigwig_to_plot, type = "l", name = labels[sample_index], col = "#fd0036", chromosome = chromosome_as_chr_roman)
-                    print(track_to_plot)
-                    all_tracks <- append(all_tracks, control_track_to_plot)
-                    all_tracks <- append(all_tracks, track_to_plot)
-                } else {
-                    sample_short_name <- comparison_samples$short_name[sample_index]
-                    bigwig_to_plot <- import(con = path_to_bigwig, which = subset_gr)
-                    track_to_plot <- DataTrack(bigwig_to_plot, type = "l", name = labels[sample_index], col = "#fd0036", chromosome = chromosome_as_chr_roman)
-                    all_tracks <- append(all_tracks, track_to_plot)
-                }
-            }
-            # Add attempt to further subset using timeid if there are more than one match.
-
-    pattern_for_bigwig_name_sans_underscore <- gsub("_|\\.bw", "", pattern_for_bigwig)
-    comparison_name_sans_underscore <- gsub("_", "", col)
-    sample_timeid <- basename(strsplit(path_to_bigwig, "_")[[1]])[1]
-    all_tracks <- append(all_tracks, annotation_track)
-    output_plot_name <- paste(plot_output_dir, "/", date_plot_created, "_", sample_timeid, "_", chromosome_as_chr_roman, "_", pattern_for_bigwig_name_sans_underscore,"_", comparison_name_sans_underscore, "_", ".svg", sep = "")
-    print("Name of the plot to be generated")
-    print(output_plot_name)
-    cat(sprintf("End of for loop for %s ====\n", col))
-    #svg(output_plot_name)
-    plotTracks(all_tracks, 
-                main = comp_title,
-                chromosome = chromosome_as_chr_roman)
-                #ylim = c(0, 100000))
-    #dev.off()
-    
-}
-    } else {
-        cat(sprintf("Testing. Only plotting %s\n", col))
-    }
-    cat("All comparisons plotted ===============\n")
-}
-}
-
-if(!interactive()){
-    main()
-    cat("rsync -nav username@domain:~/data/<dir>/plots/* /local/dir/<dir>/plots/\n")
-} else {
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(gtools)
-    })
-    #@update
-    directory_path <- "240819Bel"
-    #cat("Logic of main function\n")
-    #main_function_logic <- main
-    list_of_functions_and_variables <- ls()
-    #cat("Main function: Validating directory\n")
-    directory_path <- validate_input(directory_path)
-    chromosome_to_plot = 10
-    #chromosome_to_plot = c(10, paste0("chr", as.roman(10)))
-    cat("Main function: loading sample table\n")
-    sample_table <- load_sample_table(directory_path)
-    #cat("Main function: loading reference genome\n")
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-    #cat("Main function: Creating genomeRange_to_get\n")
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-    ## Load peaks feature file for highlight track and annotation. 
-    feature_file_pattern = "eaton_peaks"
-    #cat("Main function: load feature grange\n")
-    feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,genomeRange_to_get = genomeRange_to_get)
-    feature_track <- AnnotationTrack(feature_grange, name = paste("Origin Peaks","Eaton 2010", sep = ""))
-    ## Plot samples, determine the input control for each sample. No need to modify the files provided then. Just the logic.
-    #pattern_for_bigwig <- "_indivNorm.bw"
-    pattern_for_bigwig <- "_bamcomp.bw"
-    plot_all_sample_tracks(sample_table = sample_table,directory_path = directory_path,chromosome_to_plot = chromosome_to_plot,genomeRange_to_get = genomeRange_to_get, annotation_track = feature_track, highlight_gr = feature_grange, pattern_for_bigwig = pattern_for_bigwig)
-    #cat("Script end=====\n")
-    #cat("rsync -nav username@domain:~/data/<dir>/plots/* /local/dir/<dir>/plots/\n")
-}
diff --git a/next_generation_sequencing/005_genomeTracks/genomerangechrconversion.R b/next_generation_sequencing/005_genomeTracks/genomerangechrconversion.R
deleted file mode 100644
index 4efd2e8..0000000
--- a/next_generation_sequencing/005_genomeTracks/genomerangechrconversion.R
+++ /dev/null
@@ -1,72 +0,0 @@
-
-suppressPackageStartupMessages({
-    library(GenomicAlignments)
-    library(Gviz)
-    library(rtracklayer)
-    library(ShortRead)
-    library(tidyverse)
-})
-create_chromosome_GRange <- function(refGenome, chromosome_to_plot = 10) {
-    cat("Creating chromosome GRange for loading feature, samples, etc\n")
-    genomeRange_to_get <- GRanges(seqnames = "chrX",
-                                  ranges = IRanges(start = 1, 
-                                                   end = 170000,
-                                  strand = "*"))
-    return(genomeRange_to_get)
-}
-
-chr_convert_gr <- function(gr, verbose = FALSE) {
-      if (!is(gr, "GRanges")) {
-            stop("Input must be a GenomicRanges object, you dimwit!")
-          }
-      seqnames <- as.character(seqnames(gr))
-    
-      if (verbose) cat("Original seqnames:", paste(unique(seqnames), collapse = ", "), "\n")
-      # Function to convert a single chromosome name
-      convert_single_chr <- function(chr) {
-            chr <- gsub("^chr", "", chr, ignore.case = TRUE)
-            if (chr %in% c("X", "Y", "MT", "M")) {
-                  return(paste0("chr", chr))
-                }
-            if (grepl("^[IVXLCDM]+$", chr)) {
-                  return(paste0("chr", chr))  # Already in roman numeral format
-                } else if (grepl("^\\d+$", chr)) {
-                  return(paste0("chr", as.roman(as.integer(chr))))
-                } else {
-                  warning(paste("Unable to convert chromosome:", chr))
-                  return(paste0("chr", chr))
-                }
-          }
-      # Vectorized conversion
-      new_seqnames <- vapply(seqnames, convert_single_chr, character(1))
-      if (verbose) {
-            cat("Converted seqnames:", paste(unique(new_seqnames), collapse = ", "), "\n")
-            cat("Number of seqnames changed:", sum(new_seqnames != seqnames), "\n")
-          }
-      # Update the GenomicRanges object
-      seqlevels(gr) <- unique(new_seqnames)
-      seqnames(gr) <- new_seqnames
-      return(gr)
-}
-
-# Example usage and testing
-set.seed(42)  # For reproducibility, you numbskull
-test_gr <- GRanges(
-      seqnames = c("1", "X", "chrII", "20", "chrY", "M", "chr10", "III", "chrMT"),
-      ranges = IRanges(start = sample(1:1000, 9), width = 100)
-    )
-
-result <- chr_convert_gr(test_gr, verbose = TRUE)
-print(result)
-
-# Test with invalid input
-tryCatch(
-      chr_convert_gr(data.frame()),
-      error = function(e) cat("Error caught:", conditionMessage(e), "\n")
-    )
-
-main <- function() {
-    genomeRange_to_get <-  create_chromosome_GRange()
-    chr_convert_gr(genomeRange_to_get)
-    }
-main()
diff --git a/next_generation_sequencing/005_genomeTracks/plotWithHighlights.R b/next_generation_sequencing/005_genomeTracks/plotWithHighlights.R
deleted file mode 100755
index 1b53459..0000000
--- a/next_generation_sequencing/005_genomeTracks/plotWithHighlights.R
+++ /dev/null
@@ -1,461 +0,0 @@
-#DESCRIPTION: 
-#USAGE:
-#TODO: Need some way to aggregate the column names I use from the sampleConfig.R files to keep track of variables I use to keep consistent experiment to experiment.
-#TODO figure out if there is a way to normalize the samples 
-##TODO: Define the comparisons and plots to be generated in my sampleConfig.R template. 
-##TODO: Find the best way to have the same levels and factors when I read in the sampleGridConfig.R file. This is defined by the categories list variable. Can grab that during 002_loadSampleGrid and use it to equalize. 
-##TODO: Use 002_loadSampleGrid to open the sample_table given a directory. Use conditional statement to determine the ID. Could potentially use system function to call find and print with awk statement. R would require list.files(), strsplit, and grabbing regular expression for 5 digits.
-#Load packages using through a list of strings and suppress the messages, return a TRUE if loading was succesful
-##TODO: Need to make sure the chromosome IDs are formatted properly.
-##TODO: need to automate the creation of the experiments to plot. 
-##TODO: Need to ensure that the names I use in the columns are compatible with my short name convention for subsetting df
-main <- function() {
-    #Load packages
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-    })
-    args <- commandArgs(trailingOnly = TRUE)
-    directory_path <- validate_input(args)
-    #Add process_control_factors, get_factors_to_match
-    sample_table <- load_sample_table(directory_path)
-    chromosome_to_plot = 10
-    options(ucscChromosomeNames=FALSE)
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-
-    #feature_file_pattern = "eaton_peaks"
-    #feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-    #                                         genomeRange_to_get = genomeRange_to_get)
-
-    # Convert to input, add determine_matching_control, select_control_index
-    #control_dir <- "EatonBel"
-    #control_track <- load_control_grange_data(control_dir = control_dir, chromosome_to_plot = chromosome_to_plot,
-                #                             genomeRange_to_get = genomeRange_to_get)
-
-    #plot_all_sample_tracks(sample_table = sample_table,
-    #                       directory_name = directory_path,
-    #                       chromosome_to_plot = chromosome_to_plot, 
-    #                       genomeRange_to_get = genomeRange_to_get, 
-    #                       control_track = control_track, 
-    #                       annotation_track = origin_track)
-
-}
-
-validate_input <- function(args) {
-    if (length(args) != 1) {
-        cat("Error: Invalid number of arguments.\n")
-        cat("Usage: Rscript 001_plotAllSampleTracks.R <directory_path>\n")
-        cat("Example: Rscript 001_plotAllSampleTracks.R 240819Bel\n")
-        stop()
-    }
-    directory_path <- file.path(Sys.getenv("HOME"), "data", args[1])
-    if(!dir.exists(directory_path)) {
-        cat(sprintf("Error: Directory %s does not exist.\n", directory_path))
-        stop()
-    }
-    return(directory_path)
-}
-
-process_control_factors <- function(sample_table) {
-    cat("Process control factors from __cf_ columns\n")
-    df <- sample_table
-    cf_cols <- grep("X__cf_", names(df), value = TRUE)
-    if(length(cf_cols) == 0) {
-        cat("No columns containing __cf_ tag found in sample table")
-        stop("Verify sample table was produced with updated sampleGridConfig.")
-    }
-    control_factors <- lapply(df[cf_cols], function(x) strsplit(x[1], ",")[[1]])
-    names(control_factors) <- sub("X__cf_", "", cf_cols)
-    df[cf_cols] <- NULL
-    attr(df, "control_factors") <- control_factors
-    return(df)
-}
-
-get_factors_to_match <- function(sample_table, attribute_to_get = "control_factors") {
-    cat("Grabbing attributes from sample table\n")
-    df <- sample_table
-    control_factors <- attr(df, attribute_to_get)
-    if (is.null(control_factors)) {
-        stop("No control factors defined in sample data.\nVerify 003_updateSampleGrid.R")
-    }
-    all_factors <- unlist(control_factors)
-    return(intersect(all_factors, colnames(df)))
-}
-
-determine_matching_control <- function(sample_row, sample_table, factors_to_match) {
-    cat("Determining control row for sample row.\n")
-    df <- sample_table
-    comparison_row <- sample_row[factors_to_match]
-    rows_with_same_factors <- apply(df[, factors_to_match], 1, function(row) {
-        all(row == comparison_row)
-    })
-    is_input <- df$antibody == "Input"
-    index <- as.numeric(unname(which(is_input & rows_with_same_factors)))
-    return(index)
-}
-
-select_control_index <- function(control_indices, max_controls = 1) {
-    cat("Processing control index to ensure one is used.\n")
-    if (length(control_indices) == 0) {
-        warning("No matching control found")
-        cat("Setting control_index to 1\n")
-        control_indices <- 1
-    }
-    if (length(control_indices) > max_controls) {
-    warning(paste("Multiple matching controls found, using first", max_controls))
-    control_indices[1:max_controls]
-    } else if (length(control_indices) == 1){
-        return(control_indices)
-    }
-}
-
-load_sample_table <- function(directory_name) {
-    cat("Loading sample_table from", directory_name, "\n")
-    documentation_dir_path <- file.path(directory_name, "documentation")
-    sample_table_path <- list.files(documentation_dir_path, pattern = "sample_table", full.names = TRUE)
-    if(length(sample_table_path) == 0){
-        cat(sprintf("No files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat("Consult 000_setupExperimentDir to create sample_table.tsv for the project\n")
-        stop()
-    } else if(length(sample_table_path) > 1){
-        cat(sprintf("Multiple files with pattern sample_table found in %s\n", documentation_dir_path))
-        cat(sprintf("Files found in %s\n", documentation_dir_path))
-        print(sample_table_path)
-        cat("Consult 000_setupExperimentDir and ensure no duplicates are present for the project\n")
-        stop()
-    }
-    cat(sprintf("Reading %s\n", sample_table_path))
-    sample_table <- read.delim(sample_table_path, header = TRUE, sep ="\t")
-    if (!("sample_ID" %in% colnames(sample_table))) {
-           cat("Sample table does not contain the sample_ID column.\n")
-           cat("sample_ID column is required for analysis.\n")
-           cat("See 000_setupExperimentDir.R and 003_updateSampleGrid.R.\n")
-           stop()
-    }
-    sample_table <- process_control_factors(sample_table)
-    cat("Head of sample_table\n")
-    print(head(sample_table))
-    return(sample_table)
-}
-
-load_reference_genome <- function(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna") {
-    cat("Loading reference genome\n")
-    directory_of_refgenomes <- file.path(Sys.getenv("HOME"), "data", genome_dir)
-    if(!dir.exists(directory_of_refgenomes)) {
-        stop("Directory with reference genomes doesnt exist.\n")
-    }
-    genome_file_path <- list.files(directory_of_refgenomes, pattern = genome_pattern, full.names = TRUE, recursive = TRUE)
-    if (length(genome_file_path) > 1) {
-        cat(sprintf("More than one file matched genome pattern %s", genome_pattern))
-        print(genome_file_path)
-        stop()
-    }
-    if(!file.exists(genome_file_path)) {
-        stop("Reference genome doesnt exist.\n")
-    }
-    refGenome <- readFasta(genome_file_path)
-    refGenome <- data.frame(chrom = names(as(refGenome, "DNAStringSet")), 
-                    basePairSize = width(refGenome)) %>% filter(chrom != "chrM")
-    cat("Head of refGenome.\n")
-    print(head(refGenome))
-    return(refGenome)
-}
-
-#Create GRanges object to read in a particular chromosome
-create_chromosome_GRange <- function(refGenome) {
-    cat("Creating chromosome GRange for loading feature, samples, etc\n")
-    genomeRange_to_get <- GRanges(seqnames = refGenome$chrom,
-                                  ranges = IRanges(start = 1, 
-                                                   end = refGenome$basePairSize),
-                                  strand = "*")
-    cat("Head of Genome Range for loading other files.\n")
-    print(head(genomeRange_to_get))
-    return(genomeRange_to_get)
-}
-# Chromosome mapping functions
-chr_to_roman <- c(
-  "1" = "I", "2" = "II", "3" = "III", "4" = "IV", "5" = "V", "6" = "VI", "7" = "VII", "8" = "VIII",
-  "9" = "IX", "10" = "X", "11" = "XI", "12" = "XII", "13" = "XIII", "14" = "XIV", "15" = "XV", "16" = "XVI"
-)
-
-roman_to_chr <- setNames(names(chr_to_roman), chr_to_roman)
-
-normalize_chr_names <- function(chr_names, target_style) {
-  chr_names <- gsub("^chr", "", chr_names)
-  normalized_chr_name <- switch(target_style,
-    "UCSC" = paste0("chr", chr_names),
-    "Roman" = sapply(chr_names, function(x) paste0("chr", ifelse(x %in% names(chr_to_roman), chr_to_roman[x], x))),
-    "Numeric" = sapply(chr_names, function(x) ifelse(x %in% chr_to_roman, roman_to_chr[x], x)),
-    stop("Unknown target style")
-    )
-    cat("Structure of normalized_chr_name\n")
-    print(str(normalized_chr_name))
-    return(unname(normalized_chr_name)) 
-}
-
-determine_chr_style <- function(chr_names) {
-  if (all(grepl("^chr[0-9]+$", chr_names))) return("UCSC")
-  if (all(grepl("^chr[IVX]+$", chr_names))) return("Roman")
-  if (all(grepl("^[0-9]+$", chr_names))) return("Numeric")
-  return("Unknown")
-}
-
-load_feature_file_GRange <- function(chromosome_to_plot = 10, feature_file_pattern = "eaton_peaks", genomeRange_to_get) {
-  cat(sprintf("Loading %s feature file.\n", feature_file_pattern))
-  # Input validation
-  feature_file_dir <- file.path(Sys.getenv("HOME"), "data", "feature_files")
-  if(!dir.exists(feature_file_dir)) {
-    stop(sprintf("Directory %s does not exist.", feature_file_dir))
-  }
-  feature_file_path <- list.files(feature_file_dir, pattern = feature_file_pattern, full.names = TRUE, recursive = TRUE)
-  if(length(feature_file_path) != 1) {
-    stop(sprintf("Error finding feature file. Found %d files: %s", length(feature_file_path), paste(feature_file_path, collapse = ", ")))
-  }
-  # Load feature file and determine its style
-  feature_grange <- import.bed(feature_file_path)
-  feature_style <- determine_chr_style(seqlevels(feature_grange))
-  cat("Feature file chromosome style:", feature_style, "\n")
-  # Determine genomeRange style
-  genome_style <- determine_chr_style(seqlevels(genomeRange_to_get))
-  cat("Genome range chromosome style:", genome_style, "\n")
-
-  if (feature_style == genome_style) {
-    # Styles match, use genomeRange_to_get as is
-    cat("Styles match. Using provided genome range.\n")
-    feature_grange_subset <- subsetByOverlaps(feature_grange, genomeRange_to_get)
-  } else {
-    # Styles don't match, adjust genomeRange_to_get
-    cat("Styles don't match. Adjusting genome range to match feature file.\n")
-    adjusted_genomeRange <- genomeRange_to_get
-    new_seqlevels <- normalize_chr_names(seqlevels(genomeRange_to_get), feature_style)
-    seqlevels(adjusted_genomeRange) <- new_seqlevels
-
-    feature_grange_subset <- subsetByOverlaps(feature_grange, adjusted_genomeRange)
-
-    new_seqlevels <- normalize_chr_names(seqlevels(feature_grange_subset), genome_style)
-    seqlevels(feature_grange_subset) <- new_seqlevels
-    cat(sprintf("Confirming Feature GRange file style: %s\n", determine_chr_style(seqlevels(feature_grange_subset))))
-
-  }
-  return(feature_grange_subset)
-}
-
-load_control_grange_data <- function(control_dir, file_identifier, chromosome_to_plot = 10, genomeRange_to_get) {
-    cat("Loading control track data from", control_dir, "\n")
-    bigwig_dir_path <- file.path(Sys.getenv("HOME"), "data", control_dir, "bigwig")
-    bigwig_file_paths <- list.files(bigwig_dir_path, pattern = file_identifier, full.names = TRUE, recursive = TRUE) 
-    S288C_bigwigs <- grepl("S288C", bigwig_file_paths)
-    bigwig_file_path <- bigwig_file_paths[S288C_bigwigs]
-    if(!file.exists(bigwig_file_path)) {
-        cat(sprintf("File %s doesnt exist.\n", bigwig_file_path))
-        stop()
-    }
-    control_style <- determine_chr_style(seqlevels(import(bigwig_file_path)))
-    chromosome_to_subset <- normalize_chr_names(chromosome_to_plot, control_style)
-    subset_genome_range <- genomeRange_to_get[seqnames(genomeRange_to_get) == chromosome_to_subset]
-    control_grange <- import(bigwig_file_path, which = genomeRange_to_get)
-    return(control_grange)
-}
-minimum_self_contained_example <- function() {
-# Load required libraries
-library(Gviz)
-library(GenomicRanges)
-library(rtracklayer)
-
-# Set up example data
-chr <- "chr1"
-start <- 1000
-end <- 5000
-
-# Create a GRanges object for highlights
-highlights <- GRanges(seqnames = chr, 
-ranges = IRanges(start = c(1500, 3000), 
-end = c(2000, 3500)))
-
-# Create dummy data for DataTracks
-data1 <- runif(end - start + 1, min = 0, max = 100)
-data2 <- rnorm(end - start + 1, mean = 50, sd = 10)
-data3 <- rpois(end - start + 1, lambda = 5)
-
-# Create DataTracks
-dataTrack1 <- DataTrack(start = start:end, end = start:end, data = data1, chromosome = chr, genome = "hg19", name = "Track 1")
-dataTrack2 <- DataTrack(start = start:end, end = start:end, data = data2, chromosome = chr, genome = "hg19", name = "Track 2")
-dataTrack3 <- DataTrack(start = start:end, end = start:end, data = data3, chromosome = chr, genome = "hg19", name = "Track 3")
-
-# Create a dummy annotation track
-annotationData <- GRanges(seqnames = chr,
-ranges = IRanges(start = c(1200, 2500, 3800),
-end = c(1800, 3000, 4200)),
-strand = c("+", "-", "+"),
-feature = c("Gene A", "Gene B", "Gene C"))
-annotationTrack <- AnnotationTrack(start = start(annotationData),
-end = end(annotationData),
-chromosome = as.character(seqnames(annotationData)),
-strand = strand(annotationData),
-feature = as.character(annotationData$feature),
-name = "Genes")
-
-# Create axis track
-axisTrack <- GenomeAxisTrack()
-
-# Create highlight track encompassing all other tracks
-highlightTrack <- HighlightTrack(trackList = list(axisTrack, dataTrack1, dataTrack2, dataTrack3, annotationTrack),
-start = start(highlights),
-end = end(highlights),
-chromosome = as.character(seqnames(highlights)))
-
-# Plot the tracks
-plotTracks(highlightTrack, 
-from = start, 
-to = end, 
-chromosome = chr)
-}
-
-plot_all_sample_tracks <- function(sample_table, directory_path, chromosome_to_plot = 10, genomeRange_to_get, control_track, annotation_track, highlight_gr) {
-    main_title_of_plot_track <- paste("Complete View of Chrom", as.character(chromosome_to_plot), sep = " ")
-    date_plot_created <- stringr::str_replace_all(Sys.time(), pattern = ":| |-", replacement="")  
-    factors_to_match <- get_factors_to_match(sample_table)
-    cat("Factors in attributes of sample_table\n")
-    print(factors_to_match)
-    plot_output_dir <- file.path(directory_path, "plots")
-    bigwig_dir <- file.path(directory_path, "bigwig")
-    cat("Plotting all sample tracks.\n")
-    chromosome_as_chr_roman <- paste("chr", as.roman(chromosome_to_plot), sep = "")
-    gtrack <- GenomeAxisTrack(name = chromosome_as_chr_roman)
-    for (sample_index in 1:nrow(sample_table)) {
-        if(sample_index == 1){
-            cat("===============\n")
-            sample_ID_pattern <- sample_table$sample_ID[sample_index]
-            initial_matches <- list.files(bigwig_dir, pattern = as.character(sample_ID_pattern), full.names = TRUE, recursive = TRUE)
-            path_to_bigwig <- initial_matches[grepl("S288C", initial_matches)]
-            print("Name of the bigwig path")
-            print(path_to_bigwig)
-            if (length(path_to_bigwig) == 0){
-                cat(sprintf("No bigwig found for sample_ID: %s\n", sample_ID_pattern))
-                cat("Results of initial matches\n")
-                print(initial_matches)
-            }
-            if (length(path_to_bigwig) > 0){
-                control_index <- determine_matching_control(sample_row = sample_table[sample_index, ], sample_table, factors_to_match = factors_to_match)
-                if(length(control_index) == 0) {
-                    cat("No control index found\n")
-                    cat("Printing sample row\n")
-                    print(sample_table[sample_index, ])
-                }
-                control_index <- select_control_index(control_indices = control_index, max_controls = 1)
-                control_ID_pattern <- sample_table$sample_ID[control_index]
-                control_sample_name <-sample_table$short_name[control_index] 
-                control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                control_path_to_bigwig <- control_initial_matches[grepl("S288C", control_initial_matches)]
-                if(length(control_path_to_bigwig) == 0){
-                    cat("Appropriate control bigwig not found. Setting to first sample.\n")
-                    control_ID_pattern <- sample_table$sample_ID[1]
-                    control_sample_name <-sample_table$short_name[1] 
-                    control_initial_matches <- list.files(bigwig_dir, pattern = as.character(control_ID_pattern), full.names = TRUE, recursive = TRUE)
-                    control_path_to_bigwig <- control_initial_matches[grepl("S288C", control_initial_matches)]
-                }
-                print("Control ID pattern")
-                print(control_ID_pattern)
-                print("Name of the control bigwig path")
-                print(control_path_to_bigwig)
-                bigwig_to_plot <- import(con = path_to_bigwig, which = genomeRange_to_get)
-                cat("Bigwig plot output\n")
-                head(bigwig_to_plot)
-                sample_short_name <- sample_table$short_name[sample_index]
-                track_to_plot <- DataTrack(bigwig_to_plot, type = "l", name = sample_short_name, col = "#E41A1C", chromosome = chromosome_as_chr_roman)
-                sample_control_bigwig_to_plot <- import(con = control_path_to_bigwig, which = genomeRange_to_get)
-                sample_control_track_to_plot <- DataTrack(sample_control_bigwig_to_plot, type = "l", name = control_sample_name, col = "#377EB8", chromosome = chromosome_as_chr_roman)
-                all_tracks <- list(gtrack, sample_control_track_to_plot, track_to_plot, control_track, annotation_track)
-                cat("===============\n")
-                sample_style <- determine_chr_style(seqlevels(track_to_plot))
-                chromosome_to_subset <- normalize_chr_names(chromosome_to_plot, sample_style)
-                subset_highlight_gr <- highlight_gr[seqnames(highlight_gr) == chromosome_to_subset]
-                print("Subset highlight_gr")
-                print("Name of the plot to be generated")
-                output_plot_name <- paste(plot_output_dir, "/", date_plot_created, "_", chromosome_to_plot, "_", sample_short_name, "_", "WithInputAndHighlights", ".svg", sep = "")
-                print(output_plot_name)
-                #plotTracks(trackList = all_tracks,
-                #    main = main_title_of_plot_track,
-                #    chromosome = chromosome_as_chr_roman,
-                #    ylim = c(0, 100000))
-                cat("===============\n")
-                highlight_track <- HighlightTrack(trackList = all_tracks,
-                                        start = start(subset_highlight_gr),
-                                        end = end(subset_highlight_gr),
-                                        chromosome = seqnames(subset_highlight_gr)
-                                        )
-                                        #fill = "#FFE3E6",
-                                        #col = "#FF0000",
-                                        #alpha = 0.3)
-                cat("===============\n")
-                cat("===============\n")
-                cat("===============\n")
-                cat("===============\n")
-                #svg(output_plot_name)
-                #plotTracks(trackList = highlight_track,
-                #            main = main_title_of_plot_track,
-                #            chromosome = chromosome_as_chr_roman,
-                #            ylim = c(0, 100000))
-                #dev.off()
-              }
-        }
-    }
-    cat("Reached end of for loop\n")
-    cat("Finished plotting samples\n")
-
-}
-
-if(!interactive()){
-    main()
-} else {
-    suppressPackageStartupMessages({
-        library(QuasR)
-        library(GenomicAlignments)
-        library(Gviz)
-        library(rtracklayer)
-        library(ShortRead)
-        library(tidyverse)
-        library(gtools)
-    })
-    directory_path <- "240808Bel"
-    cat("Logic of main function\n")
-    main_function_logic <- main
-    list_of_functions_and_variables <- ls()
-    cat("Main function: Validating directory\n")
-    directory_path <- validate_input(directory_path)
-    chromosome_to_plot = 10
-    #chromosome_to_plot = c(10, paste0("chr", as.roman(10)))
-    cat("Main function: loading sample table\n")
-    sample_table <- load_sample_table(directory_path)
-    print(attr(sample_table, "control_factors"))
-    cat("Main function: loading reference genome\n")
-    refGenome <- load_reference_genome(genome_dir = "REFGENS", genome_pattern = "S288C_refgenome.fna")
-    cat("Main function: Creating genomeRange_to_get\n")
-    genomeRange_to_get <- create_chromosome_GRange(refGenome = refGenome)
-    # Load peaks feature file for highlight track and annotation. 
-    feature_file_pattern = "eaton_peaks"
-    cat("Main function: load feature grange\n")
-    feature_grange <- load_feature_file_GRange(chromosome_to_plot = chromosome_to_plot, feature_file_pattern = feature_file_pattern,genomeRange_to_get = genomeRange_to_get)
-    feature_track <- AnnotationTrack(feature_grange, name = paste("Origin Peaks","Eaton 2010", sep = ""))
-    print(head(feature_grange))
-    # Load the Eaton Bel Track data as second comparison. 
-    cat("Main function: loading control grange\n")
-    control_dir <- "EatonBel"
-    file_identifier <- "nnNnH"
-    control_grange <- load_control_grange_data(control_dir = control_dir, file_identifier = file_identifier, chromosome_to_plot = chromosome_to_plot,genomeRange_to_get = genomeRange_to_get)
-    control_track <- DataTrack(control_grange, type = "l",  name = "Eaton 2010", col = "#377EB8")
-    print(head(control_grange))
-    # Plot samples, determine the input control for each sample. No need to modify the files provided then. Just the logic.
-#    plot_all_sample_tracks(sample_table = sample_table,directory_path = directory_path,chromosome_to_plot = chromosome_to_plot,genomeRange_to_get = genomeRange_to_get,control_track = control_track,annotation_track = feature_track, highlight_gr = feature_grange)
-    minimum_self_contained_example()
-}
diff --git a/next_generation_sequencing/007_motifAnalysis/001_getMotifsFromPeakRegions.R b/next_generation_sequencing/007_motifAnalysis/001_getMotifsFromPeakRegions.R
deleted file mode 100644
index e69de29..0000000
diff --git a/next_generation_sequencing/deprecatedCode/000_directoryCreation.sh b/next_generation_sequencing/deprecatedCode/000_directoryCreation.sh
deleted file mode 100755
index f8dc3be..0000000
--- a/next_generation_sequencing/deprecatedCode/000_directoryCreation.sh
+++ /dev/null
@@ -1,13 +0,0 @@
-#!/bin/bash
-#USAGE: From anywhere, run '~/data/lab_utils/next_generation_sequencing/FTQPRC/000_directoryCreation.sh'
-#ALTERNATIVE: node_bt2build_refgenomes.sh was created as a workaround but is unnecessary now. Uses for loop for bowtie2-build. File moved to archive.
-#TODO: Debating whether I should consolidate fastqs into one directory and rename the alignment folder to BAM.
-
-data_dir=$HOME/data/
-mapfile -t bel_dirs < <(find "$data_dir" -maxdepth 1 -type d -name "*Bel*")
-
-for dir in "${bel_dirs[@]}"; do
-  echo "Creating subdirs for $dir"
-  mkdir -p "$dir"/{peak,fastq,alignment,qualityControl,bigwig,plots,logs,documentation}
-done
-
diff --git a/next_generation_sequencing/deprecatedCode/001_downloadEatonData.sh b/next_generation_sequencing/deprecatedCode/001_downloadEatonData.sh
deleted file mode 100755
index 49ad6fa..0000000
--- a/next_generation_sequencing/deprecatedCode/001_downloadEatonData.sh
+++ /dev/null
@@ -1,23 +0,0 @@
-#!/bin/bash
-#BIOPROJECT_ACCESSION=PRJNA117641
-#Eaton2010 
-#Title ORC precisely positios nucleosomes at origins of replication
-#DOWNLOAD ORC WT samples in G2 arrest.
-DOWNLOAD_TO_DIR="$1"
-OUTPUT_DIR="${HOME}/data/${DOWNLOAD_TO_DIR}"
-
-FILENAMES=(WT-G2-ORC-rep1.fastq.gz WT-G2-ORC-rep2.fastq.gz)
-WEBSITES=(ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz)
-
-FILE_INDEX=0
-for FILE in ${FILENAMES[@]}; do 
-  OUTPUT_FILE=${OUTPUT_DIR}/$FILE
-  printf "%s | %d | %s\n" "$FILE_INDEX" "$OUTPUT_FILE" "${WEBSITES[FILE_INDEX]}"
-  wget --output-document=$OUTPUT_FILE ${WEBSITES[COUNTER]}
-  cat $OUTPUT_FILE >> $OUTPUT_DIR/nnNnH.fastq.gz
-  rm $OUTPUT_FILE
-  ((FILE_INDEX++))
-done 
-
-gunzip $OUTPUT_DIR/nnNnH.fastq.gz
-#echo "gunzip $OUTPUT_DIR/nnNnH.fastq.gz"
diff --git a/next_generation_sequencing/deprecatedCode/001_ngs-functions.R b/next_generation_sequencing/deprecatedCode/001_ngs-functions.R
deleted file mode 100755
index 81e7458..0000000
--- a/next_generation_sequencing/deprecatedCode/001_ngs-functions.R
+++ /dev/null
@@ -1,161 +0,0 @@
-#Functions used to create a pipeline for Next-Generation Sequencing analysis. 
-#Includes filter and trim function, align, sort and index function. 
-#Designed to be used with a list of files and the destination files for the output files of each function. 
-#Also meant to be used with mapply to process multiple files continuously.
-
-#Filter and trim function that requires the file to be processed (filelocation) and the file to be outputed
-#Destination file (destination_file) is created if it is not missing. 
-#Uses QuasR and ShortRead packages. Outputs uncompressed fastq files.
-
-myFilterAndTrim <- function(filelocation, destination_file, folder = './'){
-  # #check and install QuasR and ShortRead packages if not installed.
-  # if (!require(QuasR) | !require(ShortRead)) {
-  #   install.packages('QuasR', 'ShortRead')
-  # }
-  # 
-  # #Load libraries that are used in function 
-  # library(QuasR)
-  # library(ShortRead)
-  
-  ##Create destination file, by default it is the current working directory
-  #If it wasn't supplied, create the destination file based on which folder was provided 
-  #Create based on whether the folder is the working directory or not
-  if (missing(destination_file)){
-    if (folder == './'){
-      destination_file <- paste('./', sprintf("processed_%s", basename(filelocation )), sep = "")
-    } else {
-      destination_file <- paste(folder, sprintf("/processed_%s", basename(filelocation )), sep = "")
-    }
-  } 
-    
-  ## open input stream
-  stream <- open(FastqStreamer(filelocation))
-  on.exit(close(stream))
-  
-  repeat {
-    ## input chunk
-    tmp_fq <- yield(stream)
-    
-    #If input is 0, stop the function
-    if (length(tmp_fq) == 0){
-      print("Processed all")
-      break
-    }
-    
-    
-    ##Trim reads that contain 'N'
-    tmp_fq <- clean(tmp_fq)
-    ##Filter duplicated reads 
-    tmp_fq <- tmp_fq[!srduplicated(tmp_fq)]
-    ##Trim reads
-    tmp_fq <- trimEnds(tmp_fq, a ="5")
-    ##Calculate meanq_reads to obtain reads with mean quality larger than 20
-    meanq_reads <- rowSums(as(quality(tmp_fq), "matrix"), na.rm = TRUE)/width(tmp_fq)
-    #meanq_reads[is.na(meanq_reads)] <- 0]
-    #Had to include a filter for NA values. Not sure why some of them have it. 
-    tmp_fq <- tmp_fq[meanq_reads[!is.na(meanq_reads)] >= 20]
-    
-    ## append to destination
-    ##Do not compress, output quality as FastqQuality 
-    writeFastq(tmp_fq, destination_file, "a", compress = FALSE , qualityType = "FastqQuality")
-  }
-  
-  
-}
-
-#Includes an align sort and index function 
-#Must provide most of the parameters required by bowtie2_samtools. Stops if they are not explicitly provided.
-#Sorts and indexes files by default!
-#Uses Rbowtie2 but also requires stringr.
-
-alignSortandIndex <- function(index_dir, index_prefix, fastq_file, alignment_file, alignment_params, output_type = ".bam", sort_and_index=TRUE){
-  #check and install QuasR and ShortRead packages if not installed.
-  # if (!require(Rbowtie2)|!require(stringr)) {
-  #   print("Rbowtie2 or stringr not installed. Installing with install.packages(c('Rbowtie2', 'stringr'))")
-  #   install.packages(c('Rbowtie2','stringr'))
-  # }
-  
-  #Load libraries that are used in function 
-  # library(Rbowtie2)
-  # library(stringr)
-  
-  #Check for input parameters.
-  if (missing(index_dir)){
-    stop("Execution stopped. Please provide diretory for index")
-  } 
-  if(missing(index_prefix)){
-    stop("Execution stopped. Please provide index prefix")
-  }
-  if(missing(alignment_file)){
-    stop("Execution stopped. Please provide alignment file path. If sam, specify output_type as .sam")
-  }
-  if(missing(alignment_params)){
-    stop("Execution stopped. Please provide alignment parameters as a string.")
-  }
-  
-  #Check that alignment file path and output_type are same
-  if (!(paste(".", tools::file_ext(alignment_file),sep = "") == output_type)){
-    stop("Execution stopped. Alignment file and output type is different. Please make sure alignment ends with .bam or .fastq")
-  }
-  
-  #Align the files using bowtie2_samtools to output as BAM or SAM. Uses Rsamtools if samtools is not installed.
-  bowtie2_samtools(bt2Index = file.path(tools::file_path_as_absolute(index_dir), index_prefix),
-                                      output = str_replace(alignment_file, pattern = output_type, replacement = ""),
-                                      outputType = str_replace(output_type, pattern = ".", replacement = ""),
-                                      seq1 = fastq_file,
-                                      seq2 = NULL,
-                                      bamFile = NULL,
-                                      alignment_params,
-                                      overwrite = TRUE)
-  
-  #Sort and Index the created bam files for further analysis. Print if file is not to be indexed and sorted.
-  if(sort_and_index){
-    print("Sorting and Indexing")
-    sortBam(alignment_file, destination = paste(tools::file_path_sans_ext(alignment_file), "_sorted", sep = ''))
-    indexBam(paste(tools::file_path_sans_ext(alignment_file), "_sorted.bam", sep = ''))
-  } else {
-    print(paste(paste(tools::file_path_sans_ext(alignment_file), "_sorted.bam", sep = ''), "not sorted and indexed."))
-  }
-  
-}
-
-#Reads in using Rsamtools functions a bam file by chunking through the chromosomes
-#Provide chromosome name and chromosome size
-#Can modify the ScanBamParam to read other ways
-bamReadPosAndQwidthByChromToRle <- function(chrom_name, bam_file, chromosome_size, ...)
-{
-  #Commented message used for diagnostic purposes
-  #print(paste("Scanning", chrom_name, "Length is", chromosome_size, sep = " "))
-  #Define parameters to read. See SAM manual for details. POS is first position. QWidth is length of read essentially
-  param <- ScanBamParam(
-    what = c('pos', 'qwidth'),
-    which = GRanges(chrom_name, IRanges(1, chromosome_size)),
-    flag = scanBamFlag(isUnmappedQuery = FALSE)
-  )
-  #Read in the bam file according to param
-  x <- scanBam(bam_file, ..., param = param)[[1]]
-  #Get coverage of an IRanges object constructed using  pos and width
-  coverage(IRanges(x[["pos"]], width = x[["qwidth"]]))
-}
-#ckage_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead")
- BASH_SECTION
-#Rcript -e 'source("/home/luised94/data/rscripts/3-assign-directory-variables.r")'#Simple function that returns the time in year,month,date,hour,minute,second with underscores in between. Add to files names 
-underscoreDate <- function(){
-  return(stringr::str_replace_all(Sys.time(), pattern = ":| |-", replacement="_"))
-}
-
-loadPackages <- function(package_list){
-  package_was_loaded <- unlist(suppressPackageStartupMessages(lapply(package_list, library, character.only = TRUE, logical.return=TRUE, quietly = TRUE)))
-  if (length(lapply(package_list[!package_was_loaded], function(x){
-    paste(x, "Package did not install")
-  })) == 0) print("All packages loaded.")
-}
-
-assignChrTracks <- function(genome_df, index_, feature_df, range_){
-  assign(genome_df$gr_var[index_], GRanges(seqnames=genome_df$chrom[index_], ranges = IRanges(start= 1, end = genome_df$size[index_]), strand = "*"), envir = .GlobalEnv)
-  assign(genome_df$chr_df[index_], feature_df %>% filter(Chromosome == index_) %>% data.frame(), envir = .GlobalEnv)
-  start <- get(genome_df$chr_df[index_]) %>% .$Position - range_
-  end <- get(genome_df$chr_df[index_]) %>% .$Position + range_
-  assign(genome_df$origin_gr_var[index_], GRanges(seqnames = genome_df$chrom[index_], ranges = IRanges(start = start, end = end), strand = "*"), envir = .GlobalEnv)
-  assign(genome_df$origin_track_var[index_], AnnotationTrack(get(genome_df$origin_gr_var[index_]), name=paste(sacCer3_df$chrom[index_], "Origins", sep = " ")), envir = .GlobalEnv)
-}
diff --git a/next_generation_sequencing/deprecatedCode/002_prepare-samples-dataframe.R b/next_generation_sequencing/deprecatedCode/002_prepare-samples-dataframe.R
deleted file mode 100755
index 8dda4c0..0000000
--- a/next_generation_sequencing/deprecatedCode/002_prepare-samples-dataframe.R
+++ /dev/null
@@ -1,42 +0,0 @@
-#Gets relative paths for the directories in data folder and assigns them to variables
-#Based on the operating system since the relative paths are different. 
-if(grepl("Windows", osVersion)){
-  source("./scripts/assign-directory-variables.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/assign-directory-variables.R")
-}
-
-#Prepare the dataframe with the input/output files ----
-
-#Get the genome index directory
-index_dir <- list.dirs(genome_directory)[grepl("index", list.dirs(genome_directory))]
-
-#Get all fastq files in fastq_directory obtained from assign-directory-variable.R 
-fastq_file_list <- list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE)[!grepl("unmapped", list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE))]
-
-#Create variable for genome
-genome_file_path <- list.files(path = genome_directory, pattern = ".fasta", full.names = TRUE)
-
-#Obtain the sample name for each fastq file
-sample_name <- str_replace(basename(fastq_file_list), pattern = paste(".", tools::file_ext(fastq_file_list), sep = ""), replacement = "")   
-
-#Create the dataframe that has all paths required to preprocessing and alignment. 
-sample_paths_df <- mapply(function(dir_, ext_){
-  #Create the name for each file depending on the folder it will be outputted to. 
-  if (grepl("processed", dir_)){
-    paste(dir_, paste("processed_", basename(sample_name), ext_, sep = ""), sep = "/")
-  } else if (!grepl("genome|fastqc", dir_)){
-    paste(dir_, paste(basename(sample_name), ext_, sep = ""), sep = "/")
-  } 
-}, dir_list, file_extension) %>% discard(is.null) %>% data.frame(sample_names = sample_name, original_file = fastq_file_list, .)
-#dim(sample_paths_df)
-
-#Add columns for variables that can be used to assign different data types 
-sample_paths_df <- sample_paths_df %>% mutate(fq_var = paste(sample_names, "_fq", sep = ""),
-                                              bw_var = paste(sample_names, "_bw", sep = ""),
-                                              track_var = paste(sample_names, "_track", sep = ""))
-#Add the columns for sorted bam files and their indexes 
-sample_paths_df <- sample_paths_df %>% mutate(sorted_bam = paste(tools::file_path_sans_ext(..data.alignment.files), "_sorted.bam", sep = ''),
-                                              bam_index = paste(tools::file_path_sans_ext(..data.alignment.files), "_sorted.bam.bai", sep = ''))
-
-print(paste("Dataframe with paths and variables to all files created", Sys.time(), sep = " "))
diff --git a/next_generation_sequencing/deprecatedCode/003_assign-directory-variables.R b/next_generation_sequencing/deprecatedCode/003_assign-directory-variables.R
deleted file mode 100755
index 693bbb2..0000000
--- a/next_generation_sequencing/deprecatedCode/003_assign-directory-variables.R
+++ /dev/null
@@ -1,44 +0,0 @@
-#Separate file to source for assigning variables instead of writing it each time.
-#Assigning directory variables ----
-suppressPackageStartupMessages(library(tidyverse, quietly = TRUE))
-#Folder to get directories for. 
-data_folder <- "./data"
-dir_names <- stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = data_folder, replace = "")
-
-#Create the variables that will be assign the directory locations
-#Some processing has to be done based on how they were created. Remove ./data/, -files and replace and - with _
-dir_variables <- paste(paste(stringr::str_replace(stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = "./data/", replace = ""), pattern = "-files", replace=""), "_directory", sep = ""))
-dir_variables <- stringr::str_replace(dir_variables, pattern = "-", "_")                       
-#Create the data frame to iterate through
-dir_df <- data.frame(names = dir_names, variables = dir_variables)
-
-#Initialize file extensions in same order as dir_variables based on initial script
-file_extension <- c(".bam", ".bw", ".fastq", ".html", ".fasta", ".bed", ".fastq")           
-
-#Create the variables depending on whether they exist or not 
-if (!all(unlist(lapply(dir_variables, exists)))) {
-  
-  dir_list <- c()
-  #For all of the directories, create the string variable with the path, assign it and add it to dir list 
-  for (i in 1:length(dir_df$names)){
-    
-    folder_variable <- paste(data_folder, dir_df$names[i], sep = "")
-    assign(dir_df$variables[i], folder_variable)
-    dir_list <- append(dir_list, folder_variable)
-  }
-  
-  print("Directory variables created.")
-  
-} else {
-  
-  print("All variables assigned.")
-}
-
-print(paste("Directory variables assigned", Sys.time(), sep = " "))
-
-#Read in functions used for other scripts.
-if(grepl("Windows", osVersion)){
-  source("./scripts/ngs-functions.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/ngs-functions.R")
-} 
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/004_assign-directory-variables.sh b/next_generation_sequencing/deprecatedCode/004_assign-directory-variables.sh
deleted file mode 100755
index 243c6d9..0000000
--- a/next_generation_sequencing/deprecatedCode/004_assign-directory-variables.sh
+++ /dev/null
@@ -1,18 +0,0 @@
-#!/bin/bash
-echo Working directory is $(pwd)
-
-module purge
-module add gnu/5.4.0
-module add r/4.2.0
-
-date
-
-cd /home/luised94/data/221024Bel_CHIP
-
-echo Now, working directory is $(pwd)
-
-Rscript -e 'source("/home/luised94/data/rscripts/3-assign-directory-variables.r")'
-
-date
-
-echo Bash script complete
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/005_assign-directory-variables.sh b/next_generation_sequencing/deprecatedCode/005_assign-directory-variables.sh
deleted file mode 100755
index 5baf653..0000000
--- a/next_generation_sequencing/deprecatedCode/005_assign-directory-variables.sh
+++ /dev/null
@@ -1,26 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1                      # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1                      # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=END           # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE 
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE. You must replace [] with your email address.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=90G         # amount of RAM per node
-#############################################
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-cd /home/luised94/data/221024Bel_CHIP
-
-export R_LIBS=/home/luised94/R/x86_64-pc-linux-gnu-library/4.2
-
-Rscript --vanilla /home/luised94/data/rscripts/regenerate-comparison-coverage-plots.R $1 $2 > /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 2>&1
-
-cat /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout >> /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.out
-
-rm /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 
-
-cd /home/luised94/data/rscripts
-
-find . -maxdepth 1 -name 'slurm*' -delete
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/006_sbatch-generate-comparison-plots.sh b/next_generation_sequencing/deprecatedCode/006_sbatch-generate-comparison-plots.sh
deleted file mode 100755
index 95d112c..0000000
--- a/next_generation_sequencing/deprecatedCode/006_sbatch-generate-comparison-plots.sh
+++ /dev/null
@@ -1,26 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1                      # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1                      # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=END           # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE 
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE. You must replace [] with your email address.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=90G         # amount of RAM per node
-#############################################
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-cd /home/luised94/data/221024Bel_CHIP
-
-export R_LIBS=/home/luised94/R/x86_64-pc-linux-gnu-library/4.2
-
-Rscript --vanilla /home/luised94/data/rscripts/generate-comparison-coverage-plots.R $1 > /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 2>&1
-
-cat /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout >> /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.out
-
-rm /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 
-
-cd /home/luised94/data/rscripts
-
-find . -maxdepth 1 -name 'slurm*' -delete
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/007_sbatch-process-and-align.sh b/next_generation_sequencing/deprecatedCode/007_sbatch-process-and-align.sh
deleted file mode 100755
index 8597c9f..0000000
--- a/next_generation_sequencing/deprecatedCode/007_sbatch-process-and-align.sh
+++ /dev/null
@@ -1,24 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1                      # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1                      # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE 
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE. You must replace [] with your email address.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=20G         # amount of RAM per node
-#############################################
-
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-cd /home/luised94/data/221024Bel_CHIP
-
-export R_LIBS=/home/luised94/R/x86_64-pc-linux-gnu-library/4.2
-
-Rscript --vanilla /home/luised94/data/rscripts/process-and-align.R > /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 2>&1
-
-cat /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout >> /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.out
-
-rm /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout
-
diff --git a/next_generation_sequencing/deprecatedCode/008_sbatch-quality-control.sh b/next_generation_sequencing/deprecatedCode/008_sbatch-quality-control.sh
deleted file mode 100755
index 7406567..0000000
--- a/next_generation_sequencing/deprecatedCode/008_sbatch-quality-control.sh
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1                      # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1                      # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE 
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE. You must replace [] with your email address.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=90G         # amount of RAM per node
-#############################################
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-cd /home/luised94/data/221024Bel_CHIP
-
-export R_LIBS=/home/luised94/R/x86_64-pc-linux-gnu-library/4.2
-
-Rscript --vanilla /home/luised94/data/rscripts/quality-control.R > /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 2>&1
-
-cat /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout >> /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.out
-
-rm /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/009_sbatch-visualize-control-files.sh b/next_generation_sequencing/deprecatedCode/009_sbatch-visualize-control-files.sh
deleted file mode 100755
index fbb16de..0000000
--- a/next_generation_sequencing/deprecatedCode/009_sbatch-visualize-control-files.sh
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/bin/bash
-#SBATCH -N 1                      # Number of nodes. You must always set -N 1 unless you receive special instruction from the system admin
-#SBATCH -n 1                      # Number of tasks. Don't specify more than 16 unless approved by the system admin
-#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL. Equivalent to the -m option in SGE 
-#SBATCH --mail-user=luised94@mit.edu  # Email to which notifications will be sent. Equivalent to the -M option in SGE. You must replace [] with your email address.
-#SBATCH --exclude=c[5-22]
-#SBATCH --mem-per-cpu=90G         # amount of RAM per node
-#############################################
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-cd /home/luised94/data/221024Bel_CHIP
-
-export R_LIBS=/home/luised94/R/x86_64-pc-linux-gnu-library/4.2
-
-Rscript --vanilla /home/luised94/data/rscripts/visualize-control-files.R > /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 2>&1
-
-cat /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout >> /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.out
-
-rm /home/luised94/data/221024Bel_CHIP/logs/slurm-${SLURM_JOBID}.Rout 
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/010_quality-control.R b/next_generation_sequencing/deprecatedCode/010_quality-control.R
deleted file mode 100755
index 84aed35..0000000
--- a/next_generation_sequencing/deprecatedCode/010_quality-control.R
+++ /dev/null
@@ -1,160 +0,0 @@
-#Get time at start of script ----
-start_date <- Sys.time()
-start_time <- as.character(stringr::str_replace_all(start_date, pattern = ":| |-", replacement="_"))
-print(paste("Script start:", start_date, sep = " "))
-
-#Prepares directory variables and the dataframe that contains paths, connections and variables to the fastq samples ----
-#This script sources assign-directory-variables.R, which in turn also sources ngs-functions.R
-if(grepl("Windows", osVersion)){
-  source("./scripts/prepare-samples-dataframe.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/prepare-samples-dataframe.R")
-}
-
-package_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead","Rsamtools","Rqc")
-loadPackages(package_to_check) #Load packages and output message when done or if any package didnt load. Takes list of characters.
-
-#Read in feature file to get sacCer3_df then delete timing variables
-df_names <- c("timing")
-
-if(grepl("Windows", osVersion)){
-  source("./scripts/readin-feature-files.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/readin-feature-files.R")
-}
-rm("timing")
-
-#Read in tab-delimited text file with info for each sample. Made in ./scripts/fastq-bmc-info-dataframe.R -----
-if(grepl("Windows", osVersion)){
-  fastq_info <- read.delim("./scripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-} else if (grepl("Linux", osVersion)){
-  fastq_info <- read.delim("../rscripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-}
-if (grepl("Linux", osVersion)) {
-  fastq_info <- bind_cols(fastq_info, sample_paths_df)
-}
-
-#Turn character columns into factors to reorder according to the levels variable.
-#Also use for plotting on Gviz. 
-#Columns to leave as characters. Then select the columns to be turned into factors
-columns_to_exclude <- c("BMC", "Index", "V2", "V3","sname", "Sample_names", "..data","original","track","fq", "bw","sorted")
-character_to_factor <- fastq_info %>% select_if(is.character) %>% dplyr::select(-contains(columns_to_exclude)) %>% colnames(.)
-
-#Use to reorder the columns. Does not generalize!!! 
-order_of_columns <- list(c(1,3,2), c(3,1,2,4), c(1:2),c(1:2),c(2,1,3,5,4),c(3,4,2,1),c(1:3))
-
-invisible(mapply(function(char_to_factor, order_for_columns){
-  #Get the reordered levels. Determined by watching the original order. 
-  reorder_levels <- levels(factor(as.factor(fastq_info[, char_to_factor])))[order_for_columns]
-  fastq_info[, char_to_factor] <<- factor(as.factor(fastq_info[, char_to_factor]), levels = reorder_levels)
-}, char_to_factor = character_to_factor, order_for_columns = order_of_columns, SIMPLIFY = FALSE))
-
-if(fastq_info %>% select_if(is.factor) %>% colnames(.) %>% length() == length(character_to_factor)){
-  print("Columns refactored")
-}
-
-#Use Rsamtools package to count mapped, unmapped and secondary alignments ----
-unmappedparam <- ScanBamParam(flag = scanBamFlag(isUnmappedQuery = TRUE))
-mappedparam <- ScanBamParam(flag = scanBamFlag(isUnmappedQuery = FALSE))
-secondaryAlignmentparam <-ScanBamParam(flag = scanBamFlag(isSecondaryAlignment = TRUE))
-totalparam <- ScanBamParam()
-paramlist <- c(unmappedparam,mappedparam,secondaryAlignmentparam,totalparam)
-# scanBamWhat()[c(5:7,13)]
-
-bam_stats <- as.data.frame(t(as.data.frame(lapply(sample_paths_df$sorted_bam, function(bam_file){
-  unlist(lapply(paramlist, function(param){
-    countBam(bam_file, param = param)$records
-  }))
-}))), row.names = FALSE) %>% rename("V1" = "Unmapped", "V2" = "Mapped", "V3" = "Secondary", "V4" = "Total") %>% mutate(across(1:2, ~ .x/Total, .names = "percent.{.col}"))
-
-#Use ShortRead to get statistics for fastq files ----
-countFastq(sample_paths_df$..data.processed.fastq.files)$records
-
-stream <- open(FastqStreamer(sample_paths_df$..data.fastq.files[1]))
-on.exit(close(stream))
-tmp_fq <- yield(stream)
-
-#If input is 0, stop the function
-if (length(tmp_fq) == 0){
-  print("Processed all")
-  break
-}
-
-length(tmp_fq)
-length(clean(tmp_fq))
-length(tmp_fq[!srduplicated(tmp_fq)])
-rowSums(as(quality(tmp_fq), "matrix"))/width(tmp_fq)
-
-width(trimEnds(tmp_fq, a ="5"))
-##Calculate meanq_reads to obtain reads with mean quality larger than 20
-meanq_reads <- rowSums(as(quality(tmp_fq), "matrix"), na.rm = TRUE)/width(tmp_fq)
-#meanq_reads[is.na(meanq_reads)] <- 0]
-#Had to include a filter for NA values. Not sure why some of them have it. 
-tmp_fq <- tmp_fq[meanq_reads[!is.na(meanq_reads)] >= 20]
-
-
-repeat {
-  ## input chunk
-  tmp_fq <- yield(stream)
-  
-  #If input is 0, stop the function
-  if (length(tmp_fq) == 0){
-    print("Processed all")
-    break
-  }
-  ##Trim reads that contain 'N'
-  tmp_fq <- clean(tmp_fq)
-  ##Filter duplicated reads 
-  tmp_fq <- tmp_fq[!srduplicated(tmp_fq)]
-  ##Trim reads
-  tmp_fq <- trimEnds(tmp_fq, a ="5")
-  ##Calculate meanq_reads to obtain reads with mean quality larger than 20
-  meanq_reads <- rowSums(as(quality(tmp_fq), "matrix"), na.rm = TRUE)/width(tmp_fq)
-  #meanq_reads[is.na(meanq_reads)] <- 0]
-  #Had to include a filter for NA values. Not sure why some of them have it. 
-  tmp_fq <- tmp_fq[meanq_reads[!is.na(meanq_reads)] >= 20]
-  
-  ## append to destination
-  ##Do not compress, output quality as FastqQuality 
-  # writeFastq(tmp_fq, destination_file, "a", compress = FALSE , qualityType = "FastqQuality")
-} 
-
-closeAllConnections()
-print("Script ended. Time Elapsed:", difftime(start_date, Sys.time(), units = "mins"))
-  #Run on cluster command ----
-  #$sbatch sbatch-quality-control.sh
-  # Attempt to use rqc package failed too much. Thought it was due to memory issues but unclear. Wed Nov 16 19:23:37 2022 ------------------------------
-  # See slurm-7275251.out to see last attempt.   
-  #Run rqc on processed fastq files by several groups 
-  #Use lapply and mapply to run rqcQA on each sample with group info, combine results and output report for each group
-  
-  #Used commented lines to test on my toy data ----
-  # sample_paths_df$pools <- unlist(c(rep("A", 5), rep("B", 4)))
-  # sample_paths_df$groups <- unlist(c(rep("potato", 4), rep("corn", 5)))
-  # groups_for_rqc <- c("pools","groups")
-  # 
-  # #Strings that contain part of column to subset by 
-  # groups_for_rqc <- c("Sample","Pool")
-  # 
-  # #For each of the columns in groups_for_rqc, grab the column name it specifies
-  # column_names_for_rqc <- unlist(lapply(groups_for_rqc, function(x) {
-  #   names(fastq_info)[grepl(x, names(fastq_info))]
-  # }))
-  # 
-  # #Apply rqcQA function to each sample and its group in processed.fastq.files for each group that we want. Use mapply inside lapply
-  # lapply(column_names_for_rqc, function(column_name, fastq_files, dataframe_with_groups){
-  #   #Subset dataframe using column name
-  #   group_info <- dataframe_with_groups[, column_name]
-  #   print(paste("Rqc by", column_name, sep = " "))
-  #   
-  #   #rqcQA on each fastq file and its group value. To get group value, subset dataframe with column name 
-  #   qcresults <- mapply(function(fastq_files_for_rqc, rqc_groups) {
-  #     rqcQA(x = fastq_files_for_rqc, sample = FALSE, group = rqc_groups, workers = 1)
-  #   }, fastq_files_for_rqc = fastq_files, rqc_groups = group_info, SIMPLIFY = FALSE)
-  #   
-  #   rqcReport(qcresults, outdir = './reports/reports-files', file = paste("rqc_report_by_",column_name, sep = ""))
-  #   
-  # }, fastq_files = sample_paths_df$..data.processed.fastq.files, dataframe_with_groups = fastq_info)
-  
-  
-  # %>% mutate(total = rowSums(across(1:3), na.rm = T))
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/011_project-startup.R b/next_generation_sequencing/deprecatedCode/011_project-startup.R
deleted file mode 100755
index c78e8ca..0000000
--- a/next_generation_sequencing/deprecatedCode/011_project-startup.R
+++ /dev/null
@@ -1,69 +0,0 @@
-#Initial project setup ----
-Sys.time()
-
-#Create directories and subdirectories to store files that will be generated during the analysis.
-#This should be run in the directory that will be used for analysis
-
-#Add this to all subdirectories
-files_suffix <- "-files"
-
-#Create folders that will have data.
-#Can modify this for other types of projects/analysis
-folder_data <- "./data"
-
-ngs_data_folders <- c("fastq", "genome", "peak", "alignment","fastqc","processed-fastq", "bw")
-data_dirs <- c()
-
-if (!dir.exists(folder_data)){
-  dir.create(folder_data)
-  
-  #Create folders for data that will be generated. 
-  lapply(ngs_data_folders, function(x){
-    dir.create(paste(paste(folder_data, x, sep = "/"), files_suffix, sep = ""))
-  })
-  
-  #Store folder names for later use.  
-  data_dirs <- c(data_dirs, paste(paste(folder_data, ngs_data_folders, sep = "/"), files_suffix, sep = ""))
-} else {
-  print(paste(folder_data, "folder exists"))
-}
-
-
-#Create folder that holds code used in the project 
-folder_code <- "./scripts"
-
-code_folders <- c("python", "R", "command-line")
-code_dirs <- c()
-#All projects will have code, so no need to check the name
-if (!dir.exists(folder_code)){
-  dir.create(folder_code)
-  
-  #Create folders for data that will be generated. 
-  lapply(code_folders, function(x){
-    dir.create(paste(paste(folder_code, x, sep = "/"), files_suffix, sep = ""))
-  })
-}
-
-#Create folder to output project reports or figures.
-folder_figures <- "./reports"
-figure_folders <- c("plots", "reports")
-figure_dirs <- c()
-#All projects will have code, so no need to check the name
-if (!dir.exists(folder_figures)){
-  dir.create(folder_figures)
-  
-  #Create folders for data that will be generated. 
-  lapply(figure_folders, function(x){
-    dir.create(paste(paste(folder_figures, x, sep = "/"), files_suffix, sep = ""))
-  })
-}
-
-log_folder <- "./logs"
-if (!dir.exists(log_folder)) {
-  dir.create(log_folder)
-}
-
-print("Folders created")
-
-
-
diff --git a/next_generation_sequencing/deprecatedCode/012_make-directories.sh b/next_generation_sequencing/deprecatedCode/012_make-directories.sh
deleted file mode 100755
index 6314987..0000000
--- a/next_generation_sequencing/deprecatedCode/012_make-directories.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/bin/bash
-date
-
-mkdir /home/luised94/data/221024Bel_CHIP /home/luised94/data/rscripts /home/luised94/data/rlibs
-
-
-
diff --git a/next_generation_sequencing/deprecatedCode/012_project-startup.sh b/next_generation_sequencing/deprecatedCode/012_project-startup.sh
deleted file mode 100755
index 57aea0b..0000000
--- a/next_generation_sequencing/deprecatedCode/012_project-startup.sh
+++ /dev/null
@@ -1,17 +0,0 @@
-#!/bin/bash
-
-module purge 
-module add gnu/5.4.0
-module add r/4.2.0
-
-date
-
-cd /home/luised94/data/221024Bel_CHIP
-
-Rscript --vanilla -e 'source("../rscripts/package-installation.R")'
-Rscript --vanilla -e 'source("../rscripts/project-startup.R")'
-
-
-date
-
-echo Bash script complete
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/013_sacCer3-genome.sh b/next_generation_sequencing/deprecatedCode/013_sacCer3-genome.sh
deleted file mode 100755
index e36b58f..0000000
--- a/next_generation_sequencing/deprecatedCode/013_sacCer3-genome.sh
+++ /dev/null
@@ -1,14 +0,0 @@
-#!/bin/bash
-echo Working directory is $(pwd)
-
-module purge
-module add gnu/5.4.0
-module add r/4.2.0
-
-date
-
-cd /home/luised94/data/221024Bel_CHIP
-
-echo Now, working directory is $(pwd)
-
-Rscript -e 'source("/home/luised94/data/rscripts/4-prepare-genome.r")'
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/014_running-R-from-command-line.sh b/next_generation_sequencing/deprecatedCode/014_running-R-from-command-line.sh
deleted file mode 100755
index 1834072..0000000
--- a/next_generation_sequencing/deprecatedCode/014_running-R-from-command-line.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-
-#Running R from command line, use default interpreter 
-Rscript -e 'renv::run("/home/luised94/data/R-scripts/1-package-installation.r", project = "/home/luised94/data/221024Bel_CHIP/")'
-
-
-Rscript -e 'renv::run("/home/luised94/data/R-scripts/2-ngs-project-startup.r", project = "/home/luised94/data/221024Bel_CHIP/")'
-
-Rscript -e 'renv::run("../R-scripts/3-assign-directory-variables.r", project = "/home/luised94/data/221024Bel_CHIP/")'
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/015_name.r b/next_generation_sequencing/deprecatedCode/015_name.r
deleted file mode 100755
index a280810..0000000
--- a/next_generation_sequencing/deprecatedCode/015_name.r
+++ /dev/null
@@ -1,11 +0,0 @@
-# git config --global user.email "you@example.com"
-# git config --global user.name "Your Name"
-
-#Using system to run the git config command to 
-git_config_email <- "git config user.email"
-git_config_name <- "git config user.name"
-email <- "liusmartinez94@gmail.com"
-username <- "Luis"
-paste(git_config_email, email)
-system(paste(git_config_email, email))
-system(paste(git_config_name, username))
diff --git a/next_generation_sequencing/deprecatedCode/016_data.sh b/next_generation_sequencing/deprecatedCode/016_data.sh
deleted file mode 100755
index 8700a9d..0000000
--- a/next_generation_sequencing/deprecatedCode/016_data.sh
+++ /dev/null
@@ -1,23 +0,0 @@
-#!/bin/bash
-pwd
-
-outputfiles=(WT-G2-ORC-rep1.fastq.gz WT-G2-ORC-rep2.fastq.gz)
-websites=(ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz)
-
-COUNTER=0
-for file in ${outputfiles[@]};
-do 
-  echo $file
-  echo $COUNTER 
-  echo ${websites[COUNTER]}
-  wget --output-document=$file ${websites[COUNTER]}
-  cat $file >> nnNnH.fastq.gz
-  rm $file
-  let COUNTER++
-done 
-
-# #Check that file is right size
-# ls -l --block-size=M
-#Do this after files have been renamed
-gunzip nnNnH.fastq.gz
-#cat 221024Bel_CHIP.txt | sed s/"    "/""/g | sed s/":"/""/g | sed s/"  "/" "/g | sed s/" "/"\t"/g 
diff --git a/next_generation_sequencing/deprecatedCode/017_ngs-functions.r b/next_generation_sequencing/deprecatedCode/017_ngs-functions.r
deleted file mode 100755
index ddbb139..0000000
--- a/next_generation_sequencing/deprecatedCode/017_ngs-functions.r
+++ /dev/null
@@ -1,160 +0,0 @@
-#Functions used to create a pipeline for Next-Generation Sequencing analysis. 
-#Includes filter and trim function, align, sort and index function. 
-#Designed to be used with a list of files and the destination files for the output files of each function. 
-#Also meant to be used with mapply to process multiple files continuously.
-
-#Filter and trim function that requires the file to be processed (filelocation) and the file to be outputed
-#Destination file (destination_file) is created if it is not missing. 
-#Uses QuasR and ShortRead packages. Outputs uncompressed fastq files.
-
-myFilterAndTrim <- function(filelocation, destination_file, folder = './'){
-  # #check and install QuasR and ShortRead packages if not installed.
-  # if (!require(QuasR) | !require(ShortRead)) {
-  #   install.packages('QuasR', 'ShortRead')
-  # }
-  # 
-  # #Load libraries that are used in function 
-  # library(QuasR)
-  # library(ShortRead)
-  
-  ##Create destination file, by default it is the current working directory
-  #If it wasn't supplied, create the destination file based on which folder was provided 
-  #Create based on whether the folder is the working directory or not
-  if (missing(destination_file)){
-    if (folder == './'){
-      destination_file <- paste('./', sprintf("processed_%s", basename(filelocation )), sep = "")
-    } else {
-      destination_file <- paste(folder, sprintf("/processed_%s", basename(filelocation )), sep = "")
-    }
-  } 
-    
-  ## open input stream
-  stream <- open(FastqStreamer(filelocation))
-  on.exit(close(stream))
-  
-  repeat {
-    ## input chunk
-    tmp_fq <- yield(stream)
-    
-    #If input is 0, stop the function
-    if (length(tmp_fq) == 0){
-      print("Processed all")
-      break
-    }
-    
-    
-    ##Trim reads that contain 'N'
-    tmp_fq <- clean(tmp_fq)
-    ##Filter duplicated reads 
-    tmp_fq <- tmp_fq[!srduplicated(tmp_fq)]
-    ##Trim reads
-    tmp_fq <- trimEnds(tmp_fq, a ="5")
-    ##Calculate meanq_reads to obtain reads with mean quality larger than 20
-    meanq_reads <- rowSums(as(quality(tmp_fq), "matrix"), na.rm = TRUE)/width(tmp_fq)
-    #meanq_reads[is.na(meanq_reads)] <- 0]
-    #Had to include a filter for NA values. Not sure why some of them have it. 
-    tmp_fq <- tmp_fq[meanq_reads[!is.na(meanq_reads)] >= 20]
-    
-    ## append to destination
-    ##Do not compress, output quality as FastqQuality 
-    writeFastq(tmp_fq, destination_file, "a", compress = FALSE , qualityType = "FastqQuality")
-  }
-  
-  
-}
-
-#Includes an align sort and index function 
-#Must provide most of the parameters required by bowtie2_samtools. Stops if they are not explicitly provided.
-#Sorts and indexes files by default!
-#Uses Rbowtie2 but also requires stringr.
-
-alignSortandIndex <- function(index_dir, index_prefix, fastq_file, alignment_file, alignment_params, output_type = ".bam", sort_and_index=TRUE){
-  #check and install QuasR and ShortRead packages if not installed.
-  # if (!require(Rbowtie2)|!require(stringr)) {
-  #   print("Rbowtie2 or stringr not installed. Installing with install.packages(c('Rbowtie2', 'stringr'))")
-  #   install.packages(c('Rbowtie2','stringr'))
-  # }
-  
-  #Load libraries that are used in function 
-  # library(Rbowtie2)
-  # library(stringr)
-  
-  #Check for input parameters.
-  if (missing(index_dir)){
-    stop("Execution stopped. Please provide diretory for index")
-  } 
-  if(missing(index_prefix)){
-    stop("Execution stopped. Please provide index prefix")
-  }
-  if(missing(alignment_file)){
-    stop("Execution stopped. Please provide alignment file path. If sam, specify output_type as .sam")
-  }
-  if(missing(alignment_params)){
-    stop("Execution stopped. Please provide alignment parameters as a string.")
-  }
-  
-  #Check that alignment file path and output_type are same
-  if (!(paste(".", tools::file_ext(alignment_file),sep = "") == output_type)){
-    stop("Execution stopped. Alignment file and output type is different. Please make sure alignment ends with .bam or .fastq")
-  }
-  
-  #Align the files using bowtie2_samtools to output as BAM or SAM. Uses Rsamtools if samtools is not installed.
-  bowtie2_samtools(bt2Index = file.path(tools::file_path_as_absolute(index_dir), index_prefix),
-                                      output = str_replace(alignment_file, pattern = output_type, replacement = ""),
-                                      outputType = str_replace(output_type, pattern = ".", replacement = ""),
-                                      seq1 = fastq_file,
-                                      seq2 = NULL,
-                                      bamFile = NULL,
-                                      alignment_params,
-                                      overwrite = TRUE)
-  
-  #Sort and Index the created bam files for further analysis. Print if file is not to be indexed and sorted.
-  if(sort_and_index){
-    print("Sorting and Indexing")
-    sortBam(alignment_file, destination = paste(tools::file_path_sans_ext(alignment_file), "_sorted", sep = ''))
-    indexBam(paste(tools::file_path_sans_ext(alignment_file), "_sorted.bam", sep = ''))
-  } else {
-    print(paste(paste(tools::file_path_sans_ext(alignment_file), "_sorted.bam", sep = ''), "not sorted and indexed."))
-  }
-  
-}
-
-#Reads in using Rsamtools functions a bam file by chunking through the chromosomes
-#Provide chromosome name and chromosome size
-#Can modify the ScanBamParam to read other ways
-bamReadPosAndQwidthByChromToRle <- function(chrom_name, bam_file, chromosome_size, ...)
-{
-  #Commented message used for diagnostic purposes
-  #print(paste("Scanning", chrom_name, "Length is", chromosome_size, sep = " "))
-  #Define parameters to read. See SAM manual for details. POS is first position. QWidth is length of read essentially
-  param <- ScanBamParam(
-    what = c('pos', 'qwidth'),
-    which = GRanges(chrom_name, IRanges(1, chromosome_size)),
-    flag = scanBamFlag(isUnmappedQuery = FALSE)
-  )
-  #Read in the bam file according to param
-  x <- scanBam(bam_file, ..., param = param)[[1]]
-  #Get coverage of an IRanges object constructed using  pos and width
-  coverage(IRanges(x[["pos"]], width = x[["qwidth"]]))
-}
-
-#Simple function that returns the time in year,month,date,hour,minute,second with underscores in between. Add to files names 
-underscoreDate <- function(){
-  return(stringr::str_replace_all(Sys.time(), pattern = ":| |-", replacement="_"))
-}
-
-loadPackages <- function(package_list){
-  package_was_loaded <- unlist(suppressPackageStartupMessages(lapply(package_list, library, character.only = TRUE, logical.return=TRUE, quietly = TRUE)))
-  if (length(lapply(package_list[!package_was_loaded], function(x){
-    paste(x, "Package did not install")
-  })) == 0) print("All packages loaded.")
-}
-
-assignChrTracks <- function(genome_df, index_, feature_df, range_){
-  assign(genome_df$gr_var[index_], GRanges(seqnames=genome_df$chrom[index_], ranges = IRanges(start= 1, end = genome_df$size[index_]), strand = "*"), envir = .GlobalEnv)
-  assign(genome_df$chr_df[index_], feature_df %>% filter(Chromosome == index_) %>% data.frame(), envir = .GlobalEnv)
-  start <- get(genome_df$chr_df[index_]) %>% .$Position - range_
-  end <- get(genome_df$chr_df[index_]) %>% .$Position + range_
-  assign(genome_df$origin_gr_var[index_], GRanges(seqnames = genome_df$chrom[index_], ranges = IRanges(start = start, end = end), strand = "*"), envir = .GlobalEnv)
-  assign(genome_df$origin_track_var[index_], AnnotationTrack(get(genome_df$origin_gr_var[index_]), name=paste(sacCer3_df$chrom[index_], "Origins", sep = " ")), envir = .GlobalEnv)
-}
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/018_scanned_rsync_with_wsl_and_R.r b/next_generation_sequencing/deprecatedCode/018_scanned_rsync_with_wsl_and_R.r
deleted file mode 100755
index bd9b4cd..0000000
--- a/next_generation_sequencing/deprecatedCode/018_scanned_rsync_with_wsl_and_R.r
+++ /dev/null
@@ -1,72 +0,0 @@
-# Running rsync through R ----
-#The following requires WSL to be installed on Windows computer 
-#
-# dir_to_rsync_from  <- "../ngs-pipeline/scripts/R-files/*"
-# rsync_dest <- "./scripts/"
-# rsync_command_dry <- 'wsl rsync --stats -nv'
-# rsync_command <- 'wsl rsync --stats -v'
-#
-# #Check the string
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-#
-# #Check the command output
-# system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Transfer if it looks good
-# system(paste(rsync_command, dir_to_rsync_from, rsync_dest))
-
-# #Transfer files to luria server 
-# rsync_command_dry <- 'wsl rsync --stats -nvr'
-# rsync_command <- 'wsl rsync --stats -vr'
-# dir_to_rsync_from  <- stringr::str_replace(paste0(getwd(), "/scripts/"), pattern = "C:", replacement = "/mnt/c")
-# rsync_dest <- "luised94@luria.mit.edu:/home/luised94/data/R-scripts/"
-# #For this you can run from command line. It requires password. Just paste output from R console to the terminel
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-# # system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Run rsync on terminal ----
-#Sync to dropbox
-wsl rsync --stats -nrv /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-#Rsync my Zotero folder
-wsl rsync -nrv /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-wsl rsync -nrv --update /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-
-#Inverted the order to sync from the cluster to the local machine
-wsl rsync --stats -nvr --update luised94@luria.mit.edu:/home/luised94/data/R-scripts/* /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/cluster-modified/
-
-#Transfering and updating to luria
-wsl rsync --stats -nv --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/* luised94@luria.mit.edu:/home/luised94/data/rscripts/
-
-#Count the lines that have fastq in it. 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files | grep -e ".fastq"
-
-find -type d -name "*fastq*" | grep "/f"
-
-
-#Run for files after rsync ----
-find -type f -exec dos2unix -k -o {} \;
-find -type f -exec chmod +x {} \;
-#----
-#Downloading Eaton data 
-wget --output-document=./fastq-files/WT-G2-ORC-rep1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz
-wget --output-document=./fastq-files/WT-G2-ORC-rep2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz 
-
-#Download BMC data 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files
-
-#Move the folder content up one directory
-mv 221024Bel/* . 
-rmdir 221024Bel
-
-
-sacct -A luised94 --format=JobName,Account,AllocNodes,AllocCPUs,AllocTres,AveDiskRead,Elapsed,JobID,ReqMem | awk '$1~/sbatch/'
-
-test_var="$(echo date +%g_%m_%d_%H_%M_%S)"
-${test_var}
-
-echo Execution Time $(${test_var}) > test.out
-
-sed -i '2 i '"$(echo Execution Time $($test_var))"'' test.out
-
-
-myInvocation="$(printf %q "$BASH_SOURCE")$((($#)) && printf ' %q' "$@")"
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/019_scanned_rsync_with_wsl_and_R.sh b/next_generation_sequencing/deprecatedCode/019_scanned_rsync_with_wsl_and_R.sh
deleted file mode 100755
index da951e1..0000000
--- a/next_generation_sequencing/deprecatedCode/019_scanned_rsync_with_wsl_and_R.sh
+++ /dev/null
@@ -1,85 +0,0 @@
-# Running rsync through R ----
-#The following requires WSL to be installed on Windows computer 
-#
-# dir_to_rsync_from  <- "../ngs-pipeline/scripts/R-files/*"
-# rsync_dest <- "./scripts/"
-# rsync_command_dry <- 'wsl rsync --stats -nv'
-# rsync_command <- 'wsl rsync --stats -v'
-#
-# #Check the string
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-#
-# #Check the command output
-# system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-
-#Transfer if it looks good
-# system(paste(rsync_command, dir_to_rsync_from, rsync_dest))
-
-# #Transfer files to luria server 
-# rsync_command_dry <- 'wsl rsync --stats -nvr'
-# rsync_command <- 'wsl rsync --stats -vr'
-# dir_to_rsync_from  <- stringr::str_replace(paste0(getwd(), "/scripts/"), pattern = "C:", replacement = "/mnt/c")
-# rsync_dest <- "luised94@luria.mit.edu:/home/luised94/data/R-scripts/"
-# #For this you can run from command line. It requires password. Just paste output from R console to the terminel
-# paste(rsync_command_dry, dir_to_rsync_from, rsync_dest)
-# # system(paste(rsync_command_dry, dir_to_rsync_from, rsync_dest))
-#Download files using svn. Downloads the folder to current working directory
-wsl svn export https://github.com/CEGRcode/2021-Rossi_Nature.git/trunk/02_References_and_Features_Files
-
-#Run rsync on terminal ----
-#Sync to dropbox
-wsl rsync --stats -nrv /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-wsl rsync --stats -nrv --update /mnt/c/Users/Luis/Projects/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-
-#Just the scripts folder that I work on most 
-wsl rsync --stats -nrv --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/* '/mnt/c/Users/Luis/Dropbox (MIT)/Lab/Code/Projects/'
-
-#Rsync my Zotero folder
-wsl rsync -nrv /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-wsl rsync -nrv --update /mnt/c/Users/Luis/Zotero/* '/mnt/c/Users/Luis/Dropbox (MIT)/Zotero/'
-
-#Transfering and updating to luria
-wsl rsync --stats -nv --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/scripts/* luised94@luria.mit.edu:/home/luised94/data/rscripts/
-
-#Inverted the order to sync from the cluster to the local machine
-wsl rsync --stats -nvr --update luised94@luria.mit.edu:/home/luised94/data/221024Bel_CHIP/reports/plots-files/* /mnt/c/Users/Luis/Projects/working-on-a-cluster/reports-and-figures/plots-files/
-wsl rsync --stats -nvr --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/reports-and-figures/plots-files/* luised94@luria.mit.edu:/home/luised94/data/221024Bel_CHIP/reports/plots-files/
-
-#Count the lines that have fastq in it. 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files | grep -e ".fastq"
-
-find -type d -name "*fastq*" | grep "/f"
-
-
-#Run for files after rsync ----
-find -type f -exec dos2unix -k -o {} \;
-find -type f -exec chmod +x {} \;
-#----
-#Downloading Eaton data 
-wget --output-document=./fastq-files/WT-G2-ORC-rep1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz
-wget --output-document=./fastq-files/WT-G2-ORC-rep2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034476/SRR034476.fastq.gz 
-
-#Download BMC data 
-srun rsync -nav /net/bmc-pub17/data/bmc/public/Bell/221024Bel /home/luised94/data/221024Bel_CHIP/data/fastq-files
-
-#Move the folder content up one directory
-mv 221024Bel/* . 
-rmdir 221024Bel
-
-
-sacct -A luised94 --format=JobName,Account,AllocNodes,AllocCPUs,AllocTres,AveDiskRead,Elapsed,JobID,ReqMem | awk '$1~/sbatch/'
-
-test_var="$(echo date +%g_%m_%d_%H_%M_%S)"
-${test_var}
-
-echo Execution Time $(${test_var}) > test.out
-
-sed -i '2 i '"$(echo Execution Time $($test_var))"'' test.out
-
-ls -lt | grep "slurm" | head -2
-
-ls -lt | grep "slurm" | head -1 | awk '{print $(NF)}'
-
-find . -maxdepth 1 -name 'slurm*' -delete
-
-myInvocation="$(printf %q "$BASH_SOURCE")$((($#)) && printf ' %q' "$@")"
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/020_1-package-installation.r b/next_generation_sequencing/deprecatedCode/020_1-package-installation.r
deleted file mode 100755
index e8dc79c..0000000
--- a/next_generation_sequencing/deprecatedCode/020_1-package-installation.r
+++ /dev/null
@@ -1,81 +0,0 @@
-#https://rdrr.io/category/biocview/GenomeAnnotation/
-#https://uclouvain-cbio.github.io/WSBIM1322/index.html
-#https://compgenomr.github.io/book/
-#https://rockefelleruniversity.github.io/RU_ChIPseq/index.html
-#https://learn.gencore.bio.nyu.edu/
-
-#Loading packages in a clean way. 
-# Install packages that will be used most of the time.
-# constant_packages <- c("renv","pacman")
-# install.packages(constant_packages)
-# 
-# 
-# pacman::p_load(ggplot2, tidyverse,BiocManager,stringr,R.utils)
-renv::install("styler")
-bioinformatics_packages <- c("BiocGenerics","MatrixGenerics",'qvalue','plot3D','ggplot2','pheatmap','cowplot',
-  'cluster', 'NbClust', 'fastICA', 'NMF','matrixStats',
-  'Rtsne', 'mosaic', 'knitr', 'genomation',
-  'ggbio', 'Gviz', 'DESeq2', 'RUVSeq',
-  'gProfileR', 'ggfortify', 'corrplot',
-  'gage', 'EDASeq', 'formatR', 'BiocFileCache',
-  'svglite', 'Rqc', 'ShortRead', 'QuasR',
-  'methylKit','FactoMineR', 'iClusterPlus',
-  'enrichR','caret','xgboost','glmnet',
-  'DALEX','kernlab','pROC','nnet','RANN',
-  'ranger','GenomeInfoDb', 'GenomicRanges',
-  'GenomicAlignments', 'ComplexHeatmap', 'circlize', 
-  'rtracklayer', 'tidyr', 'dplyr',
-  'AnnotationHub', 'GenomicFeatures', 'normr',
-  'MotifDb', 'TFBSTools', 'rGADEM', 'JASPAR2018', 
-  'BSgenome', 'htmltab', 'usethis',
-  'Rsubread', 'Rsamtools', 'Rbowtie', 'Rbowtie2' , "ChIPpeakAnno", 
-  "seqinr","GenomeInfoDbData","BSgenome.Scerevisiae.UCSC.sacCer3")
-
-BiocManager::install(bioinformatics_packages)
-#Cool way to install Stolen/borrowed from https://www.r-bloggers.com/2020/01/an-efficient-way-to-install-and-load-r-packages-2/
-#  
-# packages <- c("ggplot2", "readxl", "dplyr", "tidyr", "ggfortify", "DT", "reshape2", "knitr", "lubridate", "pwr", "psy", "car", "doBy", "imputeMissings", "RcmdrMisc", "questionr", "vcd", "multcomp", "KappaGUI", "rcompanion", "FactoMineR", "factoextra", "corrplot", "ltm", "goeveg", "corrplot", "FSA", "MASS", "scales", "nlme", "psych", "ordinal", "lmtest", "ggpubr", "dslabs", "stringr", "assist", "ggstatsplot", "forcats", "styler", "remedy", "snakecaser", "addinslist", "esquisse", "here", "summarytools", "magrittr", "tidyverse", "funModeling", "pander", "cluster", "abind")
-# 
-# # Install packages not yet installed
-# installed_packages <- packages %in% rownames(installed.packages())
-# if (any(installed_packages == FALSE)) {
-#   install.packages(packages[!installed_packages])
-# }
-
-# 
-# # Packages loading
-# invisible(lapply(packages, library, character.only = TRUE))
-
-
-#May need to change some variables.
-#Run this, after creating virtual environment and directory.
-# #Install packages for documentations and parallel processing 
-# renv::install(c("tidyverse", "R.utils", "doParallel", "snow","fs","rmarkdown", "gt","formattable","ggplot2", "webshot2",
-#               "rmarkdown","xaringan","officer",
-#               "quarto","BiocManager"))
-# #Install some files for development
-# renv::install("devtools","rcrossref","taskscheduleR","bio3d")
-# devtools::install_github("crsh/citr")
-# remotes::install_github("paleolimbot/rbbt")
-
-#Non Bioconductor package 
-#install.packages("formattable")
-
-#Enable SSL on windows: https://answers.microsoft.com/en-us/windows/forum/all/ssl-error-preventing-connection-in-windows-10/192436e5-e37b-4b4c-a4e0-c4ec744b0f5c
-#Then install basilisk.utils
-# BiocManager::install(c("basilisk.utils", "MACSr"))
-# BiocManager::install(c("nanopoRe"))
-# update.packages()
-# lapply(paste0("package:", names(sessionInfo()$otherPkgs)),   # Unload add-on packages
-#        detach,
-#        character.only = TRUE, unload = TRUE)
-
-
-deattachAll <- function(){
-  lapply(paste0("package:", names(sessionInfo()$otherPkgs)),   # Unload add-on packages
-                detach,
-                character.only = TRUE, unload = TRUE)
-}
-
-
-
diff --git a/next_generation_sequencing/deprecatedCode/021_2-ngs-project-startup.r b/next_generation_sequencing/deprecatedCode/021_2-ngs-project-startup.r
deleted file mode 100755
index 3d3909d..0000000
--- a/next_generation_sequencing/deprecatedCode/021_2-ngs-project-startup.r
+++ /dev/null
@@ -1,56 +0,0 @@
-#Please see "./scripts/R-files/1-package-installation.R" to see packages that are required.
-#Add this to all subdirectories
-files_suffix <- "-files"
-
-#Create folders that will have data.
-#Can modify this for other types of projects/analysis
-folder_data <- "./data"
-
-ngs_data_folders <- c("fastq", "genome", "peak", "alignment","fastqc","processed-fastq", "bw")
-data_dirs <- c()
-
-if (!dir.exists(folder_data)){
-  dir.create(folder_data)
-  
-  #Create folders for data that will be generated. 
-  lapply(ngs_data_folders, function(x){
-    dir.create(paste(paste(folder_data, x, sep = "/"), files_suffix, sep = ""))
-  })
-  
-  #Store folder names for later use.  
-  data_dirs <- c(data_dirs, paste(paste(folder_data, ngs_data_folders, sep = "/"), files_suffix, sep = ""))
-} else {
-  print(paste(folder_data, "folder exists"))
-}
-
-
-#Create folder that holds code used in the project 
-folder_code <- "./scripts"
-
-code_folders <- c("python", "R", "command-line")
-code_dirs <- c()
-#All projects will have code, so no need to check the name
-if (!dir.exists(folder_code)){
-  dir.create(folder_code)
-  
-  #Create folders for data that will be generated. 
-  lapply(code_folders, function(x){
-    dir.create(paste(paste(folder_code, x, sep = "/"), files_suffix, sep = ""))
-  })
-}
-
-#Create folder to output project reports or figures.
-folder_figures <- "./reports-and-figures"
-figure_folders <- c("plots", "reports")
-figure_dirs <- c()
-#All projects will have code, so no need to check the name
-if (!dir.exists(folder_figures)){
-  dir.create(folder_figures)
-  
-  #Create folders for data that will be generated. 
-  lapply(figure_folders, function(x){
-    dir.create(paste(paste(folder_figures, x, sep = "/"), files_suffix, sep = ""))
-  })
-}
-
-install.packages(c("tidyverse", "stringr","R.utils"), lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
diff --git a/next_generation_sequencing/deprecatedCode/022_3-assign-directory-variables.r b/next_generation_sequencing/deprecatedCode/022_3-assign-directory-variables.r
deleted file mode 100755
index 207c093..0000000
--- a/next_generation_sequencing/deprecatedCode/022_3-assign-directory-variables.r
+++ /dev/null
@@ -1,75 +0,0 @@
-
-cat(Sys.time())
-
-.libPaths( c( "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2", .libPaths() ) )
-
-.libPaths()
-
-library(tidyverse, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-library(stringr, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-
-#Create folders if not present. Added conditional to run project startup script before
-#assigning variables. Accounts for system OS (cluster or local).
-# if (!dir.exists("./data")){
-#   if(grepl("Windows", osVersion)){
-#     renv::run("./scripts/2-ngs-project-startup.r")
-#   } else if (grepl("Linux", osVersion)){
-#     renv::run("../R-scripts/2-ngs-project-startup.r")
-#   }
-# }
-# source("./scripts/2-ngs-project-startup.R")
-cat(date())
-#Folder to get directories for. 
-data_folder <- "./data"
-dir_names <- stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = data_folder, replace = "")
-data_folder;dir_names
-
-#Create the variables that will be assign the directory locations
-#Some processing has to be done based on how they were created. Remove ./data/, -files and replace and - with _
-dir_variables <- paste(paste(stringr::str_replace(stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = "./data/", replace = ""), pattern = "-files", replace=""), "_directory", sep = ""))
-dir_variables <- stringr::str_replace(dir_variables, pattern = "-", "_")                       
-#Create the data frame to iterate through
-dir_df <- data.frame(names = dir_names, variables = dir_variables)
-
-#Initialize file extensions in same order as dir_variables based on initial script
-file_extension <- c(".bam", ".bw", ".fastq", ".html", ".fasta", ".bed", ".fastq")           
-
-if (!all(unlist(lapply(dir_variables, exists)))) {
-  
-  dir_list <- c()
-  for (i in 1:length(dir_df$names)){
-    
-    folder_variable <- paste(data_folder, dir_df$names[i], sep = "")
-    assign(dir_df$variables[i], folder_variable)
-    dir_list <- append(dir_list, folder_variable)
-  }
-  
-  print("Directory variables created.")
-  
-} else {
-  
-  print("All variables assigned.")
-}
-
-print(dir_variables)
-print(dir_df)
-print(dir_list)
-
-getwd()
-
-cat(Sys.time())
-print("assign-directory-variables.r complete")
-
-
-# mapply(function(names, variables, dir_list = c(), folder){
-# 
-#   folder_variable <- paste(folder, names, sep = "")
-#   assign(variables, folder_variable)
-#   variables
-#   dir_list <- append(dir_list, folder_variable)
-# 
-# }, names = dir_df$names, variables = dir_df$variables, folder = data_folder)
-
-# sink("sessionInfo.txt")
-# sessionInfo()
-# sink()
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/025_4-prepare-genome.r b/next_generation_sequencing/deprecatedCode/025_4-prepare-genome.r
deleted file mode 100755
index c54d216..0000000
--- a/next_generation_sequencing/deprecatedCode/025_4-prepare-genome.r
+++ /dev/null
@@ -1,148 +0,0 @@
-
-#Assigning directory variables ----
-Sys.time()
-
-.libPaths( c( "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2", .libPaths() ) )
-
-.libPaths()
-
-library(tidyverse, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-library(stringr, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-
-#Create folders if not present. Added conditional to run project startup script before
-# #assigning variables. Accounts for system OS (cluster or local).
-# if (!dir.exists("./data")){
-#   if(grepl("Windows", osVersion)){
-#     renv::run("./scripts/2-ngs-project-startup.r")
-#   } else if (grepl("Linux", osVersion)){
-#     renv::run("../R-scripts/2-ngs-project-startup.r")
-#   }
-# }
-# source("./scripts/2-ngs-project-startup.R")
-Sys.time()
-#Folder to get directories for. 
-data_folder <- "./data"
-dir_names <- stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = data_folder, replace = "")
-print(data_folder);print(dir_names)
-
-#Create the variables that will be assign the directory locations
-#Some processing has to be done based on how they were created. Remove ./data/, -files and replace and - with _
-dir_variables <- paste(paste(stringr::str_replace(stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = "./data/", replace = ""), pattern = "-files", replace=""), "_directory", sep = ""))
-dir_variables <- stringr::str_replace(dir_variables, pattern = "-", "_")                       
-#Create the data frame to iterate through
-dir_df <- data.frame(names = dir_names, variables = dir_variables)
-
-#Initialize file extensions in same order as dir_variables based on initial script
-file_extension <- c(".bam", ".bw", ".fastq", ".html", ".fasta", ".bed", ".fastq")           
-
-if (!all(unlist(lapply(dir_variables, exists)))) {
-  
-  dir_list <- c()
-  for (i in 1:length(dir_df$names)){
-    
-    folder_variable <- paste(data_folder, dir_df$names[i], sep = "")
-    assign(dir_df$variables[i], folder_variable)
-    dir_list <- append(dir_list, folder_variable)
-  }
-  
-  print("Directory variables created.")
-  
-} else {
-  
-  print("All variables assigned.")
-}
-
-print(dir_variables)
-print(dir_df)
-print(dir_list)
-
-getwd()
-
-cat(Sys.time())
-print("assign-directory-variables.r complete")
-#Initialize directory variables 
-#Runs project startup script if folders dont exist. File location is based on operating system.
-# if (!dir.exists("./data")){
-#   if(grepl("Windows", osVersion)){
-#     renv::run("./scripts/3-assign-directory-variables.r")
-#   } else if (grepl("Linux", osVersion)){
-#     renv::run("../R-scripts/3-assign-directory-variables.r")
-#   }
-# }
-
-#Preparing genome ----
-
-genome_file_path <- paste(genome_directory, "sacCer3.fasta", sep = "/")
-index_dir <- paste(genome_directory, "sacCer3-index", sep = "/")
-print(genome_file_path)
-print(index_dir)
-
-
-if (!file.exists(genome_file_path)){
-  
-  library(BSgenome.Scerevisiae.UCSC.sacCer3)
-  sacCer3 <- BSgenome.Scerevisiae.UCSC.sacCer3
-  genome_file_path <- paste(genome_directory, "sacCer3.fasta", sep = "/")
-  index_dir <- paste(genome_directory, paste(metadata(sacCer3)$genome, "-index", sep = ""), sep = "/")
-  #Export to local folder
-  export(sacCer3, genome_file_path, format = "fasta")
-  
-  #Create directory for index
-  dir.create(index_dir)
-  print(paste(index_dir, "directory created", sep = " "))
-  
-  #Using bowtie2 to build index for sacCer3
-  #Have to download python and perl
-  #https://www.bioconductor.org/packages/release/bioc/manuals/Rbowtie2/man/Rbowtie2.pdf
-  #Works after installing python and perl. See https://www.biostars.org/p/9507828/#9508926 and https://github.com/wzthu/Rbowtie2 
-  library(Rbowtie2)
-  bt2_index_path <- paste(index_dir, "sacCer3", sep = "/") 
-  
-  #Make index 
-  cmdout <- bowtie2_build(references = tools::file_path_as_absolute(genome_file_path), 
-                          bt2Index = file.path(tools::file_path_as_absolute(index_dir), "sacCer3"),
-                          "--threads 4 --verbose", overwrite=FALSE)
-  
-    
-  
-  #Remove at the end since you cant align to BSgenome object 
-  rm(sacCer3)
-  
-} else if (!dir.exists(index_dir)){
-  
-  print(paste(genome_file_path, "file exists but index folder is not present", sep = " "))
-  dir.create(index_dir)
-  print(paste(index_dir, "directory created", sep = " "))
-  
-  library(Rbowtie2)
-  bt2_index_path <- paste(index_dir, "sacCer3", sep = "/")
-  cmdout <- bowtie2_build(references = tools::file_path_as_absolute(genome_file_path), 
-                          bt2Index = file.path(tools::file_path_as_absolute(index_dir), "sacCer3"),
-                          "--threads 4 --verbose", overwrite=FALSE)
-  
-  print(paste("Indexed ", genome_file_path, sep = ""))
-  
-} else if (!(length(list.files(index_dir) == 6))){
-  print(paste(index_dir, "directory exists but there arent six files in it", sep = " "))
-  
-  library(Rbowtie2)
-  bt2_index_path <- paste(index_dir, "sacCer3", sep = "/")
-  cmdout <- bowtie2_build(references = tools::file_path_as_absolute(genome_file_path), 
-                          bt2Index = file.path(tools::file_path_as_absolute(index_dir), "sacCer3"),
-                          "--threads 4 --verbose", overwrite=FALSE)
-  print(paste("Indexed ", genome_file_path, sep = ""))
-        
-} else {
-  
-  print(paste(genome_file_path, "file exists", sep = " "))
-  print(paste(index_dir, "directory exists", sep = " "))
-  print(paste("Exactly six files present in", index_dir, sep = ""))
-  
-}
-
-print("Verified if reference genome is ready")
-
-Sys.time()
-print("prepare-genome.r complete")
-
-
diff --git a/next_generation_sequencing/deprecatedCode/026_5-process-and-align.r b/next_generation_sequencing/deprecatedCode/026_5-process-and-align.r
deleted file mode 100755
index 318db4e..0000000
--- a/next_generation_sequencing/deprecatedCode/026_5-process-and-align.r
+++ /dev/null
@@ -1,237 +0,0 @@
-# source("./scripts/4-prepare-genome.r")
-library(ShortRead)
-library(QuasR)
-library(R.utils)
-require(tidyverse)
-
-#Assigning directory variables ----
-cat(Sys.time())
-
-.libPaths( c( "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2", .libPaths() ) )
-
-.libPaths()
-
-library(tidyverse, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-library(stringr, lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2")
-
-#Create folders if not present. Added conditional to run project startup script before
-#assigning variables. Accounts for system OS (cluster or local).
-# if (!dir.exists("./data")){
-#   if(grepl("Windows", osVersion)){
-#     renv::run("./scripts/2-ngs-project-startup.r")
-#   } else if (grepl("Linux", osVersion)){
-#     renv::run("../R-scripts/2-ngs-project-startup.r")
-#   }
-# }
-# source("./scripts/2-ngs-project-startup.R")
-Sys.time()
-#Folder to get directories for. 
-data_folder <- "./data"
-dir_names <- stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = data_folder, replace = "")
-print(data_folder);print(dir_names)
-
-#Create the variables that will be assign the directory locations
-#Some processing has to be done based on how they were created. Remove ./data/, -files and replace and - with _
-dir_variables <- paste(paste(stringr::str_replace(stringr::str_replace(list.dirs(data_folder, recursive = FALSE), pattern = "./data/", replace = ""), pattern = "-files", replace=""), "_directory", sep = ""))
-dir_variables <- stringr::str_replace(dir_variables, pattern = "-", "_")                       
-#Create the data frame to iterate through
-dir_df <- data.frame(names = dir_names, variables = dir_variables)
-
-#Initialize file extensions in same order as dir_variables based on initial script
-file_extension <- c(".bam", ".bw", ".fastq", ".html", ".fasta", ".bed", ".fastq")           
-
-#Create the variables depending on whether they exist or not 
-if (!all(unlist(lapply(dir_variables, exists)))) {
-  
-  dir_list <- c()
-  #For all of the directories, create the string variable with the path, assign it and add it to dir list 
-  for (i in 1:length(dir_df$names)){
-    
-    folder_variable <- paste(data_folder, dir_df$names[i], sep = "")
-    assign(dir_df$variables[i], folder_variable)
-    dir_list <- append(dir_list, folder_variable)
-  }
-  
-  print("Directory variables created.")
-  
-} else {
-  
-  print("All variables assigned.")
-}
-
-print(dir_variables)
-print(dir_df)
-print(dir_list)
-
-getwd()
-
-Sys.time()
-print("assign-directory-variables.r complete")
-
-#Get toy data if user wants it and if fastq-files folder is empty ----
-#download_data <- readline(prompt = "Would you like to download toy data? (T or F): ")
-
-# if (as.logical(download_data) &
-#     (length(list.files(fastq_directory)) == 0)) {
-#   toy_data <-
-#     paste(fastq_directory, "eaton_toy_data.fastq.gz", sep = "/")
-#   library(curl)
-#   print("Getting some toy data")
-#   curl_download(
-#     "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR034/SRR034475/SRR034475.fastq.gz",
-#     toy_data
-#   )
-#   gunzip(toy_data)
-#   print(paste(toy_data, "file downloaded and extracted"))
-# 
-#   print("Splitting file.")
-#   toy_data <- str_replace(toy_data, pattern = ".gz", "")
-# 
-#   stream <-
-#     open(FastqStreamer(toy_data, n = 500000, readerBlockSize = 1000))
-# 
-#   counter <- 1
-#   repeat {
-#     tmp_fq <- yield(stream)
-#     destination_file <-
-#       paste(
-#         str_replace(toy_data, pattern = ".fastq$", replacement = ""),
-#         "_chunk",
-#         counter,
-#         ".fastq",
-#         sep = ""
-#       )
-#     #If input is 0, stop the function
-#     if (length(tmp_fq) == 0) {
-#       print("Split file")
-#       break
-#     }
-#     writeFastq(tmp_fq,
-#                destination_file,
-#                "a",
-#                compress = FALSE ,
-#                qualityType = "FastqQuality")
-#     counter <- counter + 1
-#   }
-#   close(stream)
-#   file.remove(toy_data)
-# 
-# } else {
-#   print("Following files are in the directory:")
-#   list.files(fastq_directory)
-# }
-
-#Get the index directory
-index_dir <- list.dirs(genome_directory)[grepl("index", list.dirs(genome_directory))]
-
-#Get all fastq files in fastq_directory obtained from assign-directory-variable.R ----
-fastq_file_list <- list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE)[!grepl("unmapped", list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE))]
-
-#Create variable for genome
-genome_file_path <- list.files(path = genome_directory, pattern = ".fasta", full.names = TRUE)
-
-#Obtain the sample name for each fastq file
-sample_name <- str_replace(basename(fastq_file_list), pattern = paste(".", tools::file_ext(fastq_file_list), sep = ""), replacement = "")   
-
-#Create the dataframe that has all paths required to preprocessing and alignment. 
-sample_paths_df <- mapply(function(dir_, ext_){
-  #Create the name for each file depending on the folder it will be outputted to. 
-  if (grepl("processed", dir_)){
-    paste(dir_, paste("processed_", basename(sample_name), ext_, sep = ""), sep = "/")
-  } else if (!grepl("genome|fastqc", dir_)){
-    paste(dir_, paste(basename(sample_name), ext_, sep = ""), sep = "/")
-  } else {
-  }
-}, dir_list, file_extension) %>% discard(is.null) %>% data.frame(sample_names = sample_name, original_file = fastq_file_list, .)
-dim(sample_paths_df)
-#Add the variables that can be used to assign different data types 
-sample_paths_df <- sample_paths_df %>% mutate(fq_var = paste(sample_names, "_fq", sep = ""),
-                                              bw_var = paste(sample_names, "_bw", sep = ""),
-                                              track_var = paste(sample_names, "_track", sep = ""))
-
-#Load in myfilter and trim function and my align,sort and index function.
-source("/home/luised94/data/rscripts/ngs-functions.r")
-
-#Try it for one.
-# myFilterAndTrim(sample_paths_df$original_file[1], destination_file = sample_paths_df$..data.processed.fastq.files[1])
-
-mapply(myFilterAndTrim, filelocation = sample_paths_df$original_file, destination_file = sample_paths_df$..data.processed.fastq.files)
-
-#Set the alignment parameters for Bowtie2
-alignment_params <- "-q --mp 4 --met-stderr"
-
-#Try it for one.
-# library(Rbowtie2)
-# bowtie2_samtools(bt2Index = file.path(tools::file_path_as_absolute(index_dir), "sacCer3"),
-#                  output = str_replace(sample_paths_df$..data.alignment.files[1], pattern = ".bam", replacement = ""),
-#                  outputType = str_replace(".bam", pattern = ".", replacement = ""),
-#                  seq1 = sample_paths_df$..data.processed.fastq.files[1],
-#                  seq2 = NULL,
-#                  bamFile = NULL,
-#                  alignment_params)
-# 
-# sortBam(sample_paths_df$..data.alignment.files[1], destination = paste(tools::file_path_sans_ext(sample_paths_df$..data.alignment.files[1]), "_sorted", sep = ''))
-# indexBam(paste(tools::file_path_sans_ext(sample_paths_df$..data.alignment.files[1]), "_sorted.bam", sep = ''))
-
-#Try my function for one file 
-alignSortandIndex(index_dir = index_dir, index_prefix = "sacCer3", fastq_file = sample_paths_df$..data.processed.fastq.files[1],
-                  alignment_file = sample_paths_df$..data.alignment.files[1], alignment_params = alignment_params)
-
-mapply(alignSortandIndex, index_dir=index_dir, index_prefix="sacCer3", fastq_file = sample_paths_df$..data.processed.fastq.files, 
-       alignment_file = sample_paths_df$..data.alignment.files, alignment_params = alignment_params)
-
-expected_files_present <- list.files(unique(dirname(sample_paths_df$..data.alignment.files)), pattern = "_sorted.bam$", full.names = TRUE) == str_replace(sample_paths_df$..data.alignment.files, pattern = ".bam", "_sorted.bam")
-
-  if(all(expected_files_present)){
-    print("All expected sorted alignment files present.")
-    list.files(unique(dirname(sample_paths_df$..data.alignment.files)), pattern = "_sorted.bam$", full.names = TRUE)
-  } else {
-    paste(sample_paths_df$..data.alignment.files[!expected_files_present], "not present", sep = " ")
-    paste(sample_paths_df$..data.alignment.files[expected_files_present], "created", sep = " ")
-  }
-
- 
-#Will attempt to parallelize. Have to delete the files created from previous lines
-# library(snow)
-# z=vector('list',4)
-# z=1:4
-# system.time(lapply(z,function(x) Sys.sleep(1)))
-# cl<-makeCluster(detectCores()-1,type="SOCK")
-# system.time(clusterApply(cl, z,function(x) Sys.sleep(1)))
-# stopCluster(cl)
-
-
-
-# library(doParallel)
-# library(parallel)
-# 
-# no_cores <- 3
-# cl <- makeCluster(no_cores)
-# registerDoParallel(cl)
-# original_file_list <- sample_paths_df$original_file
-# processed_file_list <- sample_paths_df$..data.processed.fastq.files
-# export_to_cluster <- c('myFilterAndTrim', 'processed_file_list')
-# clusterExport(cl,export_to_cluster)
-# 
-# parLapply(cl, original_file_list, function(x){
-#   myFilterAndTrim(filelocation = x, destination_file = processed_file_list)
-# })
-# 
-# # mapply(myFilterAndTrim, filelocation = sample_paths_df$original_file, destination_file = sample_paths_df$..data.processed.fastq.files)
-# stopCluster(cl)
-
-download_data <- as.logical(readline(prompt = "Were you running toy data? (T or F): "))
-remove_toy_data <- as.logical(readline(prompt = "Would you like to delete the toy-data generated files? (T or F): "))
-also_fastq <- readline(prompt = "Would you also like to delete download toy data? (T or F) Keep to continue testing pipeline.")
-
-if (download_data & remove_toy_data){
-  unlink(paste(processed_fastq_directory, "/*", sep = ""))
-  unlink(paste(alignment_directory, "/*", sep = ""))
-  
-}
-
-
-
-
-
-
diff --git a/next_generation_sequencing/deprecatedCode/027_process-and-align.R b/next_generation_sequencing/deprecatedCode/027_process-and-align.R
deleted file mode 100755
index 201c897..0000000
--- a/next_generation_sequencing/deprecatedCode/027_process-and-align.R
+++ /dev/null
@@ -1,70 +0,0 @@
-source("./scripts/assign-directory-variables.R")
-#Prepare the dataframe with the input/output files ----
-
-#Get the index directory
-index_dir <- list.dirs(genome_directory)[grepl("index", list.dirs(genome_directory))]
-
-#Get all fastq files in fastq_directory obtained from assign-directory-variable.R 
-fastq_file_list <- list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE)[!grepl("unmapped", list.files(fastq_directory, pattern = ".fastq$", full.names = TRUE, recursive = FALSE))]
-
-#Create variable for genome
-genome_file_path <- list.files(path = genome_directory, pattern = ".fasta", full.names = TRUE)
-
-#Obtain the sample name for each fastq file
-sample_name <- str_replace(basename(fastq_file_list), pattern = paste(".", tools::file_ext(fastq_file_list), sep = ""), replacement = "")   
-
-#Create the dataframe that has all paths required to preprocessing and alignment. 
-sample_paths_df <- mapply(function(dir_, ext_){
-  #Create the name for each file depending on the folder it will be outputted to. 
-  if (grepl("processed", dir_)){
-    paste(dir_, paste("processed_", basename(sample_name), ext_, sep = ""), sep = "/")
-  } else if (!grepl("genome|fastqc", dir_)){
-    paste(dir_, paste(basename(sample_name), ext_, sep = ""), sep = "/")
-  } 
-}, dir_list, file_extension) %>% discard(is.null) %>% data.frame(sample_names = sample_name, original_file = fastq_file_list, .)
-dim(sample_paths_df)
-#Add the variables that can be used to assign different data types 
-sample_paths_df <- sample_paths_df %>% mutate(fq_var = paste(sample_names, "_fq", sep = ""),
-                                              bw_var = paste(sample_names, "_bw", sep = ""),
-                                              track_var = paste(sample_names, "_track", sep = ""))
-
-#Load in myfilter and trim function and my align,sort and index function
-source("/home/luised94/data/rscripts/ngs-functions.r")
-
-#Load the packages required by my functions ----
-package_to_check <- c("Rbowtie2", "ShortRead")
-package_was_loaded <- unlist(lapply(package_to_check, library, character.only = TRUE, logical.return=TRUE)) 
-if (length(lapply(package_to_check[!package_was_loaded], function(x){
-  paste(x, "Package did not install ")
-})) == 0) print("All packages loaded.")
-
-
-#Run my functions on one sample to sbatch submission ----
-# myFilterAndTrim(sample_paths_df$original_file[1], destination_file = sample_paths_df$..data.processed.fastq.files[1])
-#Run Rbowtie2 to align and index my file 
-# alignSortandIndex(index_dir = index_dir, index_prefix = "sacCer3", fastq_file = sample_paths_df$..data.processed.fastq.files[1],
-#                   alignment_file = sample_paths_df$..data.alignment.files[1], alignment_params = alignment_params)
- 
-
-#Align files ----
-
-#Set alignment parameters for Rbowtie2
-alignment_params <- "-q --mp 4"
-
-#Use mapply to filter,align,sort and index all of the files by referencing the different columns of the data frame. 
-mapply(myFilterAndTrim, filelocation = sample_paths_df$original_file, destination_file = sample_paths_df$..data.processed.fastq.files)
-
-mapply(alignSortandIndex, index_dir=index_dir, index_prefix="sacCer3", fastq_file = sample_paths_df$..data.processed.fastq.files, 
-       alignment_file = sample_paths_df$..data.alignment.files, alignment_params = alignment_params)
-
-#Determine if all files were created 
-expected_files_present <- list.files(unique(dirname(sample_paths_df$..data.alignment.files)), pattern = "_sorted.bam$", full.names = TRUE) == str_replace(sample_paths_df$..data.alignment.files, pattern = ".bam", "_sorted.bam")
-
-if(all(expected_files_present)){
-  print("All expected sorted alignment files present.")
-  list.files(unique(dirname(sample_paths_df$..data.alignment.files)), pattern = "_sorted.bam$", full.names = TRUE)
-} else {
-  paste(sample_paths_df$..data.alignment.files[!expected_files_present], "not present", sep = " ")
-  paste(sample_paths_df$..data.alignment.files[expected_files_present], "created", sep = " ")
-}
-
diff --git a/next_generation_sequencing/deprecatedCode/028_package-installation.R b/next_generation_sequencing/deprecatedCode/028_package-installation.R
deleted file mode 100755
index 7b80e50..0000000
--- a/next_generation_sequencing/deprecatedCode/028_package-installation.R
+++ /dev/null
@@ -1,44 +0,0 @@
-
-#If running on Bash script, you must add the R and gnu modules 
-#Had to run this manually first. May have to include this line next time I try to set it up. 
-# r = getOption("repos")
-# r["CRAN"] = "https://cloud.r-project.org/"
-# options(repos = r)
-
-Sys.time()
-
-install.packages(c("tidyverse", "R.utils", "ggplot2","BiocManager"), 
-                 lib = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2", )
-
-bioinformatics_packages <- c("BiocGenerics","MatrixGenerics",'qvalue','plot3D','ggplot2','pheatmap','cowplot',
-                             'cluster', 'NbClust', 'fastICA', 'NMF','matrixStats',
-                             'Rtsne', 'mosaic', 'knitr', 'genomation',
-                             'ggbio', 'Gviz', 'DESeq2', 'RUVSeq',
-                             'gProfileR', 'ggfortify', 'corrplot',
-                             'gage', 'EDASeq', 'formatR', 'BiocFileCache',
-                             'svglite', 'Rqc', 'ShortRead', 'QuasR',
-                             'methylKit','FactoMineR', 'iClusterPlus',
-                             'enrichR','caret','xgboost','glmnet',
-                             'DALEX','kernlab','pROC','nnet','RANN',
-                             'ranger','GenomeInfoDb', 'GenomicRanges',
-                             'GenomicAlignments', 'ComplexHeatmap', 'circlize', 
-                             'rtracklayer', 'tidyr', 'dplyr',
-                             'AnnotationHub', 'GenomicFeatures', 'normr',
-                             'MotifDb', 'TFBSTools', 'rGADEM', 'JASPAR2018', 
-                             'BSgenome', 'htmltab', 'usethis',
-                             'Rsubread', 'Rsamtools', 'Rbowtie', 'Rbowtie2' , "ChIPpeakAnno", 
-                             "seqinr","GenomeInfoDbData","BSgenome.Scerevisiae.UCSC.sacCer3")
-
-library(BiocManager)
-BiocManager::install(pkgs=bioinformatics_packages)
-
-package_to_check <- c("Rbowtie2", "BSgenome", "R.utils","ShortRead","QuasR","BiocManager")
-package_was_loaded <- unlist(lapply(package_to_check, library, character.only = TRUE, logical.return=TRUE)) #lib.loc = "/home/luised94/R/x86_64-pc-linux-gnu-library/4.2"
-if (length(lapply(package_to_check[!package_was_loaded], function(x){
-  paste(x, "Package did not install ")
-})) == 0) print("All packages loaded.")
-
-
-Sys.time()
-print("package-installation.R complete")
-
diff --git a/next_generation_sequencing/deprecatedCode/029_example.R b/next_generation_sequencing/deprecatedCode/029_example.R
deleted file mode 100755
index 222d247..0000000
--- a/next_generation_sequencing/deprecatedCode/029_example.R
+++ /dev/null
@@ -1,116 +0,0 @@
-#Get time at start of script ----
-start_time <- Sys.time()
-print("Script start")
-
-#Prepares directory variables and the dataframe that contains paths, connections and variables to the fastq samples 
-if(grepl("Windows", osVersion)){
-  source("./scripts/prepare-samples-dataframe.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/prepare-samples-dataframe.R")
-}
-
-#Load the packages required by the script ----
-package_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead")
-loadPackages(package_to_check)
-
-#Create variable for genome ----
-genome_file_path <- paste(genome_directory[1], "sacCer3.fasta", sep = "/")
-sacCer3 <- readFasta(genome_file_path)
-# names(as(sacCer3, "DNAStringSet"))
-# width(sacCer3)
-sacCer3_df <- data.frame(chrom = names(as(sacCer3, "DNAStringSet")), size = width(sacCer3)) %>% filter(chrom != "chrM")
-rm(sacCer3)
-
-
-#Output bigwig files by chunking through chromosomes ----
-#Uses mapply to go through bam files and their bw connections. Uses function bamReadPosAndQwidthByChromToRle mostly taken from the introduction to Rsamtools manual
-#https://www.bioconductor.org/packages/release/bioc/vignettes/Rsamtools/inst/doc/Rsamtools-Overview.pdf
-
-mapply(function(bam_files, bw_connection) {
-  print(paste("Working on:", bam_files, sep = " "))
-  cvg <-
-    RleList(
-      mapply(
-        FUN = bamReadPosAndQwidthByChromToRle,
-        chrom_name = sacCer3_df$chrom,
-        bam_file = bam_files,
-        chromosome_size = sacCer3_df$size,
-        SIMPLIFY = FALSE
-      )
-    )
-  print(paste("Exporting:", bw_connection, sep = " "))
-  export.bw(con = bw_connection,
-            object = cvg,
-            format = "bw")
-  
-}, bam_files = sample_paths_df$sorted_bam, bw_connection = sample_paths_df$..data.bw.files)
-
-print(paste("Bigwig Files created. Time Elasped (in mins)", difftime(start_time, Sys.time(), units = "mins"), sep = " "))
-
-paste(as.character(file.size(sample_paths_df$..data.bw.files[1])/10^3), "Kb file size for first file.", sep = "")
-
-#Read in tab-delimited file with info on fastq_files 
-#Read in tab-delimited text file with info for each sample. Made in C:/Users/Luis/Projects/working-on-a-cluster/scripts/fastq-bmc-info-dataframe.R -----
-if(grepl("Windows", osVersion)){
-  fastq_info <- read.delim("./scripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-} else if (grepl("Linux", osVersion)){
-  fastq_info <- read.delim("../rscripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-}
-
-fastq_info <- bind_cols(fastq_info, sample_paths_df)
-
-#Create GRanges object to read in a particular chromosome
-chrXIV.gr <- GRanges(seqnames=c(sacCer3_df$chrom[14]), ranges = IRanges(start = 100000, end = sacCer3_df$size[14]), strand = "*")
-
-#Create genome axis to display 
-all_tracks <- list(GenomeAxisTrack())
-
-#Assign Tracks of interest
-#Can use index or file name to subset the dataframe Wnani,WnayV,nnNnH
-files_to_visualize <- c("WnNyV", "nnNnH","Wnani")
-subset_df <- fastq_info %>% filter(sname %in% files_to_visualize)
-
-#Assign tracks using mapply and sample dataframe. May add rm to get rid of bw_variable if it is too large
-#Does not return bw_variables
-all_tracks <-append(all_tracks, value = mapply(function(bw_con, bw_variable, track_variable, track_name){
-  assign(bw_variable,  import(con = bw_con, which = chrXIV.gr))
-  assign(track_variable, DataTrack(get(bw_variable), type = "l", name = track_name, chromosome = "chrXIV"))
-}, bw_con = subset_df$..data.bw.files, bw_variable = subset_df$bw_var, track_variable =  subset_df$track_var, track_name = subset_df$sample_names)
-) 
-
-#format(object.size(get(subset_df$track_var[1]), units = "Kb"))
-
-#Plot Tracks 
-#plotTracks(list(tracks[[1]],tracks[[2]],tracks[[3]]), main = "Eaton view of Chromosome 14", chromosome = "chrXIV")
-
-#mapply output can be used directly in plotTracks. Save to svg. 
-svg(paste("./reports/plots-files/", underscoreDate(), "_Quick_comparison_of_eaton_and_V5_at_Noc", ".svg", sep = ""))
-plotTracks(all_tracks, main = "Complete View of Chromosome 14", chromosome = "chrXIV")
-dev.off()
-
-print(paste("Script complete. Time Elasped (in mins)", difftime(start_time, Sys.time(), units = "mins"), sep = " "))
-
-#Extra code and chunks to confirm how functions works ----
-# #Create the bigwig coverage files for all the bam files. Uses dataframe that holds paths to all the different files
-#This works for toy data
-# mapply(function(bam_files, bw_connection){
-#   alns <- readGAlignments(bam_files)
-#   covs <- coverage(alns)
-#   export.bw(con = bw_connection,
-#             object = covs,
-#             format = "bw")
-# }, bam_files = sample_paths_df$sorted_bam, bw_connection = sample_paths_df$..data.bw.files)
-
-#Export function from rtracklayer package overwrites by default
-# export.bw(con = sample_paths_df$..data.bw.files[1],
-#           object = RleList(cvg1),
-#           format = "bw")
-
-
-# #Establishes connection 
-# BamFile(sample_paths_df$sorted_bam[1])
-# #Reads in the alignment file according to param argument, pass BamFile object.
-# scanBam(sample_paths_df$sorted_bam[1])
-
-#Confirm that levels where same for bam files and sacCer3 
-# levels(aln[[1]]$rname)[!grepl("chrM", levels(aln[[1]]$rname))] == sacCer3_df$chrom
diff --git a/next_generation_sequencing/deprecatedCode/030_fastq-bmc-info-dataframe.R b/next_generation_sequencing/deprecatedCode/030_fastq-bmc-info-dataframe.R
deleted file mode 100755
index 55d8c91..0000000
--- a/next_generation_sequencing/deprecatedCode/030_fastq-bmc-info-dataframe.R
+++ /dev/null
@@ -1,61 +0,0 @@
-#Changing the name of the fastq files. 
-#See C:\Users\Luis\Dropbox (MIT)\Lab\Experiments\ATP hydrolysis by ORC and Cdc6\ORC\ORC1_4 interface\ORC4 R267A\Assays\Yeast Genetics\2022_08_26_CHIP-seq_ORC-MCM-supps\downloading-bmc-data.md
-
-library(tidyverse)
-
-#Read in the file with the info for the fastq sequences
-#Files were moved to be in same folder as this script. Initially in my downloads folder. Initially had to be refered to in my downloads folder.
-fastq_ids <- bind_cols(utils::read.table("./scripts/fastq-ids.tab"), utils::read.table("./scripts/221024Bel_CHIP.tab"))
-
-
-column_names <- c("BMC_ID1", "V2", "Index_Seq1", "BMC_ID2", "V3", "Index_Seq2")
-names(fastq_ids) <- column_names
-
-
-#Create the treatment vector and separate column two in treatment columns
-treatments <- c("complement", "suppressor", "cellcycle", "auxin", "antibody", "index")
-fastq_ids <- fastq_ids %>% separate("V2", treatments, sep = "_", remove = FALSE)
-# fastq_ids %>% mutate(sname_1 = c_across(get(treatments), str_c(str_sub(cur_column(), 1,1))))
-
-#Create a shorter named column that still contains treatment info. 
-#Each letter will stand for the treatment used corresponding to the same order as treatment vector made above.
-fastq_ids <- fastq_ids %>% mutate(sname = str_c(str_sub(complement,1,1), str_sub(suppressor,1,1),str_sub(cellcycle,1,1),str_sub(auxin,1,1),str_sub(antibody,1,1)))
-
-#Condition that determines if sample is 
-is_input <- fastq_ids$antibody == 'input'
-
-#Conditions that determine if sample is negative control 
-is_negative<- (fastq_ids$antibody == 'V5' & fastq_ids$complement == 'none') |
-  (fastq_ids$antibody == 'Myc' & fastq_ids$auxin == 'yes') |
-  (fastq_ids$antibody == 'UM174' & fastq_ids$cellcycle == 'Noc') |
-  (fastq_ids$antibody == 'UM174' & fastq_ids$complement == 'none' & fastq_ids$auxin == 'yes')
-
-#Creating a factor vector depending on the conditions.  
-fastq_ids$Sample_type <-  factor(case_when(is_input ~ 'input',
-                 is_negative ~ 'negative',
-                 TRUE ~ 'experiment'))
-factor(case_when(as.numeric(rownames(fastq_ids)) <= 24 ~ 'A',
-                 as.numeric(rownames(fastq_ids)) > 24 ~ 'B'))
-
-fastq_ids <- fastq_ids %>% mutate(Pool = factor(case_when(as.numeric(rownames(fastq_ids)) <= 24 ~ 'A',
-                                          as.numeric(rownames(fastq_ids)) > 24 ~ 'B')))
-
-#Creating the fastq rows for SRR files (combined by ) from eaton paper ----
-fastq_ids <- fastq_ids %>% add_row(complement = c("none"),
-                             suppressor = c("none"), 
-                             cellcycle = c("Noc"),
-                             auxin = c("no"),  
-                             antibody = c("HM1108"),
-                             index= c("49"), 
-                             sname = c("nnNnH"),
-                             Sample_type = c("eaton"),
-                             Pool = c("C"),
-                             V2 = c("none_none_Noc_no_HM1108"))
-
-#Write dataframe to file to reference in cluster
-write.table(fastq_ids, file = "./scripts/fastq_info.txt", sep = "\t")
-
-#Read with function below
-# read.table('./scripts/fastq_info.txt')
-
-# list.files(path ='.', pattern = "*.fastq$", recursive = TRUE)[!grepl("unmapped", list.files(path ='.', pattern = "*.fastq$", recursive = TRUE))]
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/031_fastq-bmc-info-dataframe.r b/next_generation_sequencing/deprecatedCode/031_fastq-bmc-info-dataframe.r
deleted file mode 100755
index e6ea7c7..0000000
--- a/next_generation_sequencing/deprecatedCode/031_fastq-bmc-info-dataframe.r
+++ /dev/null
@@ -1,61 +0,0 @@
-#Changing the name of the fastq files. 
-#See C:\Users\Luis\Dropbox (MIT)\Lab\Experiments\ATP hydrolysis by ORC and Cdc6\ORC\ORC1_4 interface\ORC4 R267A\Assays\Yeast Genetics\2022_08_26_CHIP-seq_ORC-MCM-supps\downloading-bmc-data.md
-
-library(tidyverse)
-
-#Read in the file with the info for the fastq sequences
-fastq_ids <- utils::read.table("C:/Users/Luis/Downloads/fastq-ids.tab")
-
-#Files were moved to be in same folder as this script. Initially in my downloads folder. Initially had to be refered to in my downloads folder.
-# fastq_ids <- bind_cols(utils::read.table("C:/Users/Luis/Downloads/fastq-ids.tab"), utils::read.table("C:/Users/Luis/Downloads/221024Bel_CHIP.tab"))
-column_names <- c("BMC_ID1", "V2", "Index_Seq1", "BMC_ID2", "V3", "Index_Seq2")
-names(fastq_ids) <- column_names
-
-
-#Create the treatment vector and separate column two in treatment columns
-treatments <- c("complement", "suppressor", "cellcycle", "auxin", "antibody", "index")
-fastq_ids <- fastq_ids %>% separate("V2", treatments, sep = "_", remove = FALSE)
-
-#Create a shorter named column that still contains treatment info. 
-#Each letter will stand for the treatment used corresponding to the same order as treatment vector made above.
-short_names <- str_c(str_sub(fastq_ids$complement,1,1), str_sub(fastq_ids$suppressor,1,1),str_sub(fastq_ids$cellcycle,1,1),str_sub(fastq_ids$auxin,1,1),str_sub(fastq_ids$antibody,1,1))
-fastq_ids$sname <- short_names
-
-#Condition that determines if sample is 
-is_input <- fastq_ids$antibody == 'input'
-
-#Conditions that determine if sample is negative control 
-is_negative<- (fastq_ids$antibody == 'V5' & fastq_ids$complement == 'none') |
-  (fastq_ids$antibody == 'Myc' & fastq_ids$auxin == 'yes') |
-  (fastq_ids$antibody == 'UM174' & fastq_ids$cellcycle == 'Noc') |
-  (fastq_ids$antibody == 'UM174' & fastq_ids$complement == 'none' & fastq_ids$auxin == 'yes')
-
-#Creating a factor vector depending on the conditions.  
-fastq_ids$Sample_type <-  factor(case_when(is_input ~ 'input',
-                 is_negative ~ 'negative',
-                 TRUE ~ 'experiment'))
-factor(case_when(as.numeric(rownames(fastq_ids)) <= 24 ~ 'A',
-                 as.numeric(rownames(fastq_ids)) > 24 ~ 'B'))
-
-fastq_ids <- fastq_ids %>% mutate(Pool = factor(case_when(as.numeric(rownames(fastq_ids)) <= 24 ~ 'A',
-                                          as.numeric(rownames(fastq_ids)) > 24 ~ 'B')))
-
-#Creating the fastq rows for SRR files (combined by ) from eaton paper ----
-fastq_ids <- fastq_ids %>% add_row(complement = c("none"),
-                             suppressor = c("none"), 
-                             cellcycle = c("Noc"),
-                             auxin = c("n"),  
-                             antibody = c("HM1108"),
-                             index= c("49"), 
-                             sname = c("nnNnH"),
-                             Sample_type = c("eaton"),
-                             Pool = c("C"),
-                             V2 = c("none_none_Noc_no_HM1108"))
-
-#Write dataframe to file to reference in cluster
-write.table(fastq_ids, file = "./scripts/fastq_info.txt", sep = "\t")
-
-#Read with function below
-read.table('./scripts/fastq_info.txt')
-
-# list.files(path ='.', pattern = "*.fastq$", recursive = TRUE)[!grepl("unmapped", list.files(path ='.', pattern = "*.fastq$", recursive = TRUE))]
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/032_feature-files-downloads.R b/next_generation_sequencing/deprecatedCode/032_feature-files-downloads.R
deleted file mode 100755
index 09ec4e5..0000000
--- a/next_generation_sequencing/deprecatedCode/032_feature-files-downloads.R
+++ /dev/null
@@ -1,39 +0,0 @@
-#Load packages required by script
-package_to_check <- c("curl", "tidyverse","tools","R.utils")
-package_was_loaded <- unlist(suppressPackageStartupMessages(lapply(package_to_check, library, character.only = TRUE, logical.return=TRUE, quietly = TRUE)))
-if (length(lapply(package_to_check[!package_was_loaded], function(x){
-  paste(x, "Package did not install")
-})) == 0) print("All packages loaded.")
-
-#Use subversion to get feature files from Rossi et al.
-system('wsl svn export https://github.com/CEGRcode/2021-Rossi_Nature.git/trunk/02_References_and_Features_Files')
-
-#Get folder that has features in it. 
-feature_folder <- list.dirs(recursive = FALSE)[grepl("Features", list.dirs(recursive = FALSE))]
-
-#Download origin timing data from https://www.sciencedirect.com/science/article/pii/S2211124713005834?via%3Dihub
-hawkins_timing_url <- "https://ars.els-cdn.com/content/image/1-s2.0-S2211124713005834-mmc2.xlsx"
-hawkins_timing <- paste(feature_folder, "hawkins-origins-timing.xlsx", sep = "/")
-if (!file.exists(hawkins_timing)){
-  curl_download(hawkins_timing_url, 
-                hawkins_timing)
-  
-  print(paste(hawkins_timing, "file downloaded"))
-} else {
-  print(paste(hawkins_timing, "file exists"))
-}
-
-#Download called peaks for ORC in nocodazole from https://pubmed.ncbi.nlm.nih.gov/20351051/
-eaton_orc_bed_url <- "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM424nnn/GSM424494/suppl/GSM424494_wt_G2_orc_chip_combined.bed.gz"
-eaton_orc_bed <- paste(feature_folder, "G2_orc_chip.bed.gz", sep = "/")
-if (!file.exists(eaton_orc_bed)){
-  curl_download(eaton_orc_bed_url, 
-                eaton_orc_bed)
-  gunzip(eaton_orc_bed)
-  print(paste(eaton_orc_bed, "file downloaded and extracted"))
-} else {
-  print(paste(eaton_orc_bed, "file exists"))
-}
-
-#Cant use system function because it requires password
-# system('wsl rsync --stats -nv --update /mnt/c/Users/Luis/Projects/working-on-a-cluster/02_References_and_Features_Files/* luised94@luria.mit.edu:/home/luised94/data/feature-files/')
diff --git a/next_generation_sequencing/deprecatedCode/033_generate-comparison-coverage-plots.R b/next_generation_sequencing/deprecatedCode/033_generate-comparison-coverage-plots.R
deleted file mode 100755
index fbe951d..0000000
--- a/next_generation_sequencing/deprecatedCode/033_generate-comparison-coverage-plots.R
+++ /dev/null
@@ -1,173 +0,0 @@
-#Run with: ----
-#$sbatch sbatch-generate-comparison-plots.sh 7 
-#See ./regenerate-comparison-coverage-plots.R to get parameters used for plot. 
-
-#Get time at start of script ----
-start_date <- Sys.time()
-start_time <- as.character(stringr::str_replace_all(start_date, pattern = ":| |-", replacement="_"))
-print(paste("Script start:", start_date, sep = " "))
-#Handle script arguments ----
-if(grepl("Windows", osVersion)){
-  chromosome_number <- 7
-} else if (grepl("Linux", osVersion)){
-  chromosome_number <- as.numeric(args[1])
-  args = commandArgs(trailingOnly=TRUE)
-  if (length(args)==0) {
-    stop("Pass the chromosome number to batch script", call.=FALSE)
-  } 
-  print(paste("Args used in script are:", args[1], sep = " "))
-}
-
-#Prepares directory variables and the dataframe that contains paths, connections and variables to the fastq samples ----
-#This script sources assign-directory-variables.R, which in turn also sources ngs-functions.R
-if(grepl("Windows", osVersion)){
-  source("./scripts/prepare-samples-dataframe.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/prepare-samples-dataframe.R")
-}
-
-package_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead")
-loadPackages(package_to_check) #Load packages and output message when done or if any package didnt load. Takes list of characters.
-
-#Read in feature files according to their file type. ----
-#Variables to be visualized with track data. #Also creates the sacCer3_df variable with the variables for assigining track data.
-#Set df_names variables with any of the below options.
-# df_names <- c("ChExMix","MEME","XUTs","CUTs","ORF","Nucleosome",
-#               "timing", "Rhee" , "SGD_features", "SUTs", "G2_orc")
-df_names <- c("timing")
-
-if(grepl("Windows", osVersion)){
-  source("./scripts/readin-feature-files.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/readin-feature-files.R")
-}
-
-#Read in tab-delimited text file with info for each sample. Made in ./scripts/fastq-bmc-info-dataframe.R -----
-if(grepl("Windows", osVersion)){
-  fastq_info <- read.delim("./scripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-} else if (grepl("Linux", osVersion)){
-  fastq_info <- read.delim("../rscripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-  fastq_info <- bind_cols(fastq_info, sample_paths_df)
-}
-
-#Turn character columns into factors to reorder according to the levels variable.
-#Also use for plotting on Gviz. 
-#Columns to leave as characters. Then select the columns to be turned into factors
-columns_to_exclude <- c("BMC", "Index", "V2", "V3","sname", "Sample_names", "..data","original","track","fq", "bw","track","sorted")
-character_to_factor <- fastq_info %>% select_if(is.character) %>% dplyr::select(-contains(columns_to_exclude)) %>% colnames(.)
-
-#Use to reorder the columns. Does not generalize!!! 
-order_of_columns <- list(c(1,3,2), c(3,1,2,4), c(1:2),c(1:2),c(2,1,3,5,4),c(3,4,2,1),c(1:3))
-
-invisible(mapply(function(char_to_factor, order_for_columns){
-  #Get the reordered levels. Determined by watching the original order. 
-  reorder_levels <- levels(factor(as.factor(fastq_info[, char_to_factor])))[order_for_columns]
-  fastq_info[, char_to_factor] <<- factor(as.factor(fastq_info[, char_to_factor]), levels = reorder_levels)
-}, char_to_factor = character_to_factor, order_for_columns = order_of_columns, SIMPLIFY = FALSE))
-
-if(fastq_info %>% select_if(is.factor) %>% colnames(.) %>% length() == length(character_to_factor)){
-  print("Columns refactored")
-}
-
-#Use the dataframe to assign the tracks for the chromosome GRanges and tracks ----
-#See ./ngs-functions.R for assignChrTracks details.
-
-assignChrTracks(sacCer3_df, chromosome_number, timing, 200)
-
-#Create the dataframe with the different comparisons to make and the name to assign to the plot ----
-#Dataframe has two columns, one with the files that will be accessed and the other with the name of the plot.
-#Use mapply to iterate through the dataframe. 
-#Files that will be used to subset dataframe
-#Removed nnNyU from effect of Auxin on MCM because it was giving trouble and wanted to check other samples. 
-files_to_visualize = list(c("nnani","R1ani","R4ani","Rnani","RTani","Wnani"))
-#List of names for the plots that will be generated.
-name_of_plots = c("Input_Samples")
-#Variable to construct the track name
-
-#Chromosome visualized in plots
-chromosome_for_plot = rep(chromosome_number, length(files_to_visualize))
-#Create the dataframe. Use I() function to treat list of lists as is. 
-plots_to_generate <- data.frame(set_of_files = I(files_to_visualize), plot_names = name_of_plots,
-                                plot_chromosome = chromosome_for_plot)
-
-#Setting the output folder
-if(grepl("Windows", osVersion)){
-  output_folder = "./reports-and-figures/plots-files/"
-} else if (grepl("Linux", osVersion)){
-  output_folder = "../reports/plots-files/"
-}
-
-#Create the plots ----
-name_of_files <- c()
-mapply(function(files, names, chr_num){
-  # print(files);print(names)
-  # print(str_replace_all(names, pattern = "_", replacement = " "))
-  #Subset data frame according to files and arrange by antibody to display according to that order in Gviz
-  subset_df <- fastq_info %>% filter(sname %in% files) %>% arrange(antibody, complement, suppressor, cellcycle, auxin)
-  
-  #check that subset occured correctly. File length should be same as subset row number.
-  if(length(subset_df$sname) != length(files)){
-    print("Subsetting didnt occur properly.")
-    break
-  }
-
-  #Reset all_tracks variable just in case. Add GenomeAxisTrack at top
-  all_tracks <- list(GenomeAxisTrack(name=paste("Chr ", chr_num, " Axis", sep = "")))
-
-  #Assign tracks using mapply and sample dataframe in order of subset dataframe. May add rm to get rid of bw_variable if it is too large
-  #Append to all tracks the result of mapply. Assigns bigwig variable subset by chromosome and creates track variable
-  all_tracks <-append(all_tracks, value = mapply(function(bw_con, bw_variable, track_variable, track_name){
-    # print(bw_con)
-    assign(bw_variable,  import(con = bw_con, which = get(sacCer3_df$gr_var[chr_num])))
-    assign(track_variable, DataTrack(get(bw_variable), type = "l", name = track_name, ylim = c(0, 2000)))
-  }, bw_con = subset_df$..data.bw.files, bw_variable = subset_df$bw_var, track_variable =  subset_df$track_var, track_name = subset_df$sample_names)
-  )
-
-  #Add origin track at the end so that it shows up below the tracks.
-  all_tracks <- append(all_tracks, get(sacCer3_df$origin_track_var[chr_num]))
-
-  #Create the plot.
-  # print(paste("Creating ", paste(output_folder, paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""), sep = ""))
-  svg(paste(output_folder, paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""))
-  plotTracks(all_tracks, main = paste(str_replace_all(names, pattern = "_",replacement = " "),  sep = " "))
-  dev.off()
-  print(paste(paste(output_folder, paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""), "Plot created.", sep = " "))
-  
-  name_of_files <<- append(name_of_files, paste(output_folder, paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""))
-}, files = plots_to_generate$set_of_files, names = plots_to_generate$plot_names, chr_num = chromosome_number)
-#Output a table with the values for each variable in the 221024Bel_CHIP experiment. ----
-#Add name_of_files, chr name and start_time to plots_to_generate dataframe
-plots_to_generate$file_name <- name_of_files
-plots_to_generate$plot_time <- rep(start_time, length(files_to_visualize))
-
-plot_input_file <- paste(output_folder, paste(start_time,"_",sep = ""), "chr", chromosome_number, "_", "plot_input.txt", sep = "")
-write.table(plots_to_generate, file = plot_input_file, row.names = FALSE, quote = FALSE, sep = "\t")
-
-print(paste("Plots created. Time Elasped (in mins)", difftime(start_date, Sys.time(), units = "mins"), sep = " "))
-sessionInfo()
-
-#Output a table with the unique values of the treatment column for Steve ----
-#Install kableExtra and run webshot::install_phamtomjs()
-# knitr::kable(data.frame(
-#   Variables = treatments,
-#   Values = str_replace_all(
-#     as.character(unique_values),
-#     pattern = "c|\\(|\\)|,|\"",
-#     replacement = ""
-#   )
-# ),
-# format = "html") %>% kable_classic(full_width = F, position = "center") %>% save_kable(file = "table_with_variable_values_for_221024Bel_CHIP.png", zoom = 1.5)
-
-# 
-# html_table_width <- function(kable_output, width){
-#   width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
-#   sub("<table>", paste0("<table>\n", width_html), kable_output)
-# }
-# fastq_info[, sapply(fastq_info, class) == 'character']
-# fastq_info %>% select_if(colnames(.) %in% treatments) %>% apply(., 2, table)
-# fastq_info$sample_names == fastq_info$sname
-# levels(fastq_info$antibody)
-# unique_values <- lapply(character_to_factor, FUN = function(column) {
-# unique(fastq_info[, names(fastq_info)[grepl(column, names(fastq_info))]])
-# })
-# fastq_info  %>% mutate_at(character_to_factor, as.factor) 
diff --git a/next_generation_sequencing/deprecatedCode/034_readin-feature-files.R b/next_generation_sequencing/deprecatedCode/034_readin-feature-files.R
deleted file mode 100755
index ef3fa54..0000000
--- a/next_generation_sequencing/deprecatedCode/034_readin-feature-files.R
+++ /dev/null
@@ -1,89 +0,0 @@
-
-#Gets relative paths for the directories in data folder and assigns them to variables
-if(grepl("Windows", osVersion)){
-  source("./scripts/assign-directory-variables.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/assign-directory-variables.R")
-}
-
-#Load packages required by script ----
-package_to_check <- c("tidyverse","tools","genomation","ChIPpeakAnno","GenomicFeatures","ShortRead")
-loadPackages(package_to_check)
-
-# Loading feature files in working directory ----
-
-#Get folder that contains feature files. Identified by having Feature in name
-feature_folder <- list.dirs(recursive = FALSE)[grepl("Features", list.dirs(recursive = FALSE))]
-if(grepl("Windows", osVersion)){
-  feature_folder <- list.dirs(recursive = FALSE)[grepl("Features", list.dirs(recursive = FALSE))]
-} else if (grepl("Linux", osVersion)){
-  feature_folder <- '../feature-files'
-}
-#/home/luised94/data/feature-files or ../feature-files when on cluster 
-
-#Define files that I want to load 
-annofile_extension <- c("*.bed", "*.gff3", "*.xls", "*.tab")
-
-#Create list, find files in feature_folder that contain annotations of interest
-feature_files <- c()
-for (extension in 1:length(annofile_extension)){
-  feature_files<- c(feature_files, 
-                    list.files(feature_folder, pattern = annofile_extension[extension]))
-}
-
-#Check file extensions
-unique(tools::file_ext(feature_files))
-
-#Create variable for genome to normalize chromosome name ----
-genome_file_path <- paste(genome_directory[1], "sacCer3.fasta", sep = "/")
-sacCer3 <- readFasta(genome_file_path)
-sacCer3_df <- data.frame(chrom = names(as(sacCer3, "DNAStringSet")), size = width(sacCer3)) %>% filter(chrom != "chrM")
-rm(sacCer3)
-
-
-
-#For all of the dataframes to be created, print the name of the file to be imported ----
-#Names for the data frames to be created 
-# df_names <- c("ChExMix","MEME","XUTs","CUTs","ORF","Nucleosome",
-#               "timing", "Rhee" , "SGD_features", "SUTs", "G2_orc")
-
-for(i in 1:length(df_names)){
-  #Print file used
-  print(feature_files[grepl(df_names[i], feature_files)])
-  #Create filepath to the file by identifying filename that contains name to be used for dataframe
-  #Use grepl to determine if df_name is in feature_files and index feature_files with it
-  filepath <- file.path(feature_folder, feature_files[grepl(df_names[i], feature_files)])
-  
-  
-  #if structure to load in file depending on file extension
-  if (grepl("xls|xlsx", tools::file_ext(filepath))){
-    assign(df_names[i], readxl::read_excel(filepath))
-    if(df_names[i] == "timing"){
-      assign(df_names[i], get(df_names[i]) %>% as.data.frame() %>% dplyr::select(1:7) %>% filter(!is.na(Chromosome)))
-    } 
-    
-  } else if(tools::file_ext(filepath) == "bed"){
-    assign(df_names[i], readBed(filepath))
-    print(seqlevelsStyle(eval(parse(text = df_names[i]))))
-    
-  } else if(tools::file_ext(filepath) == "gff3"){
-    assign(df_names[i], toGRanges(makeTxDbFromGFF(filepath), format = 'gene'))
-    print(seqlevelsStyle(eval(parse(text = df_names[i]))))
-    
-  } else if(tools::file_ext(filepath) == "tab"){
-    assign(df_names[i], read.delim(filepath, header = FALSE))
-    if(df_names[i] == "SGD_features"){
-      assign(df_names[i], get(df_names[i]) %>% as.data.frame() %>% dplyr::select(c(9,10,11,12,2,4,5)))
-    } 
-  }
-} 
-
-#Prepare the sacCer3 dataframe with the information to assign necessary variables for reading in track/bigwig data ----
-sacCer3_df <- sacCer3_df %>% mutate(gr_var = paste(chrom, "_gr", sep = ""),
-                                    bw_var = paste(chrom, "_bw", sep = ""),
-                                    track_var = paste(chrom, "_track", sep = ""),
-                                    origin_gr_var = paste("origin_", chrom, "_track", sep = ""),
-                                    chr_df= paste(chrom, "_df",sep = ""),
-                                    origin_track_var = paste("origin_", chrom, "_track",sep = ""))
-#Construct a GRange object from timing dataframe
-#GRanges(seqnames = timing$Chromosome, ranges = IRanges(start = timing$Position-100, end = timing$Position+100), strand = "*", timing$T1/2)
diff --git a/next_generation_sequencing/deprecatedCode/035_regenerate-comparison-coverage-plots.R b/next_generation_sequencing/deprecatedCode/035_regenerate-comparison-coverage-plots.R
deleted file mode 100755
index b66a4a8..0000000
--- a/next_generation_sequencing/deprecatedCode/035_regenerate-comparison-coverage-plots.R
+++ /dev/null
@@ -1,172 +0,0 @@
-#Run with: ----
-#Login to luria via cmd line using ssh  -A -Y account@luria.mit.edu using your username and password.
-#Add R module using module add r/4.2.0
-#$sbatch sbatch-regenerate-comparison-plots.sh 7 "chr7_plot_input"
-#See ./regenerate-comparison-coverage-plots.R to get parameters used for plot. 
-
-#Get time at start of script ----
-start_date <- Sys.time()
-start_time <- as.character(stringr::str_replace_all(start_date, pattern = ":| |-", replacement="_"))
-print(paste("Script start:", start_date, sep = " "))
-
-#Prepares directory variables and the dataframe that contains paths, connections and variables to the fastq samples ----
-#This script sources assign-directory-variables.R, which in turn also sources ngs-functions.R
-if(grepl("Windows", osVersion)){
-  source("./scripts/prepare-samples-dataframe.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/prepare-samples-dataframe.R")
-}
-
-package_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead")
-loadPackages(package_to_check) #Load packages and output message when done or if any package didnt load. Takes list of characters.
-
-#Read in feature files according to their file type. ----
-#Variables to be visualized with track data. 
-#Also creates the sacCer3_df variable with the variables for assigining track data.
-#Set df_names variables with any of the below options.
-# df_names <- c("ChExMix","MEME","XUTs","CUTs","ORF","Nucleosome",
-#               "timing", "Rhee" , "SGD_features", "SUTs", "G2_orc")
-df_names <- c("timing")
-
-if(grepl("Windows", osVersion)){
-  source("./scripts/readin-feature-files.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/readin-feature-files.R")
-}
-
-#Read in tab-delimited text file with info for each sample. Made in ./scripts/fastq-bmc-info-dataframe.R -----
-if(grepl("Windows", osVersion)){
-  fastq_info <- read.delim("./scripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-} else if (grepl("Linux", osVersion)){
-  fastq_info <- read.delim("../rscripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-  fastq_info <- bind_cols(fastq_info, sample_paths_df)
-}
-
-
-#Turn character columns into factors to reorder according to the levels variable.
-#Also use for plotting on Gviz. 
-#Columns to leave as characters. Then select the columns to be turned into factors
-columns_to_exclude <- c("BMC", "Index", "V2", "V3","sname", "Sample_names", "..data","original","track","fq", "bw","sorted")
-character_to_factor <- fastq_info %>% select_if(is.character) %>% dplyr::select(-contains(columns_to_exclude)) %>% colnames(.)
-
-#Use to reorder the columns. Does not generalize!!! 
-order_of_columns <- list(c(1,3,2), c(3,1,2,4), c(1:2),c(1:2),c(2,1,3,5,4),c(3,4,2,1),c(1:3))
-
-invisible(mapply(function(char_to_factor, order_for_columns){
-  #Get the reordered levels. Determined by watching the original order. 
-  reorder_levels <- levels(factor(as.factor(fastq_info[, char_to_factor])))[order_for_columns]
-  fastq_info[, char_to_factor] <<- factor(as.factor(fastq_info[, char_to_factor]), levels = reorder_levels)
-}, char_to_factor = character_to_factor, order_for_columns = order_of_columns, SIMPLIFY = FALSE))
-
-if(fastq_info %>% select_if(is.factor) %>% colnames(.) %>% length() == length(character_to_factor)){
-  print("Columns refactored")
-}
-
-#Handle script arguments ---- 
-#If running in windows, assign chromosome number and file id manually. 
-#If running on linux system ( i. e. the Luria cluster) assign based on arguments passed to bash script
-if(grepl("Windows", osVersion)){
-  chromosome_number <- 7
-  #String that will be used to locate the files with inputs for plots to repeat.
-  #String has to be found in any of the txt files in plot_folder. Example is "chr7_plot_input"
-  file_id <- args[2]
-  
-} else if (grepl("Linux", osVersion)){
-  args = commandArgs(trailingOnly=TRUE)
-  if (length(args)==0) {
-    stop("Pass chromosome number to script as a number.", call.=FALSE)
-  } else if (length(args)==1) {
-    # default output file
-    stop("Pass string to find a plot text file to repeat.", call.=FALSE)
-  }
-  print(paste("Args used in script are:", args[1], args[2], sep = " "))
-  chromosome_number <- as.numeric(args[1])
-  
-  #String that will be used to locate the files with inputs for plots to repeat.
-  #String has to be found in any of the txt files in plot_folder. Example is "chr7_plot_input"
-  file_id <- args[2]
-}
-
-#Use the dataframe to assign the tracks for the chromosome GRanges and tracks ----
-#See ./ngs-functions.R for assignChrTracks details.
-
-assignChrTracks(sacCer3_df, chromosome_number, timing, 200)
-
-#Get the info to redo the plots ---- 
-#If you want to read the plot file back in, it has to be processed first and then it should be equivalent. 
-if(grepl("Windows", osVersion)){
-  plot_folder <- './reports-and-figures/plots-files'
-} else if (grepl("Linux", osVersion)){
-  plot_folder <- './reports/plots-files'
-}
-
-
-#Get the files with the info to repeat the plots already made.
-library(stringr)
-plot_input_files <- list.files(plot_folder, pattern = ".txt", full.names = TRUE)[grepl(file_id, list.files(plot_folder, pattern = ".txt"))]
-plot_input <- lapply(plot_input_files, function(file_names){
-  as.data.frame(read.table(file_names, header = TRUE, sep = "\t"))
-})
-plot_input <- bind_rows(plot_input)
-plot_input <- plot_input %>% distinct(plot_names, .keep_all = TRUE)
-
-#Process the set_of_file to turn into column of lists.
-set_of_files_lists <- str_split(str_replace_all(plot_input$set_of_file, pattern = "c|\\(|\\)|,", replacement = ""), pattern = " ")
-plot_input$set_of_files <- I(set_of_files_lists)
-
-plots_to_generate <- plot_input
-plots_to_generate$chromosome_for_plot <- rep(chromosome_number, length(plots_to_generate$set_of_files))
-
-#Create the plots ----
-name_of_files <- c()
-mapply(function(files, names, chr_num){
-  # print(files);print(names)
-  # print(str_replace_all(names, pattern = "_", replacement = " "))
-  #Subset data frame according to files and arrange by sample columns to display according to that order in Gviz
-  subset_df <- fastq_info %>% filter(sname %in% files) %>% arrange(antibody, complement, suppressor, cellcycle, auxin)
-  
-  #check that subset occured correctly. File length should be same as subset row number.
-  if(length(subset_df$sname) != length(files)){
-    print("Subsetting didnt occur properly.")
-    break
-  }
-  
-  #Reset all_tracks variable just in case. Add GenomeAxisTrack at top
-  all_tracks <- list(GenomeAxisTrack(name=paste("Chr ", chr_num, " Axis", sep = "")))
-  
-  #Assign tracks using mapply and sample dataframe in order of subset dataframe. May add rm to get rid of bw_variable if it is too large
-  #Append to all tracks the result of mapply. Assigns bigwig variable subset by chromosome and creates track variable
-  all_tracks <-append(all_tracks, value = mapply(function(bw_con, bw_variable, track_variable, track_name){
-    # print(bw_con)
-    assign(bw_variable,  import(con = bw_con, which = get(sacCer3_df$gr_var[chr_num])))
-    assign(track_variable, DataTrack(get(bw_variable), type = "l", name = track_name))
-  }, bw_con = subset_df$..data.bw.files, bw_variable = subset_df$bw_var, track_variable =  subset_df$track_var, track_name = subset_df$sample_names)
-  )
-  MAX <- -Inf
-  for (track in 1:length(all_tracks)) {
-    if(class(all_tracks[[track]]) != "GenomeAxisTrack"){
-      if(max(all_tracks[track][[1]]@data) > MAX) MAX <- max(all_tracks[track][[1]]@data)
-    }
-  }
-  #Add origin track at the end so that it shows up below the tracks.
-  all_tracks <- append(all_tracks, get(sacCer3_df$origin_track_var[chr_num]))
-  
-  #Create the plot.
-  # print(paste("Creating ", paste("./reports/plots-files/", paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""), sep = ""))
-  svg(paste("./reports/plots-files/", paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""))
-  plotTracks(all_tracks, main = paste(str_replace_all(names, pattern = "_",replacement = " "),  sep = " "), ylim = c(0, MAX * 1.20))
-  dev.off()
-  print(paste(paste("./reports/plots-files/", paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""), "Plot created.", sep = " "))
-  name_of_files <<- append(name_of_files, paste("./reports/plots-files/", paste(start_time,"_",sep = ""), "chr", chr_num, "_", names, ".svg", sep = ""))
-}, files = plots_to_generate$set_of_files, names = plots_to_generate$plot_names, chr_num = chromosome_number)
-
-#Output a table with the values for each variable in the 221024Bel_CHIP experiment ---- 
-#Add name_of_files, chr name and start_time to plots_to_generate dataframe
-plots_to_generate$file_name <- name_of_files
-plots_to_generate$plot_time <- rep(start_time, length(plots_to_generate$set_of_files))
-
-plot_input_file <- paste("./reports/plots-files/", paste(start_time,"_",sep = ""), "chr", chromosome_number, "_", "plot_input.txt", sep = "")
-write.table(plots_to_generate, file = plot_input_file, row.names = FALSE, quote = FALSE, sep = "\t")
-
-print(paste("Plots created. Time Elasped (in mins)", difftime(start_date, Sys.time(), units = "mins"), sep = " "))
-sessionInfo()
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/036_renaming-fastq-files.R b/next_generation_sequencing/deprecatedCode/036_renaming-fastq-files.R
deleted file mode 100755
index 4e2a79c..0000000
--- a/next_generation_sequencing/deprecatedCode/036_renaming-fastq-files.R
+++ /dev/null
@@ -1,77 +0,0 @@
-#Rename fastq files 
-
-# fastq_ids<- read.table('./scripts/fastq_info.txt')
-library(ShortRead)
-library(tidyverse)
-#Renaming files ---- 
-
-fastq_ids <- read.table("../rscripts/fastq_info.txt") #or add an extra ../ depending on current working dir. Most scripts will be set relative to 221024Bel_CHIP
-knitr::kable(fastq_ids)
-#Obtain files that will be renamed 
-fastq_files_to_rename <- list.files(path ='.', pattern = "*.fastq$", recursive = TRUE, full.names = TRUE)[!grepl("unmapped", list.files(path ='.', pattern = "*.fastq$", recursive = TRUE))]
-print(fastq_files_to_rename)
-#Subset the dataframe. I think it was recycling the TRUE values 
-bmc_ids <- fastq_ids[!grepl("nnNnH", fastq_ids$sname),]
-prefix <- './data/fastq-files/'
-grepl()
-
-
-for (i in 1:length(fastq_files_to_rename)) {
-  correct_name <- bmc_ids$sname[as.logical(na.omit(unlist(lapply(bmc_ids$BMC_ID2, grepl, x = fastq_files_to_rename[i]))))]
-  #correct_name <- any(grepl(fastq_ids$BMC_ID2[i], fastq_files_to_rename[grepl(fastq_ids$BMC_ID2[i], fastq_files_to_rename)]))
-  print(correct_name)
-  
-  if (file.exists(fastq_files_to_rename[i])) {
-    # print(i)
-    print(fastq_files_to_rename[i])
-    destination_file <- paste(prefix, correct_name, ".fastq", sep = '')
-    print(destination_file)
-    
-    #File.rename will move the file to new location
-    #file.rename(fastq_files_to_rename[i], replacement_name)
-    #Cat should work but decided to use ShortRead package just in case
-    #cat(fastq_files_to_rename[i], file =  correct_name, append = TRUE)
-    #Using ShortRead package just in case
-    ## open input stream
-    stream <- open(FastqStreamer(fastq_files_to_rename[i]))
-    on.exit(close(stream))
-
-    repeat {
-      ## input chunk
-      tmp_fq <- yield(stream)
-
-      #If input is 0, stop the function
-      if (length(tmp_fq) == 0) {
-        print("Processed all")
-        break
-      }
-      writeFastq(
-        tmp_fq,
-        destination_file,
-        mode = "a",
-        compress = FALSE ,
-        qualityType = "FastqQuality"
-      )
-    }
-  }
-}
-
-#Get the newly created files.
-fastq_files_to_analyze <- list.files(path ='./data/fastq-files', pattern = "*.fastq$", recursive = FALSE, full.names = TRUE)[!grepl("unmapped", list.files(path ='./data/fastq-files', pattern = "*.fastq$", recursive = FALSE))]
-
-
-sum(as.numeric(format(file.size(fastq_files_to_rename), scientific=TRUE))) - sum(as.numeric(format(file.size(fastq_files_to_analyze), scientific=TRUE)))
-sum(as.numeric(format(file.size(fastq_files_to_analyze), scientific=TRUE)))
-# replacement_names <- c()
-# for (i in 1:length(fastq_files_to_rename)){
-#   
-#   print(fastq_files_to_rename[i])
-#   replacement_name <- paste(prefix, paste(fastq_ids$sname[i], ".fastq", sep = ''), sep = '')
-#   print(replacement_name)
-#   print(fastq_ids$V1[i])
-#   replacement_names <- append(replacement_names, replacement_name)
-#   print(i)
-# }
-#After running successfully, the gunzip command was used to unzip the eaton data
-# fastq_ids$file_location <- list.files(path ='.', pattern = "*.fastq.gz|*.fastq$", recursive = TRUE, full.names = TRUE)[!grepl("unmapped", list.files(path ='.', pattern = "*.fastq.gz|*.fastq$", recursive = TRUE))]
-
diff --git a/next_generation_sequencing/deprecatedCode/037_visualize-control-files.R b/next_generation_sequencing/deprecatedCode/037_visualize-control-files.R
deleted file mode 100755
index 5551e42..0000000
--- a/next_generation_sequencing/deprecatedCode/037_visualize-control-files.R
+++ /dev/null
@@ -1,122 +0,0 @@
-#Get time at start of script ----
-start_time <- Sys.time()
-print("Script start")
-
-#Prepares directory variables and the dataframe that contains paths, connections and variables to the fastq samples 
-if(grepl("Windows", osVersion)){
-  source("./scripts/prepare-samples-dataframe.R")
-} else if (grepl("Linux", osVersion)){
-  source("../rscripts/prepare-samples-dataframe.R")
-}
-
-#Load the packages required by the script ----
-package_to_check <- c("QuasR", "GenomicAlignments","Gviz","rtracklayer","ShortRead")
-loadPackages(package_to_check)
-
-#Create variable for genome ----
-genome_file_path <- paste(genome_directory[1], "sacCer3.fasta", sep = "/")
-sacCer3 <- readFasta(genome_file_path)
-# names(as(sacCer3, "DNAStringSet"))
-# width(sacCer3)
-sacCer3_df <- data.frame(chrom = names(as(sacCer3, "DNAStringSet")), size = width(sacCer3)) %>% filter(chrom != "chrM")
-rm(sacCer3)
-
-
-#Output bigwig files by chunking through chromosomes ----
-#Uses mapply to go through bam files and their bw connections. Uses function bamReadPosAndQwidthByChromToRle mostly taken from the introduction to Rsamtools manual
-#https://www.bioconductor.org/packages/release/bioc/vignettes/Rsamtools/inst/doc/Rsamtools-Overview.pdf
-
-mapply(function(bam_files, bw_connection) {
-  print(paste("Working on:", bam_files, sep = " "))
-  cvg <-
-    RleList(
-      mapply(
-        FUN = bamReadPosAndQwidthByChromToRle,
-        chrom_name = sacCer3_df$chrom,
-        bam_file = bam_files,
-        chromosome_size = sacCer3_df$size,
-        SIMPLIFY = FALSE
-      )
-    )
-  print(paste("Exporting:", bw_connection, sep = " "))
-  export.bw(con = bw_connection,
-            object = cvg,
-            format = "bw")
-  
-}, bam_files = sample_paths_df$sorted_bam, bw_connection = sample_paths_df$..data.bw.files)
-
-print(paste("Bigwig Files created. Time Elasped (in mins)", difftime(start_time, Sys.time(), units = "mins"), sep = " "))
-
-paste(as.character(file.size(sample_paths_df$..data.bw.files[1])/10^3), "Kb file size for first file.", sep = "")
-
-#Read in tab-delimited file with info on fastq_files 
-#Read in tab-delimited text file with info for each sample. Made in C:/Users/Luis/Projects/working-on-a-cluster/scripts/fastq-bmc-info-dataframe.R -----
-if(grepl("Windows", osVersion)){
-  fastq_info <- read.delim("./scripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-} else if (grepl("Linux", osVersion)){
-  fastq_info <- read.delim("../rscripts/fastq_info.txt", header = TRUE) %>% arrange(sname)
-  fastq_info <- bind_cols(fastq_info, sample_paths_df)
-}
-
-#Create GRanges object to read in a particular chromosome
-chrXIV.gr <- GRanges(seqnames=c(sacCer3_df$chrom[14]), ranges = IRanges(start = 100000, end = sacCer3_df$size[14]), strand = "*")
-
-#Create genome axis to display 
-all_tracks <- list(GenomeAxisTrack())
-
-#Assign Tracks of interest
-#Can use index or file name to subset the dataframe Wnani,WnayV,nnNnH
-# files_to_visualize <- c("WnNyV", "nnNnH","Wnani")
-# subset_df <- fastq_info %>% filter(sname %in% files_to_visualize)
-subset_df <- sample_paths_df[1:3,]
-#Assign tracks using mapply and sample dataframe. May add rm to get rid of bw_variable if it is too large
-#Does not return bw_variables
-
-all_tracks <-append(all_tracks, value = mapply(function(bw_con, bw_variable, track_variable, track_name){
-  assign(bw_variable,  import(con = bw_con, which = chrXIV.gr))
-  assign(track_variable, DataTrack(get(bw_variable), type = "l", name = track_name, chromosome = "chrXIV"))
-}, bw_con = subset_df$..data.bw.files, bw_variable = subset_df$bw_var, track_variable =  subset_df$track_var, track_name = subset_df$sample_names)
-)
-MAX <- -Inf
-for (track in 1:length(all_tracks)) {
-  if(class(all_tracks[[track]]) != "GenomeAxisTrack"){
-    if(max(all_tracks[track][[1]]@data) > MAX) MAX <- max(all_tracks[track][[1]]@data)
-  }
-}
-plotTracks(all_tracks, main = "Complete View of Chromosome 14", chromosome = "chrXIV", ylim = c(0, MAX * 1.20))
-#format(object.size(get(subset_df$track_var[1]), units = "Kb"))
-
-#Plot Tracks 
-#plotTracks(list(tracks[[1]],tracks[[2]],tracks[[3]]), main = "Eaton view of Chromosome 14", chromosome = "chrXIV")
-
-#mapply output can be used directly in plotTracks. Save to svg. 
-svg(paste("./reports/plots-files/", underscoreDate(), "_Quick_comparison_of_eaton_and_V5_at_Noc", ".svg", sep = ""))
-plotTracks(all_tracks, main = "Complete View of Chromosome 14", chromosome = "chrXIV")
-dev.off()
-
-print(paste("Script complete. Time Elasped (in mins)", difftime(start_time, Sys.time(), units = "mins"), sep = " "))
-
-#Extra code and chunks to confirm how functions works ----
-# #Create the bigwig coverage files for all the bam files. Uses dataframe that holds paths to all the different files
-#This works for toy data
-# mapply(function(bam_files, bw_connection){
-#   alns <- readGAlignments(bam_files)
-#   covs <- coverage(alns)
-#   export.bw(con = bw_connection,
-#             object = covs,
-#             format = "bw")
-# }, bam_files = sample_paths_df$sorted_bam, bw_connection = sample_paths_df$..data.bw.files)
-
-#Export function from rtracklayer package overwrites by default
-# export.bw(con = sample_paths_df$..data.bw.files[1],
-#           object = RleList(cvg1),
-#           format = "bw")
-
-
-# #Establishes connection 
-# BamFile(sample_paths_df$sorted_bam[1])
-# #Reads in the alignment file according to param argument, pass BamFile object.
-# scanBam(sample_paths_df$sorted_bam[1])
-
-#Confirm that levels where same for bam files and sacCer3 
-# levels(aln[[1]]$rname)[!grepl("chrM", levels(aln[[1]]$rname))] == sacCer3_df$chrom
\ No newline at end of file
diff --git a/next_generation_sequencing/deprecatedCode/backup_sampleGridConfig.R b/next_generation_sequencing/deprecatedCode/backup_sampleGridConfig.R
deleted file mode 100644
index 9606de2..0000000
--- a/next_generation_sequencing/deprecatedCode/backup_sampleGridConfig.R
+++ /dev/null
@@ -1,104 +0,0 @@
-#Description: Configuration file that defines the categories of an experiment, creates the combinations of all the variables and then uses a filter function to grab the combinations.
-#USAGE: This is the template for other experiments. Source the sampleGridConfig.R file in the script createSampleGrid.R, not the template file.
-# This shows an example setup for BMC CHIP-seq experiment 240808Bel.
-# @todo: Consider adding a comprehensive list or an alternative file with all of the variables that is generated programatically.
-
-current_experiment = "240808Bel"
-# Create a list with the different categories and variables in the experiment.
-categories <- list(
-    strain_source = c("lemr", "oa"),
-    rescue_allele = c("none", "wt"),
-    mcm_tag = c("none", "2", "7"),
-    auxin_treatment = c("no", "yes"),
-    cell_cycle = c("G1", "M"),
-    antibody = c("Input", "ProtG", "ALFA", "HM1108", "74", "CHA", "11HA")
-)
-
-#Define the indexes for filtering all of the combinations of the variables.
-# Pick one of the variables and define how it is related to the other variables using conditional expressions. For example, for all of the antibodies, define the other conditions it is used with.
-filter_samples <- function(combinations){
-    #is_not <- with(combinations,
-    #)
-    is_input <- with(combinations,
-        rescue_allele == "none" &
-        mcm_tag == "none" &
-        cell_cycle == "M" &
-        antibody == "Input" &
-        ((strain_source == "oa" & auxin_treatment == "no") | strain_source == "lemr")
-    )
-
-    is_protg <- with(combinations,
-            rescue_allele == "wt" &
-            mcm_tag == "none" &
-            cell_cycle == "M" &
-            antibody == "ProtG" &
-            strain_source == "oa" &
-            auxin_treatment == "no"
-    )
-
-    is_alfa <- with(combinations,
-        rescue_allele == "none" &
-        mcm_tag == "none" &
-        cell_cycle == "M" &
-        antibody == "ALFA" &
-        (( strain_source == "oa" & auxin_treatment == "no") | ( strain_source == "lemr"))
-    )
-
-    is_1108 <-  with(combinations,
-            rescue_allele == "none" &
-            mcm_tag == "none" &
-            cell_cycle == "M" &
-            antibody == "HM1108" &
-           (( strain_source == "oa" &  auxin_treatment == "no") | strain_source == "lemr")
-    )
-
-    is_174 <- with(combinations,
-        antibody == "74" &
-        auxin_treatment == "no" &
-        !( strain_source == "lemr" &  rescue_allele == "none") &
-        !( strain_source == "oa" &  rescue_allele == "wt") &
-        !( strain_source == "lemr" &  mcm_tag == "7") &
-        !( strain_source == "oa" &  mcm_tag == "2")
-    )
-
-    is_cha <- with(combinations,
-        antibody == "CHA" &
-        auxin_treatment == "no" &
-        !(strain_source == "lemr" & rescue_allele == "none") &
-        !(strain_source == "oa" & rescue_allele == "wt") &
-        !(strain_source == "lemr" & mcm_tag == "7") &
-        !(strain_source == "oa" & mcm_tag == "2")
-     )
-
-    is_11HA <- with(combinations,
-        antibody == "11HA" &
-        auxin_treatment == "no" &
-        !(strain_source == "lemr" & rescue_allele == "none") &
-        !(strain_source == "oa" & rescue_allele == "wt") &
-        !(strain_source == "lemr" & mcm_tag == "7") &
-        !(strain_source == "oa" & mcm_tag == "2") &
-        !(strain_source == "lemr" & mcm_tag == "none" & rescue_allele == "wt" & cell_cycle == "M")
-    )
-    return(combinations[is_input | is_protg | is_alfa | is_1108 | is_174 | is_cha | is_11HA , ])
-}
-
-sample_table <- filter_samples(expand.grid(categories))
-print(dim(sample_table))
-print(table(sample_table$antibody))
-print(head(sample_table))
-
-sample_table$full_name <- apply(sample_table, 1, paste, collapse = "_")
-sample_table$short_name <- apply(sample_table[,!grepl("full_name", colnames(sample_table))], 1, function(row) paste0(substr(row, 1, 1), collapse = ""))
-
-
-bmc_table <- data.frame(SampleName = sample_table$full_name,
-    Vol..uL = 10,
-    Conc = NA,
-    Type = "ChIP",
-    Genome = "Saccharomyces cerevisiae",
-    Notes = "none",
-    Pool = "A"
-)
-print(head(bmc_table))
-print(ls())
-print("sampleGridConfig section complete")
diff --git a/next_generation_sequencing/interactiveDebugAndTesting/001_setupInteractiveSessionVariables.sh b/next_generation_sequencing/interactiveDebugAndTesting/001_setupInteractiveSessionVariables.sh
deleted file mode 100755
index 9f56841..0000000
--- a/next_generation_sequencing/interactiveDebugAndTesting/001_setupInteractiveSessionVariables.sh
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/bin/bash
-#Description: Initialize variables and modules to test in interactive node. 
-#USAGE:Run source ~/data/lab_utils/next_generation_sequencing/FTQPRC/test_000_createExampleVariables.sh <dir>'
-
-#SETUP
-DIR_TO_PROCESS="$HOME/data/$1"
-REFGENOME_DIR="$HOME/data/REFGENS"
-echo "Loading modules"
-#MODULE_LOAD
-module purge
-module load gnu/5.4.0
-module load bowtie2/2.3.5.1
-module load samtools/1.10
-module load fastqc
-module load python/2.7.13
-module load deeptools/3.0.1
-module load fastp/0.20.0
-module load r/4.2.0
-
-echo "Initializing arrays"
-#INITIALIZE_ARRAY
-mapfile -t FASTQ_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.fastq" ! \( -name "*unmapped*" -o -name "processed_*" \) | sort )
-mapfile -t PROCESSED_FASTQ < <(find "${DIR_TO_PROCESS}" -type f -name "processed_*.fastq" | sort )
-mapfile -t GENOME_PATHS < <(find "${REFGENOME_DIR}" -type f -name "*_refgenome.fna" )
-mapfile -t BAM_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*.bam" | sort)
-mapfile -t BIGWIG_PATHS < <(find "${DIR_TO_PROCESS}" -type f -name "*S288C.bw" | sort )
-
-#This command was used to test curly brace for redirection.
-#{ samtools quickcheck ${BAM_PATHS[0]} && echo "quickcheck ran" || echo "quickcheck fail" ; echo "This command ran" ;} > output.txt
diff --git a/next_generation_sequencing/testData/001_subsampleFastq.sh b/next_generation_sequencing/testData/001_subsampleFastq.sh
deleted file mode 100644
index 4b19cc7..0000000
--- a/next_generation_sequencing/testData/001_subsampleFastq.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-#!bin/bash
-
-main() {
-    get_info_for_experiment
-    determine_set_of_controls_based_on_sample_data
-    grab_and_output_to_to_testData
-}
diff --git a/next_generation_sequencing/testData/test.fastq b/next_generation_sequencing/testData/test.fastq
deleted file mode 100644
index e69de29..0000000
diff --git a/prompts/codeconsolidator.md b/prompts/codeconsolidator.md
deleted file mode 100644
index 41b4fa3..0000000
--- a/prompts/codeconsolidator.md
+++ /dev/null
@@ -1,6 +0,0 @@
-You are the Consolidation and Refactoring Agent is an expert in software engineering tasked with optimizing codebases by consolidating scripts into modular structures, implementing naming conventions for clarity and consistency, refactoring code for readability, efficiency, and reusability, and identifying and removing redundant code. This agent possesses in-depth knowledge of programming languages such as R, bash, and Python, as well as expertise in natural language processing (NLP) techniques for analyzing code content and documentation.
-You have the following tasks
-- Consolidate the code into modular structures.
-- Implement a naming convention for clarity and consistency.
-- Refactor the code for readability, efficiency, and reusability.
-- Identify and remove redundant code.
diff --git a/renv.lock b/renv.lock
new file mode 100644
index 0000000..5859f22
--- /dev/null
+++ b/renv.lock
@@ -0,0 +1,3475 @@
+{
+  "R": {
+    "Version": "4.2.0",
+    "Repositories": [
+      {
+        "Name": "BioCsoft",
+        "URL": "https://bioconductor.org/packages/3.16/bioc"
+      },
+      {
+        "Name": "BioCann",
+        "URL": "https://bioconductor.org/packages/3.16/data/annotation"
+      },
+      {
+        "Name": "BioCexp",
+        "URL": "https://bioconductor.org/packages/3.16/data/experiment"
+      },
+      {
+        "Name": "BioCworkflows",
+        "URL": "https://bioconductor.org/packages/3.16/workflows"
+      },
+      {
+        "Name": "BioCbooks",
+        "URL": "https://bioconductor.org/packages/3.16/books"
+      },
+      {
+        "Name": "CRAN",
+        "URL": "https://cloud.r-project.org"
+      }
+    ]
+  },
+  "Bioconductor": {
+    "Version": "3.16"
+  },
+  "Packages": {
+    "AnnotationDbi": {
+      "Package": "AnnotationDbi",
+      "Version": "1.60.2",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "Biobase",
+        "BiocGenerics",
+        "DBI",
+        "IRanges",
+        "KEGGREST",
+        "R",
+        "RSQLite",
+        "S4Vectors",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "14966d2e5ffaa621945cc2b9a115f377"
+    },
+    "AnnotationFilter": {
+      "Package": "AnnotationFilter",
+      "Version": "1.22.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "GenomicRanges",
+        "R",
+        "lazyeval",
+        "methods",
+        "utils"
+      ],
+      "Hash": "a75bb950463d121b4ac6209c182677ed"
+    },
+    "BH": {
+      "Package": "BH",
+      "Version": "1.84.0-0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "a8235afbcd6316e6e91433ea47661013"
+    },
+    "BSgenome": {
+      "Package": "BSgenome",
+      "Version": "1.66.3",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "Rsamtools",
+        "S4Vectors",
+        "XVector",
+        "matrixStats",
+        "methods",
+        "rtracklayer",
+        "stats",
+        "utils"
+      ],
+      "Hash": "f56058d8ac778daf0e50e3ad884fc5a2"
+    },
+    "Biobase": {
+      "Package": "Biobase",
+      "Version": "2.58.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "R",
+        "methods",
+        "utils"
+      ],
+      "Hash": "96e8b620897cc9a03deff4097fa8b265"
+    },
+    "BiocFileCache": {
+      "Package": "BiocFileCache",
+      "Version": "2.6.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "DBI",
+        "R",
+        "RSQLite",
+        "curl",
+        "dbplyr",
+        "dplyr",
+        "filelock",
+        "httr",
+        "methods",
+        "rappdirs",
+        "stats",
+        "utils"
+      ],
+      "Hash": "ad6dd401c5bf0cf4761c5a5cd646583a"
+    },
+    "BiocGenerics": {
+      "Package": "BiocGenerics",
+      "Version": "0.44.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "R",
+        "graphics",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "0de19224c2cd94f48fbc0d0bc663ce3b"
+    },
+    "BiocIO": {
+      "Package": "BiocIO",
+      "Version": "1.8.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "R",
+        "S4Vectors",
+        "methods",
+        "tools"
+      ],
+      "Hash": "70ecfee078be043906300ce6c6fd710a"
+    },
+    "BiocManager": {
+      "Package": "BiocManager",
+      "Version": "1.30.25",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "utils"
+      ],
+      "Hash": "3aec5928ca10897d7a0a1205aae64627"
+    },
+    "BiocParallel": {
+      "Package": "BiocParallel",
+      "Version": "1.32.6",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BH",
+        "R",
+        "codetools",
+        "cpp11",
+        "futile.logger",
+        "methods",
+        "parallel",
+        "snow",
+        "stats",
+        "utils"
+      ],
+      "Hash": "bcdac842fb312bb25f9f82bc141889e5"
+    },
+    "BiocVersion": {
+      "Package": "BiocVersion",
+      "Version": "3.16.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "44c5824508b9a10e52dbb505c34fa880"
+    },
+    "Biostrings": {
+      "Package": "Biostrings",
+      "Version": "2.66.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "GenomeInfoDb",
+        "IRanges",
+        "R",
+        "S4Vectors",
+        "XVector",
+        "crayon",
+        "grDevices",
+        "graphics",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "5666d0758c6dea426c81b3f644c28eaf"
+    },
+    "ChIPpeakAnno": {
+      "Package": "ChIPpeakAnno",
+      "Version": "3.32.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "BiocGenerics",
+        "Biostrings",
+        "DBI",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicFeatures",
+        "GenomicRanges",
+        "IRanges",
+        "InteractionSet",
+        "KEGGREST",
+        "R",
+        "RBGL",
+        "Rsamtools",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "VennDiagram",
+        "biomaRt",
+        "dplyr",
+        "ensembldb",
+        "ggplot2",
+        "grDevices",
+        "graph",
+        "graphics",
+        "grid",
+        "matrixStats",
+        "methods",
+        "multtest",
+        "regioneR",
+        "rtracklayer",
+        "stats",
+        "utils"
+      ],
+      "Hash": "4df153912959450a0c2d7e6d646b6f14"
+    },
+    "DBI": {
+      "Package": "DBI",
+      "Version": "1.2.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods"
+      ],
+      "Hash": "065ae649b05f1ff66bb0c793107508f5"
+    },
+    "DelayedArray": {
+      "Package": "DelayedArray",
+      "Version": "0.24.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "IRanges",
+        "Matrix",
+        "MatrixGenerics",
+        "R",
+        "S4Vectors",
+        "methods",
+        "stats",
+        "stats4"
+      ],
+      "Hash": "51da2aa8f52a4f3f08b65a8f1c62530e"
+    },
+    "Formula": {
+      "Package": "Formula",
+      "Version": "1.2-5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "stats"
+      ],
+      "Hash": "7a29697b75e027767a53fde6c903eca7"
+    },
+    "GenomeInfoDb": {
+      "Package": "GenomeInfoDb",
+      "Version": "1.34.9",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "GenomeInfoDbData",
+        "IRanges",
+        "R",
+        "RCurl",
+        "S4Vectors",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "c8adcafcf06ec6681c35eafa837c69bb"
+    },
+    "GenomeInfoDbData": {
+      "Package": "GenomeInfoDbData",
+      "Version": "1.2.9",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "618b1efac0f8b8c130afac3e0eafd47c"
+    },
+    "GenomicAlignments": {
+      "Package": "GenomicAlignments",
+      "Version": "1.34.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "BiocParallel",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "Rsamtools",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "8d66ff1e23d5fe50481f3cdcb9160891"
+    },
+    "GenomicFeatures": {
+      "Package": "GenomicFeatures",
+      "Version": "1.50.4",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "Biobase",
+        "BiocGenerics",
+        "BiocIO",
+        "Biostrings",
+        "DBI",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "RCurl",
+        "RSQLite",
+        "S4Vectors",
+        "XVector",
+        "biomaRt",
+        "methods",
+        "rtracklayer",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "61e396f773cfa09ae1d49f299522ead2"
+    },
+    "GenomicFiles": {
+      "Package": "GenomicFiles",
+      "Version": "1.34.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "BiocParallel",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicRanges",
+        "IRanges",
+        "MatrixGenerics",
+        "R",
+        "Rsamtools",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "VariantAnnotation",
+        "methods",
+        "rtracklayer"
+      ],
+      "Hash": "4ed71e8b26f06c4ce70416bcba3ad04c"
+    },
+    "GenomicRanges": {
+      "Package": "GenomicRanges",
+      "Version": "1.50.2",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "GenomeInfoDb",
+        "IRanges",
+        "R",
+        "S4Vectors",
+        "XVector",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "24a5b855811e94b5bd9365a11fe5b2a8"
+    },
+    "Gviz": {
+      "Package": "Gviz",
+      "Version": "1.42.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "BSgenome",
+        "Biobase",
+        "BiocGenerics",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicFeatures",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "RColorBrewer",
+        "Rsamtools",
+        "S4Vectors",
+        "XVector",
+        "biomaRt",
+        "biovizBase",
+        "digest",
+        "ensembldb",
+        "grDevices",
+        "graphics",
+        "grid",
+        "lattice",
+        "latticeExtra",
+        "matrixStats",
+        "methods",
+        "rtracklayer",
+        "stats",
+        "utils"
+      ],
+      "Hash": "6c7cf94eb8afa4ec6258eed58faa4890"
+    },
+    "Hmisc": {
+      "Package": "Hmisc",
+      "Version": "5.1-3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "Formula",
+        "R",
+        "base64enc",
+        "cluster",
+        "colorspace",
+        "data.table",
+        "foreign",
+        "ggplot2",
+        "grid",
+        "gridExtra",
+        "gtable",
+        "htmlTable",
+        "htmltools",
+        "knitr",
+        "methods",
+        "nnet",
+        "rmarkdown",
+        "rpart",
+        "viridis"
+      ],
+      "Hash": "9a446aea30bff7e8ee20f4c0973e8851"
+    },
+    "IRanges": {
+      "Package": "IRanges",
+      "Version": "2.32.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "R",
+        "S4Vectors",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "c4d63bc50829c9d3ac6d4500bea17b06"
+    },
+    "InteractionSet": {
+      "Package": "InteractionSet",
+      "Version": "1.26.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "Matrix",
+        "Rcpp",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "methods"
+      ],
+      "Hash": "021183e893f28b9e23599e1661f0421f"
+    },
+    "KEGGREST": {
+      "Package": "KEGGREST",
+      "Version": "1.38.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "Biostrings",
+        "R",
+        "httr",
+        "methods",
+        "png"
+      ],
+      "Hash": "c868da6f0062c0b977823dec78e92b67"
+    },
+    "MASS": {
+      "Package": "MASS",
+      "Version": "7.3-56",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "af0e1955cb80bb36b7988cc657db261e"
+    },
+    "Matrix": {
+      "Package": "Matrix",
+      "Version": "1.4-1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "graphics",
+        "grid",
+        "lattice",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "699c47c606293bdfbc9fd78a93c9c8fe"
+    },
+    "MatrixGenerics": {
+      "Package": "MatrixGenerics",
+      "Version": "1.10.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "matrixStats",
+        "methods"
+      ],
+      "Hash": "0e510b9ce6c89e37c2ed562a624afbe0"
+    },
+    "ProtGenerics": {
+      "Package": "ProtGenerics",
+      "Version": "1.30.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "methods"
+      ],
+      "Hash": "f097f63d746eff6c10b80338a930034d"
+    },
+    "QuasR": {
+      "Package": "QuasR",
+      "Version": "1.38.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "BSgenome",
+        "Biobase",
+        "BiocGenerics",
+        "BiocParallel",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicFeatures",
+        "GenomicFiles",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "Rbowtie",
+        "Rhtslib",
+        "Rsamtools",
+        "S4Vectors",
+        "ShortRead",
+        "grDevices",
+        "graphics",
+        "methods",
+        "parallel",
+        "rtracklayer",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "0762672a70c3f5902205b9bf13017fae"
+    },
+    "R.methodsS3": {
+      "Package": "R.methodsS3",
+      "Version": "1.8.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "278c286fd6e9e75d0c2e8f731ea445c8"
+    },
+    "R.oo": {
+      "Package": "R.oo",
+      "Version": "1.26.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R.methodsS3",
+        "methods",
+        "utils"
+      ],
+      "Hash": "4fed809e53ddb5407b3da3d0f572e591"
+    },
+    "R.utils": {
+      "Package": "R.utils",
+      "Version": "2.12.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R.methodsS3",
+        "R.oo",
+        "methods",
+        "tools",
+        "utils"
+      ],
+      "Hash": "3dc2829b790254bfba21e60965787651"
+    },
+    "R6": {
+      "Package": "R6",
+      "Version": "2.5.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "470851b6d5d0ac559e9d01bb352b4021"
+    },
+    "RBGL": {
+      "Package": "RBGL",
+      "Version": "1.74.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BH",
+        "graph",
+        "methods"
+      ],
+      "Hash": "7855601b98e3cf9f9a4022b247fcc612"
+    },
+    "RColorBrewer": {
+      "Package": "RColorBrewer",
+      "Version": "1.1-3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "45f0398006e83a5b10b72a90663d8d8c"
+    },
+    "RCurl": {
+      "Package": "RCurl",
+      "Version": "1.98-1.16",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "bitops",
+        "methods"
+      ],
+      "Hash": "ddbdf53d15b47be4407ede6914f56fbb"
+    },
+    "RSQLite": {
+      "Package": "RSQLite",
+      "Version": "2.3.7",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "DBI",
+        "R",
+        "bit64",
+        "blob",
+        "cpp11",
+        "memoise",
+        "methods",
+        "pkgconfig",
+        "plogr",
+        "rlang"
+      ],
+      "Hash": "46b45a4dd7bb0e0f4e3fc22245817240"
+    },
+    "Rbowtie": {
+      "Package": "Rbowtie",
+      "Version": "1.38.0",
+      "Source": "Bioconductor",
+      "Hash": "a150bb12132ffbd1b88b7477e2eb6f64"
+    },
+    "Rcpp": {
+      "Package": "Rcpp",
+      "Version": "1.0.13",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "methods",
+        "utils"
+      ],
+      "Hash": "f27411eb6d9c3dada5edd444b8416675"
+    },
+    "RcppEigen": {
+      "Package": "RcppEigen",
+      "Version": "0.3.4.0.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "Rcpp",
+        "stats",
+        "utils"
+      ],
+      "Hash": "4ac8e423216b8b70cb9653d1b3f71eb9"
+    },
+    "Rhtslib": {
+      "Package": "Rhtslib",
+      "Version": "2.0.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "zlibbioc"
+      ],
+      "Hash": "068989f864b5abd94588d587e06e85bf"
+    },
+    "Rsamtools": {
+      "Package": "Rsamtools",
+      "Version": "2.14.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "BiocParallel",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "Rhtslib",
+        "S4Vectors",
+        "XVector",
+        "bitops",
+        "methods",
+        "stats",
+        "utils",
+        "zlibbioc"
+      ],
+      "Hash": "7bdb3648f3c94545ddaea548fbb253ff"
+    },
+    "S4Vectors": {
+      "Package": "S4Vectors",
+      "Version": "0.36.2",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "R",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "8f371bac4b09106b66aeff10b85bd933"
+    },
+    "ShortRead": {
+      "Package": "ShortRead",
+      "Version": "1.56.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "Biobase",
+        "BiocGenerics",
+        "BiocParallel",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicRanges",
+        "IRanges",
+        "Rhtslib",
+        "Rsamtools",
+        "S4Vectors",
+        "XVector",
+        "hwriter",
+        "lattice",
+        "latticeExtra",
+        "methods",
+        "zlibbioc"
+      ],
+      "Hash": "cbdf72b999a788d7335659ac4be99662"
+    },
+    "SummarizedExperiment": {
+      "Package": "SummarizedExperiment",
+      "Version": "1.28.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "Biobase",
+        "BiocGenerics",
+        "DelayedArray",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "Matrix",
+        "MatrixGenerics",
+        "R",
+        "S4Vectors",
+        "methods",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "ae913ff30bce222686e66e45448fcfb6"
+    },
+    "VariantAnnotation": {
+      "Package": "VariantAnnotation",
+      "Version": "1.44.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "BSgenome",
+        "Biobase",
+        "BiocGenerics",
+        "Biostrings",
+        "DBI",
+        "GenomeInfoDb",
+        "GenomicFeatures",
+        "GenomicRanges",
+        "IRanges",
+        "MatrixGenerics",
+        "R",
+        "Rhtslib",
+        "Rsamtools",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "XVector",
+        "methods",
+        "rtracklayer",
+        "utils",
+        "zlibbioc"
+      ],
+      "Hash": "c5a2ab880906b1ef58e643b8dca53cc9"
+    },
+    "VennDiagram": {
+      "Package": "VennDiagram",
+      "Version": "1.7.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "futile.logger",
+        "grid",
+        "methods"
+      ],
+      "Hash": "6b628f0aee185fdb5d6ce2b5eb477e52"
+    },
+    "XML": {
+      "Package": "XML",
+      "Version": "3.99-0.17",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods",
+        "utils"
+      ],
+      "Hash": "bc2a8a1139d8d4bd9c46086708945124"
+    },
+    "XVector": {
+      "Package": "XVector",
+      "Version": "0.38.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "IRanges",
+        "R",
+        "S4Vectors",
+        "methods",
+        "tools",
+        "utils",
+        "zlibbioc"
+      ],
+      "Hash": "83b80e46ac75044bc7516d0dc8116165"
+    },
+    "askpass": {
+      "Package": "askpass",
+      "Version": "1.2.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "sys"
+      ],
+      "Hash": "c39f4155b3ceb1a9a2799d700fbd4b6a"
+    },
+    "assertthat": {
+      "Package": "assertthat",
+      "Version": "0.2.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "tools"
+      ],
+      "Hash": "50c838a310445e954bc13f26f26a6ecf"
+    },
+    "backports": {
+      "Package": "backports",
+      "Version": "1.5.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "e1e1b9d75c37401117b636b7ae50827a"
+    },
+    "base64enc": {
+      "Package": "base64enc",
+      "Version": "0.1-3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "543776ae6848fde2f48ff3816d0628bc"
+    },
+    "biomaRt": {
+      "Package": "biomaRt",
+      "Version": "2.54.1",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "BiocFileCache",
+        "XML",
+        "digest",
+        "httr",
+        "methods",
+        "progress",
+        "rappdirs",
+        "stringr",
+        "utils",
+        "xml2"
+      ],
+      "Hash": "897a0551d269002ef9d8bf185cd33d8b"
+    },
+    "biovizBase": {
+      "Package": "biovizBase",
+      "Version": "1.46.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "AnnotationFilter",
+        "BiocGenerics",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicFeatures",
+        "GenomicRanges",
+        "Hmisc",
+        "IRanges",
+        "R",
+        "RColorBrewer",
+        "Rsamtools",
+        "S4Vectors",
+        "SummarizedExperiment",
+        "VariantAnnotation",
+        "dichromat",
+        "ensembldb",
+        "grDevices",
+        "methods",
+        "rlang",
+        "scales",
+        "stats"
+      ],
+      "Hash": "197f352614a70852545e29bd67efbce1"
+    },
+    "bit": {
+      "Package": "bit",
+      "Version": "4.5.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "5dc7b2677d65d0e874fc4aaf0e879987"
+    },
+    "bit64": {
+      "Package": "bit64",
+      "Version": "4.5.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "bit",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "e84984bf5f12a18628d9a02322128dfd"
+    },
+    "bitops": {
+      "Package": "bitops",
+      "Version": "1.0-9",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "d972ef991d58c19e6efa71b21f5e144b"
+    },
+    "blob": {
+      "Package": "blob",
+      "Version": "1.2.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "methods",
+        "rlang",
+        "vctrs"
+      ],
+      "Hash": "40415719b5a479b87949f3aa0aee737c"
+    },
+    "brew": {
+      "Package": "brew",
+      "Version": "1.0-10",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "8f4a384e19dccd8c65356dc096847b76"
+    },
+    "brio": {
+      "Package": "brio",
+      "Version": "1.1.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "c1ee497a6d999947c2c224ae46799b1a"
+    },
+    "broom": {
+      "Package": "broom",
+      "Version": "1.0.7",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "backports",
+        "dplyr",
+        "generics",
+        "glue",
+        "lifecycle",
+        "purrr",
+        "rlang",
+        "stringr",
+        "tibble",
+        "tidyr"
+      ],
+      "Hash": "8fcc818f3b9887aebaf206f141437cc9"
+    },
+    "bslib": {
+      "Package": "bslib",
+      "Version": "0.8.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "base64enc",
+        "cachem",
+        "fastmap",
+        "grDevices",
+        "htmltools",
+        "jquerylib",
+        "jsonlite",
+        "lifecycle",
+        "memoise",
+        "mime",
+        "rlang",
+        "sass"
+      ],
+      "Hash": "b299c6741ca9746fb227debcb0f9fb6c"
+    },
+    "cachem": {
+      "Package": "cachem",
+      "Version": "1.1.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "fastmap",
+        "rlang"
+      ],
+      "Hash": "cd9a672193789068eb5a2aad65a0dedf"
+    },
+    "callr": {
+      "Package": "callr",
+      "Version": "3.7.6",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "processx",
+        "utils"
+      ],
+      "Hash": "d7e13f49c19103ece9e58ad2d83a7354"
+    },
+    "cellranger": {
+      "Package": "cellranger",
+      "Version": "1.1.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "rematch",
+        "tibble"
+      ],
+      "Hash": "f61dbaec772ccd2e17705c1e872e9e7c"
+    },
+    "checkmate": {
+      "Package": "checkmate",
+      "Version": "2.3.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "backports",
+        "utils"
+      ],
+      "Hash": "0e14e01ce07e7c88fd25de6d4260d26b"
+    },
+    "cli": {
+      "Package": "cli",
+      "Version": "3.6.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "b21916dd77a27642b447374a5d30ecf3"
+    },
+    "clipr": {
+      "Package": "clipr",
+      "Version": "0.8.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "utils"
+      ],
+      "Hash": "3f038e5ac7f41d4ac41ce658c85e3042"
+    },
+    "cluster": {
+      "Package": "cluster",
+      "Version": "2.1.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics",
+        "stats",
+        "utils"
+      ],
+      "Hash": "c5f8447373ec2a0f593c694024e5b7ee"
+    },
+    "codetools": {
+      "Package": "codetools",
+      "Version": "0.2-18",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "019388fc48e48b3da0d3a76ff94608a8"
+    },
+    "colorspace": {
+      "Package": "colorspace",
+      "Version": "2.1-1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics",
+        "methods",
+        "stats"
+      ],
+      "Hash": "d954cb1c57e8d8b756165d7ba18aa55a"
+    },
+    "commonmark": {
+      "Package": "commonmark",
+      "Version": "1.9.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "14eb0596f987c71535d07c3aff814742"
+    },
+    "conflicted": {
+      "Package": "conflicted",
+      "Version": "1.2.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "memoise",
+        "rlang"
+      ],
+      "Hash": "bb097fccb22d156624fd07cd2894ddb6"
+    },
+    "cpp11": {
+      "Package": "cpp11",
+      "Version": "0.5.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "91570bba75d0c9d3f1040c835cee8fba"
+    },
+    "crayon": {
+      "Package": "crayon",
+      "Version": "1.5.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "grDevices",
+        "methods",
+        "utils"
+      ],
+      "Hash": "859d96e65ef198fd43e82b9628d593ef"
+    },
+    "credentials": {
+      "Package": "credentials",
+      "Version": "2.0.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "askpass",
+        "curl",
+        "jsonlite",
+        "openssl",
+        "sys"
+      ],
+      "Hash": "09fd631e607a236f8cc7f9604db32cb8"
+    },
+    "curl": {
+      "Package": "curl",
+      "Version": "5.2.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "d91263322a58af798f6cf3b13fd56dde"
+    },
+    "data.table": {
+      "Package": "data.table",
+      "Version": "1.16.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods"
+      ],
+      "Hash": "2e00b378fc3be69c865120d9f313039a"
+    },
+    "dbplyr": {
+      "Package": "dbplyr",
+      "Version": "2.5.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "DBI",
+        "R",
+        "R6",
+        "blob",
+        "cli",
+        "dplyr",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "methods",
+        "pillar",
+        "purrr",
+        "rlang",
+        "tibble",
+        "tidyr",
+        "tidyselect",
+        "utils",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "39b2e002522bfd258039ee4e889e0fd1"
+    },
+    "deldir": {
+      "Package": "deldir",
+      "Version": "2.0-4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics"
+      ],
+      "Hash": "24754fce82729ff85317dd195b6646a8"
+    },
+    "desc": {
+      "Package": "desc",
+      "Version": "1.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "cli",
+        "utils"
+      ],
+      "Hash": "99b79fcbd6c4d1ce087f5c5c758b384f"
+    },
+    "devtools": {
+      "Package": "devtools",
+      "Version": "2.4.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "desc",
+        "ellipsis",
+        "fs",
+        "lifecycle",
+        "memoise",
+        "miniUI",
+        "pkgbuild",
+        "pkgdown",
+        "pkgload",
+        "profvis",
+        "rcmdcheck",
+        "remotes",
+        "rlang",
+        "roxygen2",
+        "rversions",
+        "sessioninfo",
+        "stats",
+        "testthat",
+        "tools",
+        "urlchecker",
+        "usethis",
+        "utils",
+        "withr"
+      ],
+      "Hash": "ea5bc8b4a6a01e4f12d98b58329930bb"
+    },
+    "dichromat": {
+      "Package": "dichromat",
+      "Version": "2.0-0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "stats"
+      ],
+      "Hash": "16e66f2a483e124af5fc6582d26005f7"
+    },
+    "diffobj": {
+      "Package": "diffobj",
+      "Version": "0.3.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "crayon",
+        "methods",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "bcaa8b95f8d7d01a5dedfd959ce88ab8"
+    },
+    "digest": {
+      "Package": "digest",
+      "Version": "0.6.37",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "33698c4b3127fc9f506654607fb73676"
+    },
+    "downlit": {
+      "Package": "downlit",
+      "Version": "0.4.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "brio",
+        "desc",
+        "digest",
+        "evaluate",
+        "fansi",
+        "memoise",
+        "rlang",
+        "vctrs",
+        "withr",
+        "yaml"
+      ],
+      "Hash": "45a6a596bf0108ee1ff16a040a2df897"
+    },
+    "dplyr": {
+      "Package": "dplyr",
+      "Version": "1.1.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "cli",
+        "generics",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "methods",
+        "pillar",
+        "rlang",
+        "tibble",
+        "tidyselect",
+        "utils",
+        "vctrs"
+      ],
+      "Hash": "fedd9d00c2944ff00a0e2696ccf048ec"
+    },
+    "dtplyr": {
+      "Package": "dtplyr",
+      "Version": "1.3.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "data.table",
+        "dplyr",
+        "glue",
+        "lifecycle",
+        "rlang",
+        "tibble",
+        "tidyselect",
+        "vctrs"
+      ],
+      "Hash": "54ed3ea01b11e81a86544faaecfef8e2"
+    },
+    "ellipsis": {
+      "Package": "ellipsis",
+      "Version": "0.3.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "rlang"
+      ],
+      "Hash": "bb0eec2fe32e88d9e2836c2f73ea2077"
+    },
+    "ensembldb": {
+      "Package": "ensembldb",
+      "Version": "2.22.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "AnnotationDbi",
+        "AnnotationFilter",
+        "Biobase",
+        "BiocGenerics",
+        "Biostrings",
+        "DBI",
+        "GenomeInfoDb",
+        "GenomicFeatures",
+        "GenomicRanges",
+        "IRanges",
+        "ProtGenerics",
+        "R",
+        "RSQLite",
+        "Rsamtools",
+        "S4Vectors",
+        "curl",
+        "methods",
+        "rtracklayer"
+      ],
+      "Hash": "abad22ca000751e019923890dfb9820e"
+    },
+    "evaluate": {
+      "Package": "evaluate",
+      "Version": "1.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "3fd29944b231036ad67c3edb32e02201"
+    },
+    "fansi": {
+      "Package": "fansi",
+      "Version": "1.0.6",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "utils"
+      ],
+      "Hash": "962174cf2aeb5b9eea581522286a911f"
+    },
+    "farver": {
+      "Package": "farver",
+      "Version": "2.1.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "680887028577f3fa2a81e410ed0d6e42"
+    },
+    "fastmap": {
+      "Package": "fastmap",
+      "Version": "1.2.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "aa5e1cd11c2d15497494c5292d7ffcc8"
+    },
+    "filelock": {
+      "Package": "filelock",
+      "Version": "1.0.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "192053c276525c8495ccfd523aa8f2d1"
+    },
+    "fontawesome": {
+      "Package": "fontawesome",
+      "Version": "0.5.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "htmltools",
+        "rlang"
+      ],
+      "Hash": "c2efdd5f0bcd1ea861c2d4e2a883a67d"
+    },
+    "forcats": {
+      "Package": "forcats",
+      "Version": "1.0.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "rlang",
+        "tibble"
+      ],
+      "Hash": "1a0a9a3d5083d0d573c4214576f1e690"
+    },
+    "foreign": {
+      "Package": "foreign",
+      "Version": "0.8-82",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "32b25c97ce306a760c4d9f787991b5d9"
+    },
+    "formatR": {
+      "Package": "formatR",
+      "Version": "1.14",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "63cb26d12517c7863f5abb006c5e0f25"
+    },
+    "fs": {
+      "Package": "fs",
+      "Version": "1.6.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods"
+      ],
+      "Hash": "15aeb8c27f5ea5161f9f6a641fafd93a"
+    },
+    "futile.logger": {
+      "Package": "futile.logger",
+      "Version": "1.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "futile.options",
+        "lambda.r",
+        "utils"
+      ],
+      "Hash": "99f0ace8c05ec7d3683d27083c4f1e7e"
+    },
+    "futile.options": {
+      "Package": "futile.options",
+      "Version": "1.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "0d9bf02413ddc2bbe8da9ce369dcdd2b"
+    },
+    "gargle": {
+      "Package": "gargle",
+      "Version": "1.5.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "fs",
+        "glue",
+        "httr",
+        "jsonlite",
+        "lifecycle",
+        "openssl",
+        "rappdirs",
+        "rlang",
+        "stats",
+        "utils",
+        "withr"
+      ],
+      "Hash": "fc0b272e5847c58cd5da9b20eedbd026"
+    },
+    "generics": {
+      "Package": "generics",
+      "Version": "0.1.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods"
+      ],
+      "Hash": "15e9634c0fcd294799e9b2e929ed1b86"
+    },
+    "gert": {
+      "Package": "gert",
+      "Version": "2.1.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "askpass",
+        "credentials",
+        "openssl",
+        "rstudioapi",
+        "sys",
+        "zip"
+      ],
+      "Hash": "ae855ad6d7be20dd7b05d43d25700398"
+    },
+    "ggplot2": {
+      "Package": "ggplot2",
+      "Version": "3.5.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "MASS",
+        "R",
+        "cli",
+        "glue",
+        "grDevices",
+        "grid",
+        "gtable",
+        "isoband",
+        "lifecycle",
+        "mgcv",
+        "rlang",
+        "scales",
+        "stats",
+        "tibble",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "44c6a2f8202d5b7e878ea274b1092426"
+    },
+    "gh": {
+      "Package": "gh",
+      "Version": "1.4.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "gitcreds",
+        "glue",
+        "httr2",
+        "ini",
+        "jsonlite",
+        "lifecycle",
+        "rlang"
+      ],
+      "Hash": "fbbbc48eba7a6626a08bb365e44b563b"
+    },
+    "gitcreds": {
+      "Package": "gitcreds",
+      "Version": "0.1.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "ab08ac61f3e1be454ae21911eb8bc2fe"
+    },
+    "glue": {
+      "Package": "glue",
+      "Version": "1.8.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods"
+      ],
+      "Hash": "5899f1eaa825580172bb56c08266f37c"
+    },
+    "googledrive": {
+      "Package": "googledrive",
+      "Version": "2.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "gargle",
+        "glue",
+        "httr",
+        "jsonlite",
+        "lifecycle",
+        "magrittr",
+        "pillar",
+        "purrr",
+        "rlang",
+        "tibble",
+        "utils",
+        "uuid",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "e99641edef03e2a5e87f0a0b1fcc97f4"
+    },
+    "googlesheets4": {
+      "Package": "googlesheets4",
+      "Version": "1.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cellranger",
+        "cli",
+        "curl",
+        "gargle",
+        "glue",
+        "googledrive",
+        "httr",
+        "ids",
+        "lifecycle",
+        "magrittr",
+        "methods",
+        "purrr",
+        "rematch2",
+        "rlang",
+        "tibble",
+        "utils",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "d6db1667059d027da730decdc214b959"
+    },
+    "graph": {
+      "Package": "graph",
+      "Version": "1.76.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "R",
+        "methods",
+        "stats",
+        "stats4",
+        "utils"
+      ],
+      "Hash": "ed149e9995e7d1cbfebe1820980eed68"
+    },
+    "gridExtra": {
+      "Package": "gridExtra",
+      "Version": "2.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "grDevices",
+        "graphics",
+        "grid",
+        "gtable",
+        "utils"
+      ],
+      "Hash": "7d7f283939f563670a697165b2cf5560"
+    },
+    "gtable": {
+      "Package": "gtable",
+      "Version": "0.3.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "grid",
+        "lifecycle",
+        "rlang"
+      ],
+      "Hash": "e18861963cbc65a27736e02b3cd3c4a0"
+    },
+    "gtools": {
+      "Package": "gtools",
+      "Version": "3.9.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "methods",
+        "stats",
+        "utils"
+      ],
+      "Hash": "588d091c35389f1f4a9d533c8d709b35"
+    },
+    "haven": {
+      "Package": "haven",
+      "Version": "2.5.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "cpp11",
+        "forcats",
+        "hms",
+        "lifecycle",
+        "methods",
+        "readr",
+        "rlang",
+        "tibble",
+        "tidyselect",
+        "vctrs"
+      ],
+      "Hash": "9171f898db9d9c4c1b2c745adc2c1ef1"
+    },
+    "highr": {
+      "Package": "highr",
+      "Version": "0.11",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "xfun"
+      ],
+      "Hash": "d65ba49117ca223614f71b60d85b8ab7"
+    },
+    "hms": {
+      "Package": "hms",
+      "Version": "1.1.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "lifecycle",
+        "methods",
+        "pkgconfig",
+        "rlang",
+        "vctrs"
+      ],
+      "Hash": "b59377caa7ed00fa41808342002138f9"
+    },
+    "htmlTable": {
+      "Package": "htmlTable",
+      "Version": "2.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "checkmate",
+        "htmltools",
+        "htmlwidgets",
+        "knitr",
+        "magrittr",
+        "methods",
+        "rstudioapi",
+        "stringr"
+      ],
+      "Hash": "ca027d8771f2c039aed82f00a81e725b"
+    },
+    "htmltools": {
+      "Package": "htmltools",
+      "Version": "0.5.8.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "base64enc",
+        "digest",
+        "fastmap",
+        "grDevices",
+        "rlang",
+        "utils"
+      ],
+      "Hash": "81d371a9cc60640e74e4ab6ac46dcedc"
+    },
+    "htmlwidgets": {
+      "Package": "htmlwidgets",
+      "Version": "1.6.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "grDevices",
+        "htmltools",
+        "jsonlite",
+        "knitr",
+        "rmarkdown",
+        "yaml"
+      ],
+      "Hash": "04291cc45198225444a397606810ac37"
+    },
+    "httpuv": {
+      "Package": "httpuv",
+      "Version": "1.6.15",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "Rcpp",
+        "later",
+        "promises",
+        "utils"
+      ],
+      "Hash": "d55aa087c47a63ead0f6fc10f8fa1ee0"
+    },
+    "httr": {
+      "Package": "httr",
+      "Version": "1.4.7",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "curl",
+        "jsonlite",
+        "mime",
+        "openssl"
+      ],
+      "Hash": "ac107251d9d9fd72f0ca8049988f1d7f"
+    },
+    "httr2": {
+      "Package": "httr2",
+      "Version": "1.0.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "cli",
+        "curl",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "openssl",
+        "rappdirs",
+        "rlang",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "d84e4c33206aaace37714901ac2b00c3"
+    },
+    "hwriter": {
+      "Package": "hwriter",
+      "Version": "1.3.2.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "5b2c97022c4b5cfbc3838315a3bf7867"
+    },
+    "ids": {
+      "Package": "ids",
+      "Version": "1.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "openssl",
+        "uuid"
+      ],
+      "Hash": "99df65cfef20e525ed38c3d2577f7190"
+    },
+    "ini": {
+      "Package": "ini",
+      "Version": "0.3.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "6154ec2223172bce8162d4153cda21f7"
+    },
+    "interp": {
+      "Package": "interp",
+      "Version": "1.1-6",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "Rcpp",
+        "RcppEigen",
+        "deldir"
+      ],
+      "Hash": "824328a500468b76e61402b2cb3033df"
+    },
+    "isoband": {
+      "Package": "isoband",
+      "Version": "0.2.7",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "grid",
+        "utils"
+      ],
+      "Hash": "0080607b4a1a7b28979aecef976d8bc2"
+    },
+    "jpeg": {
+      "Package": "jpeg",
+      "Version": "0.1-10",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "031a0b683d001a7519202f0628fc0358"
+    },
+    "jquerylib": {
+      "Package": "jquerylib",
+      "Version": "0.1.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "htmltools"
+      ],
+      "Hash": "5aab57a3bd297eee1c1d862735972182"
+    },
+    "jsonlite": {
+      "Package": "jsonlite",
+      "Version": "1.8.9",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "methods"
+      ],
+      "Hash": "4e993b65c2c3ffbffce7bb3e2c6f832b"
+    },
+    "knitr": {
+      "Package": "knitr",
+      "Version": "1.48",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "evaluate",
+        "highr",
+        "methods",
+        "tools",
+        "xfun",
+        "yaml"
+      ],
+      "Hash": "acf380f300c721da9fde7df115a5f86f"
+    },
+    "labeling": {
+      "Package": "labeling",
+      "Version": "0.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "graphics",
+        "stats"
+      ],
+      "Hash": "b64ec208ac5bc1852b285f665d6368b3"
+    },
+    "lambda.r": {
+      "Package": "lambda.r",
+      "Version": "1.2.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "formatR"
+      ],
+      "Hash": "b1e925c4b9ffeb901bacf812cbe9a6ad"
+    },
+    "later": {
+      "Package": "later",
+      "Version": "1.3.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "Rcpp",
+        "rlang"
+      ],
+      "Hash": "a3e051d405326b8b0012377434c62b37"
+    },
+    "lattice": {
+      "Package": "lattice",
+      "Version": "0.20-45",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics",
+        "grid",
+        "stats",
+        "utils"
+      ],
+      "Hash": "b64cdbb2b340437c4ee047a1f4c4377b"
+    },
+    "latticeExtra": {
+      "Package": "latticeExtra",
+      "Version": "0.6-30",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "MASS",
+        "R",
+        "RColorBrewer",
+        "grDevices",
+        "grid",
+        "interp",
+        "jpeg",
+        "lattice",
+        "png",
+        "stats",
+        "utils"
+      ],
+      "Hash": "0857b103442b4657416508733c8c6678"
+    },
+    "lazyeval": {
+      "Package": "lazyeval",
+      "Version": "0.2.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "d908914ae53b04d4c0c0fd72ecc35370"
+    },
+    "lifecycle": {
+      "Package": "lifecycle",
+      "Version": "1.0.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "rlang"
+      ],
+      "Hash": "b8552d117e1b808b09a832f589b79035"
+    },
+    "lubridate": {
+      "Package": "lubridate",
+      "Version": "1.9.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "generics",
+        "methods",
+        "timechange"
+      ],
+      "Hash": "680ad542fbcf801442c83a6ac5a2126c"
+    },
+    "magrittr": {
+      "Package": "magrittr",
+      "Version": "2.0.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "7ce2733a9826b3aeb1775d56fd305472"
+    },
+    "matrixStats": {
+      "Package": "matrixStats",
+      "Version": "1.4.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "8885ffb1f46e820dede6b2ca9442abca"
+    },
+    "memoise": {
+      "Package": "memoise",
+      "Version": "2.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "cachem",
+        "rlang"
+      ],
+      "Hash": "e2817ccf4a065c5d9d7f2cfbe7c1d78c"
+    },
+    "mgcv": {
+      "Package": "mgcv",
+      "Version": "1.8-40",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "Matrix",
+        "R",
+        "graphics",
+        "methods",
+        "nlme",
+        "splines",
+        "stats",
+        "utils"
+      ],
+      "Hash": "c6b2fdb18cf68ab613bd564363e1ba0d"
+    },
+    "mime": {
+      "Package": "mime",
+      "Version": "0.12",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "tools"
+      ],
+      "Hash": "18e9c28c1d3ca1560ce30658b22ce104"
+    },
+    "miniUI": {
+      "Package": "miniUI",
+      "Version": "0.1.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "htmltools",
+        "shiny",
+        "utils"
+      ],
+      "Hash": "fec5f52652d60615fdb3957b3d74324a"
+    },
+    "modelr": {
+      "Package": "modelr",
+      "Version": "0.1.11",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "broom",
+        "magrittr",
+        "purrr",
+        "rlang",
+        "tibble",
+        "tidyr",
+        "tidyselect",
+        "vctrs"
+      ],
+      "Hash": "4f50122dc256b1b6996a4703fecea821"
+    },
+    "multtest": {
+      "Package": "multtest",
+      "Version": "2.54.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "Biobase",
+        "BiocGenerics",
+        "MASS",
+        "R",
+        "methods",
+        "stats4",
+        "survival"
+      ],
+      "Hash": "90541e657865c30c0750bf9acf991b4e"
+    },
+    "munsell": {
+      "Package": "munsell",
+      "Version": "0.5.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "colorspace",
+        "methods"
+      ],
+      "Hash": "4fd8900853b746af55b81fda99da7695"
+    },
+    "nlme": {
+      "Package": "nlme",
+      "Version": "3.1-157",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "graphics",
+        "lattice",
+        "stats",
+        "utils"
+      ],
+      "Hash": "dbca60742be0c9eddc5205e5c7ca1f44"
+    },
+    "nnet": {
+      "Package": "nnet",
+      "Version": "7.3-17",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "stats",
+        "utils"
+      ],
+      "Hash": "cb1d8d9f300a7e536b89c8a88c53f610"
+    },
+    "openssl": {
+      "Package": "openssl",
+      "Version": "2.2.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "askpass"
+      ],
+      "Hash": "d413e0fef796c9401a4419485f709ca1"
+    },
+    "pillar": {
+      "Package": "pillar",
+      "Version": "1.9.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "cli",
+        "fansi",
+        "glue",
+        "lifecycle",
+        "rlang",
+        "utf8",
+        "utils",
+        "vctrs"
+      ],
+      "Hash": "15da5a8412f317beeee6175fbc76f4bb"
+    },
+    "pkgbuild": {
+      "Package": "pkgbuild",
+      "Version": "1.4.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "callr",
+        "cli",
+        "desc",
+        "processx"
+      ],
+      "Hash": "a29e8e134a460a01e0ca67a4763c595b"
+    },
+    "pkgconfig": {
+      "Package": "pkgconfig",
+      "Version": "2.0.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "utils"
+      ],
+      "Hash": "01f28d4278f15c76cddbea05899c5d6f"
+    },
+    "pkgdown": {
+      "Package": "pkgdown",
+      "Version": "2.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "bslib",
+        "callr",
+        "cli",
+        "desc",
+        "digest",
+        "downlit",
+        "fontawesome",
+        "fs",
+        "httr2",
+        "jsonlite",
+        "openssl",
+        "purrr",
+        "ragg",
+        "rlang",
+        "rmarkdown",
+        "tibble",
+        "whisker",
+        "withr",
+        "xml2",
+        "yaml"
+      ],
+      "Hash": "df2912d5873422b55a13002510f02c9f"
+    },
+    "pkgload": {
+      "Package": "pkgload",
+      "Version": "1.4.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "desc",
+        "fs",
+        "glue",
+        "lifecycle",
+        "methods",
+        "pkgbuild",
+        "processx",
+        "rlang",
+        "rprojroot",
+        "utils",
+        "withr"
+      ],
+      "Hash": "2ec30ffbeec83da57655b850cf2d3e0e"
+    },
+    "plogr": {
+      "Package": "plogr",
+      "Version": "0.2.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "09eb987710984fc2905c7129c7d85e65"
+    },
+    "png": {
+      "Package": "png",
+      "Version": "0.1-8",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "bd54ba8a0a5faded999a7aab6e46b374"
+    },
+    "praise": {
+      "Package": "praise",
+      "Version": "1.0.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "a555924add98c99d2f411e37e7d25e9f"
+    },
+    "prettyunits": {
+      "Package": "prettyunits",
+      "Version": "1.2.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "6b01fc98b1e86c4f705ce9dcfd2f57c7"
+    },
+    "processx": {
+      "Package": "processx",
+      "Version": "3.8.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "ps",
+        "utils"
+      ],
+      "Hash": "0c90a7d71988856bad2a2a45dd871bb9"
+    },
+    "profvis": {
+      "Package": "profvis",
+      "Version": "0.4.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "htmlwidgets",
+        "rlang",
+        "vctrs"
+      ],
+      "Hash": "bffa126bf92987e677c12cfb5651fc1d"
+    },
+    "progress": {
+      "Package": "progress",
+      "Version": "1.2.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "crayon",
+        "hms",
+        "prettyunits"
+      ],
+      "Hash": "f4625e061cb2865f111b47ff163a5ca6"
+    },
+    "promises": {
+      "Package": "promises",
+      "Version": "1.3.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R6",
+        "Rcpp",
+        "fastmap",
+        "later",
+        "magrittr",
+        "rlang",
+        "stats"
+      ],
+      "Hash": "434cd5388a3979e74be5c219bcd6e77d"
+    },
+    "ps": {
+      "Package": "ps",
+      "Version": "1.8.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "4b9c8485b0c7eecdf0a9ba5132a45576"
+    },
+    "purrr": {
+      "Package": "purrr",
+      "Version": "1.0.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "lifecycle",
+        "magrittr",
+        "rlang",
+        "vctrs"
+      ],
+      "Hash": "1cba04a4e9414bdefc9dcaa99649a8dc"
+    },
+    "ragg": {
+      "Package": "ragg",
+      "Version": "1.3.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "systemfonts",
+        "textshaping"
+      ],
+      "Hash": "0595fe5e47357111f29ad19101c7d271"
+    },
+    "rappdirs": {
+      "Package": "rappdirs",
+      "Version": "0.3.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "5e3c5dc0b071b21fa128676560dbe94d"
+    },
+    "rcmdcheck": {
+      "Package": "rcmdcheck",
+      "Version": "1.4.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R6",
+        "callr",
+        "cli",
+        "curl",
+        "desc",
+        "digest",
+        "pkgbuild",
+        "prettyunits",
+        "rprojroot",
+        "sessioninfo",
+        "utils",
+        "withr",
+        "xopen"
+      ],
+      "Hash": "8f25ebe2ec38b1f2aef3b0d2ef76f6c4"
+    },
+    "readr": {
+      "Package": "readr",
+      "Version": "2.1.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "cli",
+        "clipr",
+        "cpp11",
+        "crayon",
+        "hms",
+        "lifecycle",
+        "methods",
+        "rlang",
+        "tibble",
+        "tzdb",
+        "utils",
+        "vroom"
+      ],
+      "Hash": "9de96463d2117f6ac49980577939dfb3"
+    },
+    "readxl": {
+      "Package": "readxl",
+      "Version": "1.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cellranger",
+        "cpp11",
+        "progress",
+        "tibble",
+        "utils"
+      ],
+      "Hash": "8cf9c239b96df1bbb133b74aef77ad0a"
+    },
+    "regioneR": {
+      "Package": "regioneR",
+      "Version": "1.30.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BSgenome",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicRanges",
+        "IRanges",
+        "S4Vectors",
+        "graphics",
+        "memoise",
+        "methods",
+        "parallel",
+        "rtracklayer",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "6411a63297335b3b14f69d4e45504f6c"
+    },
+    "rematch": {
+      "Package": "rematch",
+      "Version": "2.0.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "cbff1b666c6fa6d21202f07e2318d4f1"
+    },
+    "rematch2": {
+      "Package": "rematch2",
+      "Version": "2.1.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "tibble"
+      ],
+      "Hash": "76c9e04c712a05848ae7a23d2f170a40"
+    },
+    "remotes": {
+      "Package": "remotes",
+      "Version": "2.5.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "methods",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "3ee025083e66f18db6cf27b56e23e141"
+    },
+    "renv": {
+      "Package": "renv",
+      "Version": "1.0.11",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "utils"
+      ],
+      "Hash": "47623f66b4e80b3b0587bc5d7b309888"
+    },
+    "reprex": {
+      "Package": "reprex",
+      "Version": "2.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "callr",
+        "cli",
+        "clipr",
+        "fs",
+        "glue",
+        "knitr",
+        "lifecycle",
+        "rlang",
+        "rmarkdown",
+        "rstudioapi",
+        "utils",
+        "withr"
+      ],
+      "Hash": "97b1d5361a24d9fb588db7afe3e5bcbf"
+    },
+    "restfulr": {
+      "Package": "restfulr",
+      "Version": "0.0.15",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "RCurl",
+        "S4Vectors",
+        "XML",
+        "methods",
+        "rjson",
+        "yaml"
+      ],
+      "Hash": "44651c1e68eda9d462610aca9f15a815"
+    },
+    "rjson": {
+      "Package": "rjson",
+      "Version": "0.2.23",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "7a04e9eff95857dbf557b4e5f0b3d1a8"
+    },
+    "rlang": {
+      "Package": "rlang",
+      "Version": "1.1.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "3eec01f8b1dee337674b2e34ab1f9bc1"
+    },
+    "rmarkdown": {
+      "Package": "rmarkdown",
+      "Version": "2.28",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "bslib",
+        "evaluate",
+        "fontawesome",
+        "htmltools",
+        "jquerylib",
+        "jsonlite",
+        "knitr",
+        "methods",
+        "tinytex",
+        "tools",
+        "utils",
+        "xfun",
+        "yaml"
+      ],
+      "Hash": "062470668513dcda416927085ee9bdc7"
+    },
+    "roxygen2": {
+      "Package": "roxygen2",
+      "Version": "7.3.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "brew",
+        "cli",
+        "commonmark",
+        "cpp11",
+        "desc",
+        "knitr",
+        "methods",
+        "pkgload",
+        "purrr",
+        "rlang",
+        "stringi",
+        "stringr",
+        "utils",
+        "withr",
+        "xml2"
+      ],
+      "Hash": "6ee25f9054a70f44d615300ed531ba8d"
+    },
+    "rpart": {
+      "Package": "rpart",
+      "Version": "4.1.16",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics",
+        "stats"
+      ],
+      "Hash": "ea3ca1d9473daabb3cd0f1b4f974c1ed"
+    },
+    "rprojroot": {
+      "Package": "rprojroot",
+      "Version": "2.0.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "4c8415e0ec1e29f3f4f6fc108bef0144"
+    },
+    "rstudioapi": {
+      "Package": "rstudioapi",
+      "Version": "0.17.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "fb9f5fce8f609e9b66f0bea5c783f88a"
+    },
+    "rtracklayer": {
+      "Package": "rtracklayer",
+      "Version": "1.58.0",
+      "Source": "Bioconductor",
+      "Requirements": [
+        "BiocGenerics",
+        "BiocIO",
+        "Biostrings",
+        "GenomeInfoDb",
+        "GenomicAlignments",
+        "GenomicRanges",
+        "IRanges",
+        "R",
+        "RCurl",
+        "Rsamtools",
+        "S4Vectors",
+        "XML",
+        "XVector",
+        "methods",
+        "restfulr",
+        "tools",
+        "zlibbioc"
+      ],
+      "Hash": "7587e3aec1d4a7c4b159a5be63c9653e"
+    },
+    "rversions": {
+      "Package": "rversions",
+      "Version": "2.1.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "curl",
+        "utils",
+        "xml2"
+      ],
+      "Hash": "a9881dfed103e83f9de151dc17002cd1"
+    },
+    "rvest": {
+      "Package": "rvest",
+      "Version": "1.0.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "httr",
+        "lifecycle",
+        "magrittr",
+        "rlang",
+        "selectr",
+        "tibble",
+        "xml2"
+      ],
+      "Hash": "0bcf0c6f274e90ea314b812a6d19a519"
+    },
+    "sass": {
+      "Package": "sass",
+      "Version": "0.4.9",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R6",
+        "fs",
+        "htmltools",
+        "rappdirs",
+        "rlang"
+      ],
+      "Hash": "d53dbfddf695303ea4ad66f86e99b95d"
+    },
+    "scales": {
+      "Package": "scales",
+      "Version": "1.3.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "RColorBrewer",
+        "cli",
+        "farver",
+        "glue",
+        "labeling",
+        "lifecycle",
+        "munsell",
+        "rlang",
+        "viridisLite"
+      ],
+      "Hash": "c19df082ba346b0ffa6f833e92de34d1"
+    },
+    "selectr": {
+      "Package": "selectr",
+      "Version": "0.4-2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "methods",
+        "stringr"
+      ],
+      "Hash": "3838071b66e0c566d55cc26bd6e27bf4"
+    },
+    "sessioninfo": {
+      "Package": "sessioninfo",
+      "Version": "1.2.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "tools",
+        "utils"
+      ],
+      "Hash": "3f9796a8d0a0e8c6eb49a4b029359d1f"
+    },
+    "shiny": {
+      "Package": "shiny",
+      "Version": "1.9.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "bslib",
+        "cachem",
+        "commonmark",
+        "crayon",
+        "fastmap",
+        "fontawesome",
+        "glue",
+        "grDevices",
+        "htmltools",
+        "httpuv",
+        "jsonlite",
+        "later",
+        "lifecycle",
+        "methods",
+        "mime",
+        "promises",
+        "rlang",
+        "sourcetools",
+        "tools",
+        "utils",
+        "withr",
+        "xtable"
+      ],
+      "Hash": "6a293995a66e12c48d13aa1f957d09c7"
+    },
+    "snow": {
+      "Package": "snow",
+      "Version": "0.4-4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "utils"
+      ],
+      "Hash": "40b74690debd20c57d93d8c246b305d4"
+    },
+    "sourcetools": {
+      "Package": "sourcetools",
+      "Version": "0.1.7-1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "5f5a7629f956619d519205ec475fe647"
+    },
+    "stringi": {
+      "Package": "stringi",
+      "Version": "1.8.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "stats",
+        "tools",
+        "utils"
+      ],
+      "Hash": "39e1144fd75428983dc3f63aa53dfa91"
+    },
+    "stringr": {
+      "Package": "stringr",
+      "Version": "1.5.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "rlang",
+        "stringi",
+        "vctrs"
+      ],
+      "Hash": "960e2ae9e09656611e0b8214ad543207"
+    },
+    "survival": {
+      "Package": "survival",
+      "Version": "3.3-1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "Matrix",
+        "R",
+        "graphics",
+        "methods",
+        "splines",
+        "stats",
+        "utils"
+      ],
+      "Hash": "f6189c70451d3d68e0d571235576e833"
+    },
+    "sys": {
+      "Package": "sys",
+      "Version": "3.4.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "de342ebfebdbf40477d0758d05426646"
+    },
+    "systemfonts": {
+      "Package": "systemfonts",
+      "Version": "1.1.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cpp11",
+        "lifecycle"
+      ],
+      "Hash": "213b6b8ed5afbf934843e6c3b090d418"
+    },
+    "testthat": {
+      "Package": "testthat",
+      "Version": "3.2.1.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "R6",
+        "brio",
+        "callr",
+        "cli",
+        "desc",
+        "digest",
+        "evaluate",
+        "jsonlite",
+        "lifecycle",
+        "magrittr",
+        "methods",
+        "pkgload",
+        "praise",
+        "processx",
+        "ps",
+        "rlang",
+        "utils",
+        "waldo",
+        "withr"
+      ],
+      "Hash": "3f6e7e5e2220856ff865e4834766bf2b"
+    },
+    "textshaping": {
+      "Package": "textshaping",
+      "Version": "0.4.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cpp11",
+        "lifecycle",
+        "systemfonts"
+      ],
+      "Hash": "5142f8bc78ed3d819d26461b641627ce"
+    },
+    "tibble": {
+      "Package": "tibble",
+      "Version": "3.2.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "fansi",
+        "lifecycle",
+        "magrittr",
+        "methods",
+        "pillar",
+        "pkgconfig",
+        "rlang",
+        "utils",
+        "vctrs"
+      ],
+      "Hash": "a84e2cc86d07289b3b6f5069df7a004c"
+    },
+    "tidyr": {
+      "Package": "tidyr",
+      "Version": "1.3.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "cpp11",
+        "dplyr",
+        "glue",
+        "lifecycle",
+        "magrittr",
+        "purrr",
+        "rlang",
+        "stringr",
+        "tibble",
+        "tidyselect",
+        "utils",
+        "vctrs"
+      ],
+      "Hash": "915fb7ce036c22a6a33b5a8adb712eb1"
+    },
+    "tidyselect": {
+      "Package": "tidyselect",
+      "Version": "1.2.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "lifecycle",
+        "rlang",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "829f27b9c4919c16b593794a6344d6c0"
+    },
+    "tidyverse": {
+      "Package": "tidyverse",
+      "Version": "2.0.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "broom",
+        "cli",
+        "conflicted",
+        "dbplyr",
+        "dplyr",
+        "dtplyr",
+        "forcats",
+        "ggplot2",
+        "googledrive",
+        "googlesheets4",
+        "haven",
+        "hms",
+        "httr",
+        "jsonlite",
+        "lubridate",
+        "magrittr",
+        "modelr",
+        "pillar",
+        "purrr",
+        "ragg",
+        "readr",
+        "readxl",
+        "reprex",
+        "rlang",
+        "rstudioapi",
+        "rvest",
+        "stringr",
+        "tibble",
+        "tidyr",
+        "xml2"
+      ],
+      "Hash": "c328568cd14ea89a83bd4ca7f54ae07e"
+    },
+    "timechange": {
+      "Package": "timechange",
+      "Version": "0.3.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cpp11"
+      ],
+      "Hash": "c5f3c201b931cd6474d17d8700ccb1c8"
+    },
+    "tinytex": {
+      "Package": "tinytex",
+      "Version": "0.53",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "xfun"
+      ],
+      "Hash": "9db859e8aabbb474293dde3097839420"
+    },
+    "tzdb": {
+      "Package": "tzdb",
+      "Version": "0.4.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cpp11"
+      ],
+      "Hash": "f561504ec2897f4d46f0c7657e488ae1"
+    },
+    "urlchecker": {
+      "Package": "urlchecker",
+      "Version": "1.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "curl",
+        "tools",
+        "xml2"
+      ],
+      "Hash": "409328b8e1253c8d729a7836fe7f7a16"
+    },
+    "usethis": {
+      "Package": "usethis",
+      "Version": "3.0.0",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "clipr",
+        "crayon",
+        "curl",
+        "desc",
+        "fs",
+        "gert",
+        "gh",
+        "glue",
+        "jsonlite",
+        "lifecycle",
+        "purrr",
+        "rappdirs",
+        "rlang",
+        "rprojroot",
+        "rstudioapi",
+        "stats",
+        "utils",
+        "whisker",
+        "withr",
+        "yaml"
+      ],
+      "Hash": "b2fbf93c2127bedd2cbe9b799530d5d2"
+    },
+    "utf8": {
+      "Package": "utf8",
+      "Version": "1.2.4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "62b65c52671e6665f803ff02954446e9"
+    },
+    "uuid": {
+      "Package": "uuid",
+      "Version": "1.2-1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "34e965e62a41fcafb1ca60e9b142085b"
+    },
+    "vctrs": {
+      "Package": "vctrs",
+      "Version": "0.6.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "glue",
+        "lifecycle",
+        "rlang"
+      ],
+      "Hash": "c03fa420630029418f7e6da3667aac4a"
+    },
+    "viridis": {
+      "Package": "viridis",
+      "Version": "0.6.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "ggplot2",
+        "gridExtra",
+        "viridisLite"
+      ],
+      "Hash": "acd96d9fa70adeea4a5a1150609b9745"
+    },
+    "viridisLite": {
+      "Package": "viridisLite",
+      "Version": "0.4.2",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R"
+      ],
+      "Hash": "c826c7c4241b6fc89ff55aaea3fa7491"
+    },
+    "vroom": {
+      "Package": "vroom",
+      "Version": "1.6.5",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "bit64",
+        "cli",
+        "cpp11",
+        "crayon",
+        "glue",
+        "hms",
+        "lifecycle",
+        "methods",
+        "progress",
+        "rlang",
+        "stats",
+        "tibble",
+        "tidyselect",
+        "tzdb",
+        "vctrs",
+        "withr"
+      ],
+      "Hash": "390f9315bc0025be03012054103d227c"
+    },
+    "waldo": {
+      "Package": "waldo",
+      "Version": "0.5.3",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "diffobj",
+        "glue",
+        "methods",
+        "rematch2",
+        "rlang",
+        "tibble"
+      ],
+      "Hash": "16aa934a49658677d8041df9017329b9"
+    },
+    "whisker": {
+      "Package": "whisker",
+      "Version": "0.4.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "c6abfa47a46d281a7d5159d0a8891e88"
+    },
+    "withr": {
+      "Package": "withr",
+      "Version": "3.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "graphics"
+      ],
+      "Hash": "07909200e8bbe90426fbfeb73e1e27aa"
+    },
+    "xfun": {
+      "Package": "xfun",
+      "Version": "0.48",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "grDevices",
+        "stats",
+        "tools"
+      ],
+      "Hash": "89e455b87c84e227eb7f60a1b4e5fe1f"
+    },
+    "xml2": {
+      "Package": "xml2",
+      "Version": "1.3.6",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "cli",
+        "methods",
+        "rlang"
+      ],
+      "Hash": "1d0336142f4cd25d8d23cd3ba7a8fb61"
+    },
+    "xopen": {
+      "Package": "xopen",
+      "Version": "1.0.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "processx"
+      ],
+      "Hash": "423df1e86d5533fcb73c6b02b4923b49"
+    },
+    "xtable": {
+      "Package": "xtable",
+      "Version": "1.8-4",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Requirements": [
+        "R",
+        "stats",
+        "utils"
+      ],
+      "Hash": "b8acdf8af494d9ec19ccb2481a9b11c2"
+    },
+    "yaml": {
+      "Package": "yaml",
+      "Version": "2.3.10",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "51dab85c6c98e50a18d7551e9d49f76c"
+    },
+    "zip": {
+      "Package": "zip",
+      "Version": "2.3.1",
+      "Source": "Repository",
+      "Repository": "CRAN",
+      "Hash": "fcc4bd8e6da2d2011eb64a5e5cc685ab"
+    },
+    "zlibbioc": {
+      "Package": "zlibbioc",
+      "Version": "1.44.0",
+      "Source": "Bioconductor",
+      "Hash": "6f578730acbdfc7ce2ca49df29bb5352"
+    }
+  }
+}
diff --git a/renv/.gitignore b/renv/.gitignore
new file mode 100644
index 0000000..0ec0cbb
--- /dev/null
+++ b/renv/.gitignore
@@ -0,0 +1,7 @@
+library/
+local/
+cellar/
+lock/
+python/
+sandbox/
+staging/
diff --git a/renv/activate.R b/renv/activate.R
new file mode 100644
index 0000000..0eb5108
--- /dev/null
+++ b/renv/activate.R
@@ -0,0 +1,1305 @@
+
+local({
+
+  # the requested version of renv
+  version <- "1.0.11"
+  attr(version, "sha") <- NULL
+
+  # the project directory
+  project <- Sys.getenv("RENV_PROJECT")
+  if (!nzchar(project))
+    project <- getwd()
+
+  # use start-up diagnostics if enabled
+  diagnostics <- Sys.getenv("RENV_STARTUP_DIAGNOSTICS", unset = "FALSE")
+  if (diagnostics) {
+    start <- Sys.time()
+    profile <- tempfile("renv-startup-", fileext = ".Rprof")
+    utils::Rprof(profile)
+    on.exit({
+      utils::Rprof(NULL)
+      elapsed <- signif(difftime(Sys.time(), start, units = "auto"), digits = 2L)
+      writeLines(sprintf("- renv took %s to run the autoloader.", format(elapsed)))
+      writeLines(sprintf("- Profile: %s", profile))
+      print(utils::summaryRprof(profile))
+    }, add = TRUE)
+  }
+
+  # figure out whether the autoloader is enabled
+  enabled <- local({
+
+    # first, check config option
+    override <- getOption("renv.config.autoloader.enabled")
+    if (!is.null(override))
+      return(override)
+
+    # if we're being run in a context where R_LIBS is already set,
+    # don't load -- presumably we're being run as a sub-process and
+    # the parent process has already set up library paths for us
+    rcmd <- Sys.getenv("R_CMD", unset = NA)
+    rlibs <- Sys.getenv("R_LIBS", unset = NA)
+    if (!is.na(rlibs) && !is.na(rcmd))
+      return(FALSE)
+
+    # next, check environment variables
+    # TODO: prefer using the configuration one in the future
+    envvars <- c(
+      "RENV_CONFIG_AUTOLOADER_ENABLED",
+      "RENV_AUTOLOADER_ENABLED",
+      "RENV_ACTIVATE_PROJECT"
+    )
+
+    for (envvar in envvars) {
+      envval <- Sys.getenv(envvar, unset = NA)
+      if (!is.na(envval))
+        return(tolower(envval) %in% c("true", "t", "1"))
+    }
+
+    # enable by default
+    TRUE
+
+  })
+
+  # bail if we're not enabled
+  if (!enabled) {
+
+    # if we're not enabled, we might still need to manually load
+    # the user profile here
+    profile <- Sys.getenv("R_PROFILE_USER", unset = "~/.Rprofile")
+    if (file.exists(profile)) {
+      cfg <- Sys.getenv("RENV_CONFIG_USER_PROFILE", unset = "TRUE")
+      if (tolower(cfg) %in% c("true", "t", "1"))
+        sys.source(profile, envir = globalenv())
+    }
+
+    return(FALSE)
+
+  }
+
+  # avoid recursion
+  if (identical(getOption("renv.autoloader.running"), TRUE)) {
+    warning("ignoring recursive attempt to run renv autoloader")
+    return(invisible(TRUE))
+  }
+
+  # signal that we're loading renv during R startup
+  options(renv.autoloader.running = TRUE)
+  on.exit(options(renv.autoloader.running = NULL), add = TRUE)
+
+  # signal that we've consented to use renv
+  options(renv.consent = TRUE)
+
+  # load the 'utils' package eagerly -- this ensures that renv shims, which
+  # mask 'utils' packages, will come first on the search path
+  library(utils, lib.loc = .Library)
+
+  # unload renv if it's already been loaded
+  if ("renv" %in% loadedNamespaces())
+    unloadNamespace("renv")
+
+  # load bootstrap tools   
+  ansify <- function(text) {
+    if (renv_ansify_enabled())
+      renv_ansify_enhanced(text)
+    else
+      renv_ansify_default(text)
+  }
+  
+  renv_ansify_enabled <- function() {
+  
+    override <- Sys.getenv("RENV_ANSIFY_ENABLED", unset = NA)
+    if (!is.na(override))
+      return(as.logical(override))
+  
+    pane <- Sys.getenv("RSTUDIO_CHILD_PROCESS_PANE", unset = NA)
+    if (identical(pane, "build"))
+      return(FALSE)
+  
+    testthat <- Sys.getenv("TESTTHAT", unset = "false")
+    if (tolower(testthat) %in% "true")
+      return(FALSE)
+  
+    iderun <- Sys.getenv("R_CLI_HAS_HYPERLINK_IDE_RUN", unset = "false")
+    if (tolower(iderun) %in% "false")
+      return(FALSE)
+  
+    TRUE
+  
+  }
+  
+  renv_ansify_default <- function(text) {
+    text
+  }
+  
+  renv_ansify_enhanced <- function(text) {
+  
+    # R help links
+    pattern <- "`\\?(renv::(?:[^`])+)`"
+    replacement <- "`\033]8;;ide:help:\\1\a?\\1\033]8;;\a`"
+    text <- gsub(pattern, replacement, text, perl = TRUE)
+  
+    # runnable code
+    pattern <- "`(renv::(?:[^`])+)`"
+    replacement <- "`\033]8;;ide:run:\\1\a\\1\033]8;;\a`"
+    text <- gsub(pattern, replacement, text, perl = TRUE)
+  
+    # return ansified text
+    text
+  
+  }
+  
+  renv_ansify_init <- function() {
+  
+    envir <- renv_envir_self()
+    if (renv_ansify_enabled())
+      assign("ansify", renv_ansify_enhanced, envir = envir)
+    else
+      assign("ansify", renv_ansify_default, envir = envir)
+  
+  }
+  
+  `%||%` <- function(x, y) {
+    if (is.null(x)) y else x
+  }
+  
+  catf <- function(fmt, ..., appendLF = TRUE) {
+  
+    quiet <- getOption("renv.bootstrap.quiet", default = FALSE)
+    if (quiet)
+      return(invisible())
+  
+    msg <- sprintf(fmt, ...)
+    cat(msg, file = stdout(), sep = if (appendLF) "\n" else "")
+  
+    invisible(msg)
+  
+  }
+  
+  header <- function(label,
+                     ...,
+                     prefix = "#",
+                     suffix = "-",
+                     n = min(getOption("width"), 78))
+  {
+    label <- sprintf(label, ...)
+    n <- max(n - nchar(label) - nchar(prefix) - 2L, 8L)
+    if (n <= 0)
+      return(paste(prefix, label))
+  
+    tail <- paste(rep.int(suffix, n), collapse = "")
+    paste0(prefix, " ", label, " ", tail)
+  
+  }
+  
+  heredoc <- function(text, leave = 0) {
+  
+    # remove leading, trailing whitespace
+    trimmed <- gsub("^\\s*\\n|\\n\\s*$", "", text)
+  
+    # split into lines
+    lines <- strsplit(trimmed, "\n", fixed = TRUE)[[1L]]
+  
+    # compute common indent
+    indent <- regexpr("[^[:space:]]", lines)
+    common <- min(setdiff(indent, -1L)) - leave
+    text <- paste(substring(lines, common), collapse = "\n")
+  
+    # substitute in ANSI links for executable renv code
+    ansify(text)
+  
+  }
+  
+  startswith <- function(string, prefix) {
+    substring(string, 1, nchar(prefix)) == prefix
+  }
+  
+  bootstrap <- function(version, library) {
+  
+    friendly <- renv_bootstrap_version_friendly(version)
+    section <- header(sprintf("Bootstrapping renv %s", friendly))
+    catf(section)
+  
+    # attempt to download renv
+    catf("- Downloading renv ... ", appendLF = FALSE)
+    withCallingHandlers(
+      tarball <- renv_bootstrap_download(version),
+      error = function(err) {
+        catf("FAILED")
+        stop("failed to download:\n", conditionMessage(err))
+      }
+    )
+    catf("OK")
+    on.exit(unlink(tarball), add = TRUE)
+  
+    # now attempt to install
+    catf("- Installing renv  ... ", appendLF = FALSE)
+    withCallingHandlers(
+      status <- renv_bootstrap_install(version, tarball, library),
+      error = function(err) {
+        catf("FAILED")
+        stop("failed to install:\n", conditionMessage(err))
+      }
+    )
+    catf("OK")
+  
+    # add empty line to break up bootstrapping from normal output
+    catf("")
+  
+    return(invisible())
+  }
+  
+  renv_bootstrap_tests_running <- function() {
+    getOption("renv.tests.running", default = FALSE)
+  }
+  
+  renv_bootstrap_repos <- function() {
+  
+    # get CRAN repository
+    cran <- getOption("renv.repos.cran", "https://cloud.r-project.org")
+  
+    # check for repos override
+    repos <- Sys.getenv("RENV_CONFIG_REPOS_OVERRIDE", unset = NA)
+    if (!is.na(repos)) {
+  
+      # check for RSPM; if set, use a fallback repository for renv
+      rspm <- Sys.getenv("RSPM", unset = NA)
+      if (identical(rspm, repos))
+        repos <- c(RSPM = rspm, CRAN = cran)
+  
+      return(repos)
+  
+    }
+  
+    # check for lockfile repositories
+    repos <- tryCatch(renv_bootstrap_repos_lockfile(), error = identity)
+    if (!inherits(repos, "error") && length(repos))
+      return(repos)
+  
+    # retrieve current repos
+    repos <- getOption("repos")
+  
+    # ensure @CRAN@ entries are resolved
+    repos[repos == "@CRAN@"] <- cran
+  
+    # add in renv.bootstrap.repos if set
+    default <- c(FALLBACK = "https://cloud.r-project.org")
+    extra <- getOption("renv.bootstrap.repos", default = default)
+    repos <- c(repos, extra)
+  
+    # remove duplicates that might've snuck in
+    dupes <- duplicated(repos) | duplicated(names(repos))
+    repos[!dupes]
+  
+  }
+  
+  renv_bootstrap_repos_lockfile <- function() {
+  
+    lockpath <- Sys.getenv("RENV_PATHS_LOCKFILE", unset = "renv.lock")
+    if (!file.exists(lockpath))
+      return(NULL)
+  
+    lockfile <- tryCatch(renv_json_read(lockpath), error = identity)
+    if (inherits(lockfile, "error")) {
+      warning(lockfile)
+      return(NULL)
+    }
+  
+    repos <- lockfile$R$Repositories
+    if (length(repos) == 0)
+      return(NULL)
+  
+    keys <- vapply(repos, `[[`, "Name", FUN.VALUE = character(1))
+    vals <- vapply(repos, `[[`, "URL", FUN.VALUE = character(1))
+    names(vals) <- keys
+  
+    return(vals)
+  
+  }
+  
+  renv_bootstrap_download <- function(version) {
+  
+    sha <- attr(version, "sha", exact = TRUE)
+  
+    methods <- if (!is.null(sha)) {
+  
+      # attempting to bootstrap a development version of renv
+      c(
+        function() renv_bootstrap_download_tarball(sha),
+        function() renv_bootstrap_download_github(sha)
+      )
+  
+    } else {
+  
+      # attempting to bootstrap a release version of renv
+      c(
+        function() renv_bootstrap_download_tarball(version),
+        function() renv_bootstrap_download_cran_latest(version),
+        function() renv_bootstrap_download_cran_archive(version)
+      )
+  
+    }
+  
+    for (method in methods) {
+      path <- tryCatch(method(), error = identity)
+      if (is.character(path) && file.exists(path))
+        return(path)
+    }
+  
+    stop("All download methods failed")
+  
+  }
+  
+  renv_bootstrap_download_impl <- function(url, destfile) {
+  
+    mode <- "wb"
+  
+    # https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17715
+    fixup <-
+      Sys.info()[["sysname"]] == "Windows" &&
+      substring(url, 1L, 5L) == "file:"
+  
+    if (fixup)
+      mode <- "w+b"
+  
+    args <- list(
+      url      = url,
+      destfile = destfile,
+      mode     = mode,
+      quiet    = TRUE
+    )
+  
+    if ("headers" %in% names(formals(utils::download.file))) {
+      headers <- renv_bootstrap_download_custom_headers(url)
+      if (length(headers) && is.character(headers))
+        args$headers <- headers
+    }
+  
+    do.call(utils::download.file, args)
+  
+  }
+  
+  renv_bootstrap_download_custom_headers <- function(url) {
+  
+    headers <- getOption("renv.download.headers")
+    if (is.null(headers))
+      return(character())
+  
+    if (!is.function(headers))
+      stopf("'renv.download.headers' is not a function")
+  
+    headers <- headers(url)
+    if (length(headers) == 0L)
+      return(character())
+  
+    if (is.list(headers))
+      headers <- unlist(headers, recursive = FALSE, use.names = TRUE)
+  
+    ok <-
+      is.character(headers) &&
+      is.character(names(headers)) &&
+      all(nzchar(names(headers)))
+  
+    if (!ok)
+      stop("invocation of 'renv.download.headers' did not return a named character vector")
+  
+    headers
+  
+  }
+  
+  renv_bootstrap_download_cran_latest <- function(version) {
+  
+    spec <- renv_bootstrap_download_cran_latest_find(version)
+    type  <- spec$type
+    repos <- spec$repos
+  
+    baseurl <- utils::contrib.url(repos = repos, type = type)
+    ext <- if (identical(type, "source"))
+      ".tar.gz"
+    else if (Sys.info()[["sysname"]] == "Windows")
+      ".zip"
+    else
+      ".tgz"
+    name <- sprintf("renv_%s%s", version, ext)
+    url <- paste(baseurl, name, sep = "/")
+  
+    destfile <- file.path(tempdir(), name)
+    status <- tryCatch(
+      renv_bootstrap_download_impl(url, destfile),
+      condition = identity
+    )
+  
+    if (inherits(status, "condition"))
+      return(FALSE)
+  
+    # report success and return
+    destfile
+  
+  }
+  
+  renv_bootstrap_download_cran_latest_find <- function(version) {
+  
+    # check whether binaries are supported on this system
+    binary <-
+      getOption("renv.bootstrap.binary", default = TRUE) &&
+      !identical(.Platform$pkgType, "source") &&
+      !identical(getOption("pkgType"), "source") &&
+      Sys.info()[["sysname"]] %in% c("Darwin", "Windows")
+  
+    types <- c(if (binary) "binary", "source")
+  
+    # iterate over types + repositories
+    for (type in types) {
+      for (repos in renv_bootstrap_repos()) {
+  
+        # build arguments for utils::available.packages() call
+        args <- list(type = type, repos = repos)
+  
+        # add custom headers if available -- note that
+        # utils::available.packages() will pass this to download.file()
+        if ("headers" %in% names(formals(utils::download.file))) {
+          headers <- renv_bootstrap_download_custom_headers(repos)
+          if (length(headers) && is.character(headers))
+            args$headers <- headers
+        }
+  
+        # retrieve package database
+        db <- tryCatch(
+          as.data.frame(
+            do.call(utils::available.packages, args),
+            stringsAsFactors = FALSE
+          ),
+          error = identity
+        )
+  
+        if (inherits(db, "error"))
+          next
+  
+        # check for compatible entry
+        entry <- db[db$Package %in% "renv" & db$Version %in% version, ]
+        if (nrow(entry) == 0)
+          next
+  
+        # found it; return spec to caller
+        spec <- list(entry = entry, type = type, repos = repos)
+        return(spec)
+  
+      }
+    }
+  
+    # if we got here, we failed to find renv
+    fmt <- "renv %s is not available from your declared package repositories"
+    stop(sprintf(fmt, version))
+  
+  }
+  
+  renv_bootstrap_download_cran_archive <- function(version) {
+  
+    name <- sprintf("renv_%s.tar.gz", version)
+    repos <- renv_bootstrap_repos()
+    urls <- file.path(repos, "src/contrib/Archive/renv", name)
+    destfile <- file.path(tempdir(), name)
+  
+    for (url in urls) {
+  
+      status <- tryCatch(
+        renv_bootstrap_download_impl(url, destfile),
+        condition = identity
+      )
+  
+      if (identical(status, 0L))
+        return(destfile)
+  
+    }
+  
+    return(FALSE)
+  
+  }
+  
+  renv_bootstrap_download_tarball <- function(version) {
+  
+    # if the user has provided the path to a tarball via
+    # an environment variable, then use it
+    tarball <- Sys.getenv("RENV_BOOTSTRAP_TARBALL", unset = NA)
+    if (is.na(tarball))
+      return()
+  
+    # allow directories
+    if (dir.exists(tarball)) {
+      name <- sprintf("renv_%s.tar.gz", version)
+      tarball <- file.path(tarball, name)
+    }
+  
+    # bail if it doesn't exist
+    if (!file.exists(tarball)) {
+  
+      # let the user know we weren't able to honour their request
+      fmt <- "- RENV_BOOTSTRAP_TARBALL is set (%s) but does not exist."
+      msg <- sprintf(fmt, tarball)
+      warning(msg)
+  
+      # bail
+      return()
+  
+    }
+  
+    catf("- Using local tarball '%s'.", tarball)
+    tarball
+  
+  }
+  
+  renv_bootstrap_github_token <- function() {
+    for (envvar in c("GITHUB_TOKEN", "GITHUB_PAT", "GH_TOKEN")) {
+      envval <- Sys.getenv(envvar, unset = NA)
+      if (!is.na(envval))
+        return(envval)
+    }
+  }
+  
+  renv_bootstrap_download_github <- function(version) {
+  
+    enabled <- Sys.getenv("RENV_BOOTSTRAP_FROM_GITHUB", unset = "TRUE")
+    if (!identical(enabled, "TRUE"))
+      return(FALSE)
+  
+    # prepare download options
+    token <- renv_bootstrap_github_token()
+    if (nzchar(Sys.which("curl")) && nzchar(token)) {
+      fmt <- "--location --fail --header \"Authorization: token %s\""
+      extra <- sprintf(fmt, token)
+      saved <- options("download.file.method", "download.file.extra")
+      options(download.file.method = "curl", download.file.extra = extra)
+      on.exit(do.call(base::options, saved), add = TRUE)
+    } else if (nzchar(Sys.which("wget")) && nzchar(token)) {
+      fmt <- "--header=\"Authorization: token %s\""
+      extra <- sprintf(fmt, token)
+      saved <- options("download.file.method", "download.file.extra")
+      options(download.file.method = "wget", download.file.extra = extra)
+      on.exit(do.call(base::options, saved), add = TRUE)
+    }
+  
+    url <- file.path("https://api.github.com/repos/rstudio/renv/tarball", version)
+    name <- sprintf("renv_%s.tar.gz", version)
+    destfile <- file.path(tempdir(), name)
+  
+    status <- tryCatch(
+      renv_bootstrap_download_impl(url, destfile),
+      condition = identity
+    )
+  
+    if (!identical(status, 0L))
+      return(FALSE)
+  
+    renv_bootstrap_download_augment(destfile)
+  
+    return(destfile)
+  
+  }
+  
+  # Add Sha to DESCRIPTION. This is stop gap until #890, after which we
+  # can use renv::install() to fully capture metadata.
+  renv_bootstrap_download_augment <- function(destfile) {
+    sha <- renv_bootstrap_git_extract_sha1_tar(destfile)
+    if (is.null(sha)) {
+      return()
+    }
+  
+    # Untar
+    tempdir <- tempfile("renv-github-")
+    on.exit(unlink(tempdir, recursive = TRUE), add = TRUE)
+    untar(destfile, exdir = tempdir)
+    pkgdir <- dir(tempdir, full.names = TRUE)[[1]]
+  
+    # Modify description
+    desc_path <- file.path(pkgdir, "DESCRIPTION")
+    desc_lines <- readLines(desc_path)
+    remotes_fields <- c(
+      "RemoteType: github",
+      "RemoteHost: api.github.com",
+      "RemoteRepo: renv",
+      "RemoteUsername: rstudio",
+      "RemotePkgRef: rstudio/renv",
+      paste("RemoteRef: ", sha),
+      paste("RemoteSha: ", sha)
+    )
+    writeLines(c(desc_lines[desc_lines != ""], remotes_fields), con = desc_path)
+  
+    # Re-tar
+    local({
+      old <- setwd(tempdir)
+      on.exit(setwd(old), add = TRUE)
+  
+      tar(destfile, compression = "gzip")
+    })
+    invisible()
+  }
+  
+  # Extract the commit hash from a git archive. Git archives include the SHA1
+  # hash as the comment field of the tarball pax extended header
+  # (see https://www.kernel.org/pub/software/scm/git/docs/git-archive.html)
+  # For GitHub archives this should be the first header after the default one
+  # (512 byte) header.
+  renv_bootstrap_git_extract_sha1_tar <- function(bundle) {
+  
+    # open the bundle for reading
+    # We use gzcon for everything because (from ?gzcon)
+    # > Reading from a connection which does not supply a 'gzip' magic
+    # > header is equivalent to reading from the original connection
+    conn <- gzcon(file(bundle, open = "rb", raw = TRUE))
+    on.exit(close(conn))
+  
+    # The default pax header is 512 bytes long and the first pax extended header
+    # with the comment should be 51 bytes long
+    # `52 comment=` (11 chars) + 40 byte SHA1 hash
+    len <- 0x200 + 0x33
+    res <- rawToChar(readBin(conn, "raw", n = len)[0x201:len])
+  
+    if (grepl("^52 comment=", res)) {
+      sub("52 comment=", "", res)
+    } else {
+      NULL
+    }
+  }
+  
+  renv_bootstrap_install <- function(version, tarball, library) {
+  
+    # attempt to install it into project library
+    dir.create(library, showWarnings = FALSE, recursive = TRUE)
+    output <- renv_bootstrap_install_impl(library, tarball)
+  
+    # check for successful install
+    status <- attr(output, "status")
+    if (is.null(status) || identical(status, 0L))
+      return(status)
+  
+    # an error occurred; report it
+    header <- "installation of renv failed"
+    lines <- paste(rep.int("=", nchar(header)), collapse = "")
+    text <- paste(c(header, lines, output), collapse = "\n")
+    stop(text)
+  
+  }
+  
+  renv_bootstrap_install_impl <- function(library, tarball) {
+  
+    # invoke using system2 so we can capture and report output
+    bin <- R.home("bin")
+    exe <- if (Sys.info()[["sysname"]] == "Windows") "R.exe" else "R"
+    R <- file.path(bin, exe)
+  
+    args <- c(
+      "--vanilla", "CMD", "INSTALL", "--no-multiarch",
+      "-l", shQuote(path.expand(library)),
+      shQuote(path.expand(tarball))
+    )
+  
+    system2(R, args, stdout = TRUE, stderr = TRUE)
+  
+  }
+  
+  renv_bootstrap_platform_prefix <- function() {
+  
+    # construct version prefix
+    version <- paste(R.version$major, R.version$minor, sep = ".")
+    prefix <- paste("R", numeric_version(version)[1, 1:2], sep = "-")
+  
+    # include SVN revision for development versions of R
+    # (to avoid sharing platform-specific artefacts with released versions of R)
+    devel <-
+      identical(R.version[["status"]],   "Under development (unstable)") ||
+      identical(R.version[["nickname"]], "Unsuffered Consequences")
+  
+    if (devel)
+      prefix <- paste(prefix, R.version[["svn rev"]], sep = "-r")
+  
+    # build list of path components
+    components <- c(prefix, R.version$platform)
+  
+    # include prefix if provided by user
+    prefix <- renv_bootstrap_platform_prefix_impl()
+    if (!is.na(prefix) && nzchar(prefix))
+      components <- c(prefix, components)
+  
+    # build prefix
+    paste(components, collapse = "/")
+  
+  }
+  
+  renv_bootstrap_platform_prefix_impl <- function() {
+  
+    # if an explicit prefix has been supplied, use it
+    prefix <- Sys.getenv("RENV_PATHS_PREFIX", unset = NA)
+    if (!is.na(prefix))
+      return(prefix)
+  
+    # if the user has requested an automatic prefix, generate it
+    auto <- Sys.getenv("RENV_PATHS_PREFIX_AUTO", unset = NA)
+    if (is.na(auto) && getRversion() >= "4.4.0")
+      auto <- "TRUE"
+  
+    if (auto %in% c("TRUE", "True", "true", "1"))
+      return(renv_bootstrap_platform_prefix_auto())
+  
+    # empty string on failure
+    ""
+  
+  }
+  
+  renv_bootstrap_platform_prefix_auto <- function() {
+  
+    prefix <- tryCatch(renv_bootstrap_platform_os(), error = identity)
+    if (inherits(prefix, "error") || prefix %in% "unknown") {
+  
+      msg <- paste(
+        "failed to infer current operating system",
+        "please file a bug report at https://github.com/rstudio/renv/issues",
+        sep = "; "
+      )
+  
+      warning(msg)
+  
+    }
+  
+    prefix
+  
+  }
+  
+  renv_bootstrap_platform_os <- function() {
+  
+    sysinfo <- Sys.info()
+    sysname <- sysinfo[["sysname"]]
+  
+    # handle Windows + macOS up front
+    if (sysname == "Windows")
+      return("windows")
+    else if (sysname == "Darwin")
+      return("macos")
+  
+    # check for os-release files
+    for (file in c("/etc/os-release", "/usr/lib/os-release"))
+      if (file.exists(file))
+        return(renv_bootstrap_platform_os_via_os_release(file, sysinfo))
+  
+    # check for redhat-release files
+    if (file.exists("/etc/redhat-release"))
+      return(renv_bootstrap_platform_os_via_redhat_release())
+  
+    "unknown"
+  
+  }
+  
+  renv_bootstrap_platform_os_via_os_release <- function(file, sysinfo) {
+  
+    # read /etc/os-release
+    release <- utils::read.table(
+      file             = file,
+      sep              = "=",
+      quote            = c("\"", "'"),
+      col.names        = c("Key", "Value"),
+      comment.char     = "#",
+      stringsAsFactors = FALSE
+    )
+  
+    vars <- as.list(release$Value)
+    names(vars) <- release$Key
+  
+    # get os name
+    os <- tolower(sysinfo[["sysname"]])
+  
+    # read id
+    id <- "unknown"
+    for (field in c("ID", "ID_LIKE")) {
+      if (field %in% names(vars) && nzchar(vars[[field]])) {
+        id <- vars[[field]]
+        break
+      }
+    }
+  
+    # read version
+    version <- "unknown"
+    for (field in c("UBUNTU_CODENAME", "VERSION_CODENAME", "VERSION_ID", "BUILD_ID")) {
+      if (field %in% names(vars) && nzchar(vars[[field]])) {
+        version <- vars[[field]]
+        break
+      }
+    }
+  
+    # join together
+    paste(c(os, id, version), collapse = "-")
+  
+  }
+  
+  renv_bootstrap_platform_os_via_redhat_release <- function() {
+  
+    # read /etc/redhat-release
+    contents <- readLines("/etc/redhat-release", warn = FALSE)
+  
+    # infer id
+    id <- if (grepl("centos", contents, ignore.case = TRUE))
+      "centos"
+    else if (grepl("redhat", contents, ignore.case = TRUE))
+      "redhat"
+    else
+      "unknown"
+  
+    # try to find a version component (very hacky)
+    version <- "unknown"
+  
+    parts <- strsplit(contents, "[[:space:]]")[[1L]]
+    for (part in parts) {
+  
+      nv <- tryCatch(numeric_version(part), error = identity)
+      if (inherits(nv, "error"))
+        next
+  
+      version <- nv[1, 1]
+      break
+  
+    }
+  
+    paste(c("linux", id, version), collapse = "-")
+  
+  }
+  
+  renv_bootstrap_library_root_name <- function(project) {
+  
+    # use project name as-is if requested
+    asis <- Sys.getenv("RENV_PATHS_LIBRARY_ROOT_ASIS", unset = "FALSE")
+    if (asis)
+      return(basename(project))
+  
+    # otherwise, disambiguate based on project's path
+    id <- substring(renv_bootstrap_hash_text(project), 1L, 8L)
+    paste(basename(project), id, sep = "-")
+  
+  }
+  
+  renv_bootstrap_library_root <- function(project) {
+  
+    prefix <- renv_bootstrap_profile_prefix()
+  
+    path <- Sys.getenv("RENV_PATHS_LIBRARY", unset = NA)
+    if (!is.na(path))
+      return(paste(c(path, prefix), collapse = "/"))
+  
+    path <- renv_bootstrap_library_root_impl(project)
+    if (!is.null(path)) {
+      name <- renv_bootstrap_library_root_name(project)
+      return(paste(c(path, prefix, name), collapse = "/"))
+    }
+  
+    renv_bootstrap_paths_renv("library", project = project)
+  
+  }
+  
+  renv_bootstrap_library_root_impl <- function(project) {
+  
+    root <- Sys.getenv("RENV_PATHS_LIBRARY_ROOT", unset = NA)
+    if (!is.na(root))
+      return(root)
+  
+    type <- renv_bootstrap_project_type(project)
+    if (identical(type, "package")) {
+      userdir <- renv_bootstrap_user_dir()
+      return(file.path(userdir, "library"))
+    }
+  
+  }
+  
+  renv_bootstrap_validate_version <- function(version, description = NULL) {
+  
+    # resolve description file
+    #
+    # avoid passing lib.loc to `packageDescription()` below, since R will
+    # use the loaded version of the package by default anyhow. note that
+    # this function should only be called after 'renv' is loaded
+    # https://github.com/rstudio/renv/issues/1625
+    description <- description %||% packageDescription("renv")
+  
+    # check whether requested version 'version' matches loaded version of renv
+    sha <- attr(version, "sha", exact = TRUE)
+    valid <- if (!is.null(sha))
+      renv_bootstrap_validate_version_dev(sha, description)
+    else
+      renv_bootstrap_validate_version_release(version, description)
+  
+    if (valid)
+      return(TRUE)
+  
+    # the loaded version of renv doesn't match the requested version;
+    # give the user instructions on how to proceed
+    dev <- identical(description[["RemoteType"]], "github")
+    remote <- if (dev)
+      paste("rstudio/renv", description[["RemoteSha"]], sep = "@")
+    else
+      paste("renv", description[["Version"]], sep = "@")
+  
+    # display both loaded version + sha if available
+    friendly <- renv_bootstrap_version_friendly(
+      version = description[["Version"]],
+      sha     = if (dev) description[["RemoteSha"]]
+    )
+  
+    fmt <- heredoc("
+      renv %1$s was loaded from project library, but this project is configured to use renv %2$s.
+      - Use `renv::record(\"%3$s\")` to record renv %1$s in the lockfile.
+      - Use `renv::restore(packages = \"renv\")` to install renv %2$s into the project library.
+    ")
+    catf(fmt, friendly, renv_bootstrap_version_friendly(version), remote)
+  
+    FALSE
+  
+  }
+  
+  renv_bootstrap_validate_version_dev <- function(version, description) {
+    expected <- description[["RemoteSha"]]
+    is.character(expected) && startswith(expected, version)
+  }
+  
+  renv_bootstrap_validate_version_release <- function(version, description) {
+    expected <- description[["Version"]]
+    is.character(expected) && identical(expected, version)
+  }
+  
+  renv_bootstrap_hash_text <- function(text) {
+  
+    hashfile <- tempfile("renv-hash-")
+    on.exit(unlink(hashfile), add = TRUE)
+  
+    writeLines(text, con = hashfile)
+    tools::md5sum(hashfile)
+  
+  }
+  
+  renv_bootstrap_load <- function(project, libpath, version) {
+  
+    # try to load renv from the project library
+    if (!requireNamespace("renv", lib.loc = libpath, quietly = TRUE))
+      return(FALSE)
+  
+    # warn if the version of renv loaded does not match
+    renv_bootstrap_validate_version(version)
+  
+    # execute renv load hooks, if any
+    hooks <- getHook("renv::autoload")
+    for (hook in hooks)
+      if (is.function(hook))
+        tryCatch(hook(), error = warnify)
+  
+    # load the project
+    renv::load(project)
+  
+    TRUE
+  
+  }
+  
+  renv_bootstrap_profile_load <- function(project) {
+  
+    # if RENV_PROFILE is already set, just use that
+    profile <- Sys.getenv("RENV_PROFILE", unset = NA)
+    if (!is.na(profile) && nzchar(profile))
+      return(profile)
+  
+    # check for a profile file (nothing to do if it doesn't exist)
+    path <- renv_bootstrap_paths_renv("profile", profile = FALSE, project = project)
+    if (!file.exists(path))
+      return(NULL)
+  
+    # read the profile, and set it if it exists
+    contents <- readLines(path, warn = FALSE)
+    if (length(contents) == 0L)
+      return(NULL)
+  
+    # set RENV_PROFILE
+    profile <- contents[[1L]]
+    if (!profile %in% c("", "default"))
+      Sys.setenv(RENV_PROFILE = profile)
+  
+    profile
+  
+  }
+  
+  renv_bootstrap_profile_prefix <- function() {
+    profile <- renv_bootstrap_profile_get()
+    if (!is.null(profile))
+      return(file.path("profiles", profile, "renv"))
+  }
+  
+  renv_bootstrap_profile_get <- function() {
+    profile <- Sys.getenv("RENV_PROFILE", unset = "")
+    renv_bootstrap_profile_normalize(profile)
+  }
+  
+  renv_bootstrap_profile_set <- function(profile) {
+    profile <- renv_bootstrap_profile_normalize(profile)
+    if (is.null(profile))
+      Sys.unsetenv("RENV_PROFILE")
+    else
+      Sys.setenv(RENV_PROFILE = profile)
+  }
+  
+  renv_bootstrap_profile_normalize <- function(profile) {
+  
+    if (is.null(profile) || profile %in% c("", "default"))
+      return(NULL)
+  
+    profile
+  
+  }
+  
+  renv_bootstrap_path_absolute <- function(path) {
+  
+    substr(path, 1L, 1L) %in% c("~", "/", "\\") || (
+      substr(path, 1L, 1L) %in% c(letters, LETTERS) &&
+      substr(path, 2L, 3L) %in% c(":/", ":\\")
+    )
+  
+  }
+  
+  renv_bootstrap_paths_renv <- function(..., profile = TRUE, project = NULL) {
+    renv <- Sys.getenv("RENV_PATHS_RENV", unset = "renv")
+    root <- if (renv_bootstrap_path_absolute(renv)) NULL else project
+    prefix <- if (profile) renv_bootstrap_profile_prefix()
+    components <- c(root, renv, prefix, ...)
+    paste(components, collapse = "/")
+  }
+  
+  renv_bootstrap_project_type <- function(path) {
+  
+    descpath <- file.path(path, "DESCRIPTION")
+    if (!file.exists(descpath))
+      return("unknown")
+  
+    desc <- tryCatch(
+      read.dcf(descpath, all = TRUE),
+      error = identity
+    )
+  
+    if (inherits(desc, "error"))
+      return("unknown")
+  
+    type <- desc$Type
+    if (!is.null(type))
+      return(tolower(type))
+  
+    package <- desc$Package
+    if (!is.null(package))
+      return("package")
+  
+    "unknown"
+  
+  }
+  
+  renv_bootstrap_user_dir <- function() {
+    dir <- renv_bootstrap_user_dir_impl()
+    path.expand(chartr("\\", "/", dir))
+  }
+  
+  renv_bootstrap_user_dir_impl <- function() {
+  
+    # use local override if set
+    override <- getOption("renv.userdir.override")
+    if (!is.null(override))
+      return(override)
+  
+    # use R_user_dir if available
+    tools <- asNamespace("tools")
+    if (is.function(tools$R_user_dir))
+      return(tools$R_user_dir("renv", "cache"))
+  
+    # try using our own backfill for older versions of R
+    envvars <- c("R_USER_CACHE_DIR", "XDG_CACHE_HOME")
+    for (envvar in envvars) {
+      root <- Sys.getenv(envvar, unset = NA)
+      if (!is.na(root))
+        return(file.path(root, "R/renv"))
+    }
+  
+    # use platform-specific default fallbacks
+    if (Sys.info()[["sysname"]] == "Windows")
+      file.path(Sys.getenv("LOCALAPPDATA"), "R/cache/R/renv")
+    else if (Sys.info()[["sysname"]] == "Darwin")
+      "~/Library/Caches/org.R-project.R/R/renv"
+    else
+      "~/.cache/R/renv"
+  
+  }
+  
+  renv_bootstrap_version_friendly <- function(version, shafmt = NULL, sha = NULL) {
+    sha <- sha %||% attr(version, "sha", exact = TRUE)
+    parts <- c(version, sprintf(shafmt %||% " [sha: %s]", substring(sha, 1L, 7L)))
+    paste(parts, collapse = "")
+  }
+  
+  renv_bootstrap_exec <- function(project, libpath, version) {
+    if (!renv_bootstrap_load(project, libpath, version))
+      renv_bootstrap_run(version, libpath)
+  }
+  
+  renv_bootstrap_run <- function(version, libpath) {
+  
+    # perform bootstrap
+    bootstrap(version, libpath)
+  
+    # exit early if we're just testing bootstrap
+    if (!is.na(Sys.getenv("RENV_BOOTSTRAP_INSTALL_ONLY", unset = NA)))
+      return(TRUE)
+  
+    # try again to load
+    if (requireNamespace("renv", lib.loc = libpath, quietly = TRUE)) {
+      return(renv::load(project = getwd()))
+    }
+  
+    # failed to download or load renv; warn the user
+    msg <- c(
+      "Failed to find an renv installation: the project will not be loaded.",
+      "Use `renv::activate()` to re-initialize the project."
+    )
+  
+    warning(paste(msg, collapse = "\n"), call. = FALSE)
+  
+  }
+  
+  renv_json_read <- function(file = NULL, text = NULL) {
+  
+    jlerr <- NULL
+  
+    # if jsonlite is loaded, use that instead
+    if ("jsonlite" %in% loadedNamespaces()) {
+  
+      json <- tryCatch(renv_json_read_jsonlite(file, text), error = identity)
+      if (!inherits(json, "error"))
+        return(json)
+  
+      jlerr <- json
+  
+    }
+  
+    # otherwise, fall back to the default JSON reader
+    json <- tryCatch(renv_json_read_default(file, text), error = identity)
+    if (!inherits(json, "error"))
+      return(json)
+  
+    # report an error
+    if (!is.null(jlerr))
+      stop(jlerr)
+    else
+      stop(json)
+  
+  }
+  
+  renv_json_read_jsonlite <- function(file = NULL, text = NULL) {
+    text <- paste(text %||% readLines(file, warn = FALSE), collapse = "\n")
+    jsonlite::fromJSON(txt = text, simplifyVector = FALSE)
+  }
+  
+  renv_json_read_default <- function(file = NULL, text = NULL) {
+  
+    # find strings in the JSON
+    text <- paste(text %||% readLines(file, warn = FALSE), collapse = "\n")
+    pattern <- '["](?:(?:\\\\.)|(?:[^"\\\\]))*?["]'
+    locs <- gregexpr(pattern, text, perl = TRUE)[[1]]
+  
+    # if any are found, replace them with placeholders
+    replaced <- text
+    strings <- character()
+    replacements <- character()
+  
+    if (!identical(c(locs), -1L)) {
+  
+      # get the string values
+      starts <- locs
+      ends <- locs + attr(locs, "match.length") - 1L
+      strings <- substring(text, starts, ends)
+  
+      # only keep those requiring escaping
+      strings <- grep("[[\\]{}:]", strings, perl = TRUE, value = TRUE)
+  
+      # compute replacements
+      replacements <- sprintf('"\032%i\032"', seq_along(strings))
+  
+      # replace the strings
+      mapply(function(string, replacement) {
+        replaced <<- sub(string, replacement, replaced, fixed = TRUE)
+      }, strings, replacements)
+  
+    }
+  
+    # transform the JSON into something the R parser understands
+    transformed <- replaced
+    transformed <- gsub("{}", "`names<-`(list(), character())", transformed, fixed = TRUE)
+    transformed <- gsub("[[{]", "list(", transformed, perl = TRUE)
+    transformed <- gsub("[]}]", ")", transformed, perl = TRUE)
+    transformed <- gsub(":", "=", transformed, fixed = TRUE)
+    text <- paste(transformed, collapse = "\n")
+  
+    # parse it
+    json <- parse(text = text, keep.source = FALSE, srcfile = NULL)[[1L]]
+  
+    # construct map between source strings, replaced strings
+    map <- as.character(parse(text = strings))
+    names(map) <- as.character(parse(text = replacements))
+  
+    # convert to list
+    map <- as.list(map)
+  
+    # remap strings in object
+    remapped <- renv_json_read_remap(json, map)
+  
+    # evaluate
+    eval(remapped, envir = baseenv())
+  
+  }
+  
+  renv_json_read_remap <- function(json, map) {
+  
+    # fix names
+    if (!is.null(names(json))) {
+      lhs <- match(names(json), names(map), nomatch = 0L)
+      rhs <- match(names(map), names(json), nomatch = 0L)
+      names(json)[rhs] <- map[lhs]
+    }
+  
+    # fix values
+    if (is.character(json))
+      return(map[[json]] %||% json)
+  
+    # handle true, false, null
+    if (is.name(json)) {
+      text <- as.character(json)
+      if (text == "true")
+        return(TRUE)
+      else if (text == "false")
+        return(FALSE)
+      else if (text == "null")
+        return(NULL)
+    }
+  
+    # recurse
+    if (is.recursive(json)) {
+      for (i in seq_along(json)) {
+        json[i] <- list(renv_json_read_remap(json[[i]], map))
+      }
+    }
+  
+    json
+  
+  }
+
+  # load the renv profile, if any
+  renv_bootstrap_profile_load(project)
+
+  # construct path to library root
+  root <- renv_bootstrap_library_root(project)
+
+  # construct library prefix for platform
+  prefix <- renv_bootstrap_platform_prefix()
+
+  # construct full libpath
+  libpath <- file.path(root, prefix)
+
+  # run bootstrap code
+  renv_bootstrap_exec(project, libpath, version)
+
+  invisible()
+
+})
diff --git a/renv/settings.json b/renv/settings.json
new file mode 100644
index 0000000..e4524f7
--- /dev/null
+++ b/renv/settings.json
@@ -0,0 +1,21 @@
+{
+  "bioconductor.version": null,
+  "external.libraries": [],
+  "ignored.packages": [
+    "flowCore"
+  ],
+  "package.dependency.fields": [
+    "Imports",
+    "Depends",
+    "LinkingTo"
+  ],
+  "ppm.enabled": null,
+  "ppm.ignored.urls": [],
+  "r.version": null,
+  "snapshot.type": "implicit",
+  "use.cache": true,
+  "vcs.ignore.cellar": true,
+  "vcs.ignore.library": true,
+  "vcs.ignore.local": true,
+  "vcs.manage.ignores": true
+}
diff --git a/rollback_plan.md b/rollback_plan.md
new file mode 100644
index 0000000..894a972
--- /dev/null
+++ b/rollback_plan.md
@@ -0,0 +1,12 @@
+ROLLBACK PLAN:
+If issues occur at any step:
+bash
+# Record current commit for reference
+git rev-parse HEAD > /tmp/last_working_commit
+
+# Rollback
+git reset --hard HEAD~1
+./bash/tests/core/run_tests.sh
+
+# If still broken
+git reset --hard $(cat /tmp/last_working_commit)
diff --git a/rsyncGitRepositories.sh b/rsyncGitRepositories.sh
deleted file mode 100755
index 345f3d3..0000000
--- a/rsyncGitRepositories.sh
+++ /dev/null
@@ -1,45 +0,0 @@
-#!/bin/bash
-#if [ $# -ne 1 ]; then
-echo "Usage: $0 "
-#fi
-# Set default values for source and destination directories
-SOURCE_DIR="$HOME"
-DEST_DIR="/mnt/c/Users/${WINDOWS_USER}/Dropbox (MIT)/"
-
-# Check if the source and destination directories exist
-if [ ! -d "$SOURCE_DIR" ]; then
-    echo "Error: Source directory '$SOURCE_DIR' does not exist."
-    exit 1
-fi
-
-if [ ! -d "$DEST_DIR" ]; then
-    printf "Error: Destination directory '%s' does not exist.\n" "$DEST_DIR"
-    printf "Please ensure that the Dropbox folder is mounted and accessible.\n"
-    exit 1
-fi
-
-# Print source and destination directories for debugging
-printf "Source directory: %s\n" "$SOURCE_DIR"
-printf "Destination directory: %s\n" "$DEST_DIR"
-
-find "$SOURCE_DIR" -maxdepth 1 -type d -print0 | while IFS= read -r -d $'\0' dir; do
-
-  # Check if the directory is a Git repository
-  if [ -d "$dir/.git" -a "$dir" != "$HOME/.nvm" -a "$dir" != "$HOME/cytolib" ]; then
-    repo_name=$(basename "$dir")
-
-    printf "Found Git repository: %s\n" "$dir"
-    printf "Syncing to: %s\n" "$DEST_DIR"
-
-    # Rsync the Git repository, excluding the .git directory
-    rsync -av --delete --exclude=.git/ "$dir" "$DEST_DIR"
-
-    if [ $? -eq 0 ]; then
-      printf "Successfully synced %s.\n" "$repo_name"
-    else
-      printf "Error syncing %s. Please check the logs.\n" "$repo_name"
-    fi
-
-  fi
-
-done
diff --git a/simple_approach_for_bmc_folder_preparation.md b/simple_approach_for_bmc_folder_preparation.md
new file mode 100644
index 0000000..63afe46
--- /dev/null
+++ b/simple_approach_for_bmc_folder_preparation.md
@@ -0,0 +1,11 @@
+# 
+
+#
+
+# Steps
+Run srun according to email from bmc. 
+```bash
+srun rsync -av /net/bmc-pub17/data/bmc/public/Bell/241007Bel/ ~/data/241007Bel/fastq
+srun rsync -av /net/bmc-pub17/data/bmc/public/Bell/241010Bel/ ~/data/241010Bel/fastq
+```
+
